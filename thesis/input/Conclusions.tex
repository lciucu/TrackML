\chapter{Conclusions}
\label{Conclusions}

\section{Conclusions}

CERN experiments at the LHC plan to continue their studies of the building blocks of the Universe by studying proton-proton collisions with ever instantaneous luminosity. This improves the probability that interesting particles are produced, such as the Higgs boson, top quarks, W and Z bosons, and hopefully also particles predicted by models Beyond the Standard Model. It translates to increasing number of collisions per bunch crossing (pile-up $\mu$). For Run-4, $\mu$ is expected to reach a value of 200, from a maximum around 60 today. The drawback is that the particle events become much busier, with many more particles overlapping. Reconstructiong these particles becomes harder. So much so that computing resources are predicted to not be enough to reconstruct Run-4 detector data and simulation data. 

\ \\The solution is to improve dramatically particle reconstruction from the algorithm and software side. In this regard, machine learning is a promising avenue studied thorougly at CERN. In this thesis, a deep neural network (DNN) is studied to reconstruct particles from hit data in the inner detector, using simulations of a general particle physics detector at CERN, provided by the TrackML data challenge. The inner detector is split into subdetectors and regions defined by the \volumeID, \layerID~and \moduleID. For each simulated hit, both the reconstructed position and the original (true) position is known. Also the \particleID~of the particle producing the hit is known. This allows to build a supervised machine learning algorithm, to study groups of hits at a time and try to identify the particle with the largest number of hits in the group. First all hits in an event are organised in an approximate nearest neighbour tree based on their spatial position. For each hit in the detector a group of hits is created consisting of hits that are closest to the direction of the line that connects this hit with the original collision in the center of the detector. Such group is denoted a bucket. Since a particle leaves typically around 10 hits in the detector, the bucket size is chosen to be 20. However,the number of hits belonging to the particle with the largest number of hits is counted. If this number is smaller than 10, then it is considered that none of the hits belongs to this particle, and the labels of all hits are reset to -1. The goal is therefore that in each group of 20 hits, to identify those that belong to a particle that has at least 10 hits. It is a multi-class binary classificiation problem, as for each hit in the bucket there is a question that can be answered by yes or no, namely if the hit belongs to the particle with the largest number of hits in the bucket. 

\ \\A deep neural network algorithm is trained, using Keras and TensorFlow in Python. 100 events are used, split 70\% in training and 30\% in testing. The training data set is balanced. The test sample is studied both in a balanced and unbalanced format The balanced training set has about 130k buckets, the balanced testing set about 62k buckets, and the unbalanced testing seet about 3.2M buckets. Hyper-parameters are tuned and the resulting structure of the best model is summarised in Section~\ref{sec:BestModel}.

\ \\The final performance metrics are shown for the unbalanced dataset, ensuring a particle reconstruction efficiency of 71.3\%. 

\section{Future Plans}

Given more time, several improvements or new studies may enhance the current results.

\ \\The output labels of the hits of no and yes are represented by -1.0 and 1.0. One can represent them also as 0.0 and 1.0. This leads to the use of other activation functions on the last layer and to other loss functions. A preliminary study suggests that -1.0/1.0 behave better than 0.0/1.0, but a more thorough study may be performed. 

\ \\Only 100 events have been studied in this project. The entire TrackML dataset is 100 times larger, consisting of 10000 events. Deep learning methods benefit from using large quantities of data. Using the entire dataset should provide a better peforming model. The technical challenges will remain the computing power needed for training. Dedicated resources at CERN and member institutes, such as the University of Geneva, using CPU and ideally also GPU, may then be used to improve the training and inference times.

\ \\Once the current question is addressed, namely of identifying hits belonging to the particle with the largest number of hits, more complex questions may be tackled, such as identifing several particles at once from a given bucket, probably with using a larger bucket size.

\ \\To conclude, this study is a stepping stone towards improving tracking reconstruction for Run-4 at the LHC at pile-up $\mu=200$, for fast real-time inference for trigger and particle offline reconstruction, using advanced machine learning techniques.
