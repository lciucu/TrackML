\chapter{Conclusions}
\label{Conclusions}

\section{Conclusions}

CERN experiments at the LHC plan to continue their studies of the building blocks of the Universe by studying proton-proton collisions with ever increasing instantaneous luminosity. This improves the probability that interesting particles are produced, such as Higgs bosons, top quarks, W and Z bosons, and hopefully also particles predicted by models Beyond the Standard Model. Higher instantaneous luminosity translates to increasing number of collisions per bunch crossing (pile-up $\mu$). For Run-4, $\mu$ is expected to reach a value of 200, from a maximum around 60 in Run-2. The drawback to increasing $\mu$ is that the particle events become much busier, with many more particles overlapping. Reconstructiong these particles becomes harder. So much so that computing resources are predicted to not be enough to reconstruct Run-4 detector and simulation data. 

\ \\The solution is to dramatically improve particle reconstruction with improved algorithms and software. In this regard, machine learning is a promising avenue studied thorougly at CERN. In this thesis, a deep neural network (DNN) is studied to reconstruct particles from hit data in the inner detector, using simulations of a general-purpose particle physics detector at CERN, provided by the TrackML data challenge. The inner detector is split into sub-detectors and regions defined by the \volumeID, \layerID~and \moduleID. For each simulated hit, both the reconstructed position and the original (true) position are known. Also the \particleID~of the particle producing the hit is known. This allows a supervised machine learning algorithm to be built. The aim of the algorithm is to study groups of hits at a time and try to identify the particle with the largest number of hits in the group. This particle is called the majority particle.

\ \\First, all hits in an event are organised in an approximate nearest neighbour tree based on their spatial position. For each query hit in the detector a group is created consisting of hits that are closest to the direction of the line that connects the query hit with the original collision in the centre of the detector. Each such group is denoted a bucket. A particle typically leaves around 10 hits in the detector, but less than 20. For this reason, the bucket size is chosen to be 20. The number of hits belonging to the majority particle is counted. If this number is smaller than 10, then it is considered that none of the hits belong to this particle, and the labels of all hits in the bucket are set to -1. The goal is therefore that in each group of 20 hits, to identify those that belong to a particle that has 10 or more hits. This is a multi-class binary classificiation problem, as for each hit in the bucket there is a question that can be answered by yes or no, namely if the hit belongs to the majority particle.

\ \\A deep neural network algorithm is trained, using Keras and TensorFlow in Python. 100 events are used, split 70\% in training and 30\% in testing. The training dataset is balanced. The testing dataset is studied both in a balanced and unbalanced format. The balanced training dataset has about 130k buckets, the balanced testing dataset about 62k buckets, and the unbalanced testing dataset about 3.2M buckets. Hyper-parameters for the model are tuned. The resulting structure of the best-performing model is summarised in Section~\ref{sec:BestModel}.

\ \\The final performance metrics are shown for the unbalanced dataset, resulting in a particle reconstruction efficiency of 71.3\%. 

\section{Future Plans}

Given more time, several improvements or new studies may enhance the current results.

\ \\The output labels of the hits belonging or not to the majority particle in a bucket are represented by +1 and -1. One can represent them also as +1 and 0. This leads to the use of other activation functions on the final layer and to alternative loss functions. A preliminary study suggests that +1/-1 behave better than +1/0, but a more thorough study may be performed. 

\ \\Only 100 events have been studied in this project. The entire TrackML dataset is 100 times larger, consisting of 10000 events. Deep learning methods benefit from using large quantities of data. Using the entire dataset should result in a better peforming model. The principal technical challenge remains the computing power needed for training. Dedicated resources at CERN and member institutes, such as the University of Geneva, using CPU and ideally also GPU, may then be used to improve the training and inference times.

\ \\Once the current question is addressed, namely of identifying hits belonging to the majority particle, more complex questions may be tackled. For example, identifing several particles at once from a given bucket, probably using a larger bucket size.

\ \\To conclude, this study is a stepping stone towards improving particle track reconstruction for Run-4 at the LHC at pile-up $\mu=200$, using advanced machine learning techniques.
