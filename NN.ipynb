{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# for reproducibility, before importing keras \n",
    "# we need to set random numbers in both numpy and tensorflow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "np.random.seed(98383822)\n",
    "tf.random.set_seed(2)\n",
    "import keras\n",
    "\n",
    "import os\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import pandas as pd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug=False\n",
    "verbose=True\n",
    "doTrain=True\n",
    "doLoadModel=True\n",
    "doLoadMetrics=True\n",
    "doPlotMetrics=True\n",
    "doPredict=True\n",
    "doLoadPredict=True\n",
    "doCalculateMetrics2=True\n",
    "doPlotMetrics2=True\n",
    "doCalculateMetrics3=True\n",
    "doPlotMetrics3=True\n",
    "modelName=\"Balanced\"\n",
    "numberOfEpochs=120\n",
    "batchSize=50000\n",
    "extensions=\"png,pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/Luiza_SSD/ATLAS/TrackML'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p(name,nparray):\n",
    "    print(\"Start\",name)\n",
    "    print(nparray)\n",
    "    print(\"End\",name,\"shape\",nparray.shape,\"dtype\",nparray.dtype,\"type\",type(nparray))\n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderStem=\"/Volumes/Luiza_SSD\"\n",
    "#folderStem=\"/Users/luizaadelinaciucu/Work\"\n",
    "#\n",
    "inputFolderName=folderStem+\"/ATLAS/TrackML/output_new_ev_000_100\"\n",
    "minNbPositiveHit=\"00\"\n",
    "outputFolderName=inputFolder=inputFolderName+\"_min_\"+minNbPositiveHit\n",
    "# if output folder does not exist, create it\n",
    "if not os.path.exists(outputFolderName):\n",
    "    os.makedirs(outputFolderName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eventNumber=\"000000082\"\n",
    "eventNumber=\"all\"\n",
    "\n",
    "\n",
    "if modelName==\"Unbalanced\":\n",
    "    # without Balanced in the names\n",
    "    #\n",
    "    nparray_Input_Train=np.load(inputFolderName+\"/NN_2_data_Input_Train_\"+eventNumber+\".npy\")\n",
    "    nparray_Input_Test=np.load(inputFolderName+\"/NN_2_data_Input_Test_\"+eventNumber+\".npy\")\n",
    "    #\n",
    "    nparray_Output_Train=np.load(inputFolderName+\"/NN_2_data_OutputMin\"+minNbPositiveHit+\"_Train_\"+eventNumber+\".npy\")\n",
    "    nparray_Output_Test=np.load(inputFolderName+\"/NN_2_data_OutputMin\"+minNbPositiveHit+\"_Test_\"+eventNumber+\".npy\")\n",
    "    #\n",
    "    nparray_VolumeID_Train=np.load(inputFolderName+\"/NN_2_data_VolumeID_Train_\"+eventNumber+\".npy\")\n",
    "    nparray_VolumeID_Test=np.load(inputFolderName+\"/NN_2_data_VolumeID_Test_\"+eventNumber+\".npy\")\n",
    "\n",
    "if modelName==\"Balanced\":\n",
    "    # with Balanced in the names\n",
    "    #\n",
    "    nparray_Input_Train=np.load(inputFolderName+\"/NN_2_data_InputBalanced_Train_\"+eventNumber+\".npy\")\n",
    "    nparray_Input_Test=np.load(inputFolderName+\"/NN_2_data_InputBalanced_Test_\"+eventNumber+\".npy\")\n",
    "    #\n",
    "    nparray_Output_Train=np.load(inputFolderName+\"/NN_2_data_OutputBalancedMin\"+minNbPositiveHit+\"_Train_\"+eventNumber+\".npy\")\n",
    "    nparray_Output_Test=np.load(inputFolderName+\"/NN_2_data_OutputBalancedMin\"+minNbPositiveHit+\"_Test_\"+eventNumber+\".npy\")\n",
    "    #\n",
    "    nparray_VolumeID_Train=np.load(inputFolderName+\"/NN_2_data_VolumeIDBalanced_Train_\"+eventNumber+\".npy\")\n",
    "    nparray_VolumeID_Test=np.load(inputFolderName+\"/NN_2_data_VolumeIDBalanced_Test_\"+eventNumber+\".npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Input_Train\n",
      "[[[ 3.68880e+01]\n",
      "  [ 1.38507e+00]\n",
      "  [-1.29750e+03]\n",
      "  ...\n",
      "  [ 3.74781e+01]\n",
      "  [-1.87127e+00]\n",
      "  [-1.29750e+03]]\n",
      "\n",
      " [[ 4.72534e+01]\n",
      "  [ 3.84431e+00]\n",
      "  [-1.50250e+03]\n",
      "  ...\n",
      "  [ 3.99709e+01]\n",
      "  [ 5.32750e+00]\n",
      "  [-1.30250e+03]]\n",
      "\n",
      " [[ 7.07738e+01]\n",
      "  [ 8.25329e+00]\n",
      "  [-1.30250e+03]\n",
      "  ...\n",
      "  [ 4.35503e+01]\n",
      "  [ 7.16400e+00]\n",
      "  [-8.22500e+02]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-3.39920e+02]\n",
      "  [ 3.81811e+02]\n",
      "  [ 1.50450e+03]\n",
      "  ...\n",
      "  [-1.87632e+02]\n",
      "  [ 1.87763e+02]\n",
      "  [ 7.77216e+02]]\n",
      "\n",
      " [[-9.43905e+02]\n",
      "  [ 6.73048e-01]\n",
      "  [ 2.95550e+03]\n",
      "  ...\n",
      "  [-4.10712e+02]\n",
      "  [ 9.52379e+00]\n",
      "  [ 1.22150e+03]]\n",
      "\n",
      " [[-8.70819e+02]\n",
      "  [-1.58230e+00]\n",
      "  [ 2.95550e+03]\n",
      "  ...\n",
      "  [-3.34212e+01]\n",
      "  [-1.16950e+00]\n",
      "  [ 1.12495e+02]]]\n",
      "End Input_Train shape (2213733, 60, 1) dtype float32 type <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "p(\"Input_Train\",nparray_Input_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Input_Test\n",
      "[[[ 6.10612e+01]\n",
      "  [ 5.48271e+00]\n",
      "  [-1.30250e+03]\n",
      "  ...\n",
      "  [ 3.11451e+01]\n",
      "  [ 3.69277e+00]\n",
      "  [-6.97500e+02]]\n",
      "\n",
      " [[ 5.66778e+01]\n",
      "  [ 6.69252e+00]\n",
      "  [-1.30250e+03]\n",
      "  ...\n",
      "  [ 5.79303e+01]\n",
      "  [ 4.73475e+00]\n",
      "  [-1.30250e+03]]\n",
      "\n",
      " [[ 3.15741e+01]\n",
      "  [ 3.40823e+00]\n",
      "  [-1.49750e+03]\n",
      "  ...\n",
      "  [ 3.07265e+01]\n",
      "  [-2.34300e+00]\n",
      "  [-1.49750e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 3.41097e+02]\n",
      "  [ 9.00468e+02]\n",
      "  [ 2.55550e+03]\n",
      "  ...\n",
      "  [ 2.75365e+02]\n",
      "  [ 7.69948e+02]\n",
      "  [ 2.14450e+03]]\n",
      "\n",
      " [[ 9.90678e-01]\n",
      "  [-1.56666e+02]\n",
      "  [ 6.02000e+02]\n",
      "  ...\n",
      "  [ 2.14916e-01]\n",
      "  [-3.37550e+01]\n",
      "  [ 1.26807e+02]]\n",
      "\n",
      " [[-7.47925e+02]\n",
      "  [ 2.43178e+02]\n",
      "  [ 2.94450e+03]\n",
      "  ...\n",
      "  [-1.47942e+02]\n",
      "  [ 4.31422e+01]\n",
      "  [ 5.98000e+02]]]\n",
      "End Input_Test shape (983145, 60, 1) dtype float32 type <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "p(\"Input_Test\",nparray_Input_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Output_Train\n",
      "[[ 1  1  1 ... -1 -1 -1]\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " ...\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " [ 1  1  1 ... -1 -1 -1]]\n",
      "End Output_Train shape (2213733, 20) dtype int8 type <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "p (\"Output_Train\", nparray_Output_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Output_Test\n",
      "[[ 1  1  1 ... -1 -1 -1]\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " [-1 -1 -1 ...  1 -1 -1]\n",
      " ...\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " [ 1  1  1 ... -1 -1  1]\n",
      " [ 1  1  1 ... -1  1 -1]]\n",
      "End Output_Test shape (983145, 20) dtype int8 type <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "p (\"Output_Test\",nparray_Output_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start VolumeID_Train\n",
      "[[ 7  7  7 ...  7  7  7]\n",
      " [ 7  7  7 ...  7  7  7]\n",
      " [ 7  7  7 ...  7  7  7]\n",
      " ...\n",
      " [14 18 14 ...  8  8 13]\n",
      " [18 18 14 ...  8  8 14]\n",
      " [18 18 18 ...  8 13  8]]\n",
      "End VolumeID_Train shape (2213733, 20) dtype uint8 type <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "p(\"VolumeID_Train\",nparray_VolumeID_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start VolumeID_Test\n",
      "[[ 7  7  7 ...  7  7  7]\n",
      " [ 7  7  7 ...  7  7  7]\n",
      " [ 7  7  7 ...  7  7  7]\n",
      " ...\n",
      " [18  8  8 ... 13 14 18]\n",
      " [ 9  9 18 ...  8  8  8]\n",
      " [18 14 14 ...  8  8  9]]\n",
      "End VolumeID_Test shape (983145, 20) dtype uint8 type <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "p(\"VolumeID_Test\",nparray_VolumeID_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility, before importing keras \n",
    "# we need to set random numbers in both numpy and tensorflow\n",
    "#np.random.seed(98383822)\n",
    "#tf.random.set_seed(2)\n",
    "#import keras\n",
    "\n",
    "nrNodesInputLayer=nparray_Input_Train.shape[1] # three inputs (x, y, z) for each hit in the batch\n",
    "nrNodesOutputLayer=nparray_Output_Train.shape[1] # one output for each hit in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    \n",
    "    # nr nodes on the hidden layers\n",
    "    k=5\n",
    "    nrNodesHiddenLayer=nrNodesOutputLayer*k\n",
    "\n",
    "    # create empty model\n",
    "    model=keras.models.Sequential()\n",
    "\n",
    "    # add first layer \n",
    "    model.add(keras.layers.Dense(nrNodesInputLayer,activation='linear',input_shape=(nrNodesInputLayer,1)))\n",
    "\n",
    "    # flatten first layer\n",
    "    model.add(keras.layers.Flatten())\n",
    "\n",
    "    # add hidden layers\n",
    "    model.add(keras.layers.Dense(nrNodesHiddenLayer,activation='elu'))\n",
    "    model.add(keras.layers.Dense(nrNodesHiddenLayer,activation='elu'))\n",
    "    \n",
    "    # add output layer\n",
    "    model.add(keras.layers.Dense(nrNodesOutputLayer,activation='tanh'))\n",
    "\n",
    "    # model geometry done\n",
    "\n",
    "    # choosing how the NN learns\n",
    "    # https://keras.io/models/sequential/\n",
    "    # learning method squared hinge\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False),\n",
    "        loss=keras.losses.squared_hinge,\n",
    "        metrics=['binary_accuracy'],\n",
    "        ),\n",
    "    # done if\n",
    "\n",
    "   # now model is done we are ready to train \n",
    "    return model\n",
    "# done function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,modelName,numberOfEpochs,batchSize):\n",
    "    print(\"*** Start train_model for modeName\",modelName,\"***\")\n",
    "    # train the model and return for each epoch the accuracy and loss values\n",
    "    # in a variable called history\n",
    "    # https://keras.io/models/sequential\n",
    "    history=model.fit(\n",
    "            nparray_Input_Train,\n",
    "            nparray_Output_Train,\n",
    "            batchSize,\n",
    "            numberOfEpochs,\n",
    "            validation_data=(nparray_Input_Test,nparray_Output_Test),\n",
    "            shuffle=False,\n",
    "            )\n",
    "    # done if\n",
    "    # the train (fit) function outputs a history\n",
    "    # retrieve from it the accuracy, loss, train, test\n",
    "    nparray_accuracyBinary_Train=history.history[\"binary_accuracy\"]\n",
    "    nparray_accuracyBinary_Test=history.history[\"val_binary_accuracy\"]\n",
    "    nparray_loss_Train=history.history[\"loss\"]\n",
    "    nparray_loss_Test=history.history[\"val_loss\"]\n",
    "    \n",
    "    # save the loss, accuracies, weights + biases of the trained model to a file\n",
    "    # create the name stem, specific for this training\n",
    "    outputFileNameStem=\"NN_3_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    # create the file names for accuracy and loss, train and test\n",
    "    outputFileNameAccuracyBinaryTrain=outputFolderName+\"/\"+outputFileNameStem+\"_accuracyBinary_Train.npy\"\n",
    "    outputFileNameAccuracyBinaryTest=outputFolderName+\"/\"+outputFileNameStem+\"_accuracyBinary_Test.npy\"\n",
    "    outputFileNameLossTrain=outputFolderName+\"/\"+outputFileNameStem+\"_loss_Train.npy\"\n",
    "    outputFileNameLossTest=outputFolderName+\"/\"+outputFileNameStem+\"_loss_Test.npy\"\n",
    "    # create the file name for the weights and biases\n",
    "    outputFileNameWeights=outputFolderName+\"/\"+outputFileNameStem+\"_weights.hdf5\"\n",
    "    \n",
    "    # save to files\n",
    "    #\n",
    "    np.save(outputFileNameAccuracyBinaryTrain,nparray_accuracyBinary_Train)\n",
    "    np.save(outputFileNameAccuracyBinaryTest,nparray_accuracyBinary_Test)\n",
    "    np.save(outputFileNameLossTrain,nparray_loss_Train)\n",
    "    np.save(outputFileNameLossTest,nparray_loss_Test)\n",
    "    #\n",
    "    model.save_weights(outputFileNameWeights)\n",
    "\n",
    "    # ready to return\n",
    "    return (model,nparray_accuracyBinary_Train,nparray_accuracyBinary_Test,nparray_loss_Train,nparray_loss_Test)\n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Start train_model for modeName Balanced ***\n",
      "Train on 2213733 samples, validate on 983145 samples\n",
      "Epoch 1/120\n",
      "2213733/2213733 [==============================] - 205s 93us/step - loss: 1.3935 - binary_accuracy: 0.6512 - val_loss: 1.3166 - val_binary_accuracy: 0.6708\n",
      "Epoch 2/120\n",
      "2213733/2213733 [==============================] - 203s 92us/step - loss: 1.3293 - binary_accuracy: 0.6676 - val_loss: 1.3326 - val_binary_accuracy: 0.6668\n",
      "Epoch 3/120\n",
      "2213733/2213733 [==============================] - 201s 91us/step - loss: 1.3270 - binary_accuracy: 0.6682 - val_loss: 1.3284 - val_binary_accuracy: 0.6679\n",
      "Epoch 4/120\n",
      "2213733/2213733 [==============================] - 202s 91us/step - loss: 1.3240 - binary_accuracy: 0.6690 - val_loss: 1.3222 - val_binary_accuracy: 0.6694\n",
      "Epoch 5/120\n",
      "2213733/2213733 [==============================] - 209s 94us/step - loss: 1.3165 - binary_accuracy: 0.6708 - val_loss: 1.3205 - val_binary_accuracy: 0.6698\n",
      "Epoch 6/120\n",
      "2213733/2213733 [==============================] - 191s 86us/step - loss: 1.3286 - binary_accuracy: 0.6678 - val_loss: 1.3212 - val_binary_accuracy: 0.6697\n",
      "Epoch 7/120\n",
      "2213733/2213733 [==============================] - 190s 86us/step - loss: 1.3080 - binary_accuracy: 0.6729 - val_loss: 1.3173 - val_binary_accuracy: 0.6706\n",
      "Epoch 8/120\n",
      "2213733/2213733 [==============================] - 185s 84us/step - loss: 1.3099 - binary_accuracy: 0.6725 - val_loss: 1.3194 - val_binary_accuracy: 0.6701\n",
      "Epoch 9/120\n",
      "2213733/2213733 [==============================] - 188s 85us/step - loss: 1.3066 - binary_accuracy: 0.6733 - val_loss: 1.2781 - val_binary_accuracy: 0.6804\n",
      "Epoch 10/120\n",
      "2213733/2213733 [==============================] - 185s 84us/step - loss: 1.2786 - binary_accuracy: 0.6803 - val_loss: 1.2842 - val_binary_accuracy: 0.6789\n",
      "Epoch 11/120\n",
      "2213733/2213733 [==============================] - 186s 84us/step - loss: 1.2816 - binary_accuracy: 0.6796 - val_loss: 1.2810 - val_binary_accuracy: 0.6797\n",
      "Epoch 12/120\n",
      "2213733/2213733 [==============================] - 186s 84us/step - loss: 1.2765 - binary_accuracy: 0.6808 - val_loss: 1.2756 - val_binary_accuracy: 0.6811\n",
      "Epoch 13/120\n",
      "2213733/2213733 [==============================] - 186s 84us/step - loss: 1.2749 - binary_accuracy: 0.6812 - val_loss: 1.2689 - val_binary_accuracy: 0.6828\n",
      "Epoch 14/120\n",
      "2213733/2213733 [==============================] - 187s 84us/step - loss: 1.2675 - binary_accuracy: 0.6831 - val_loss: 1.2712 - val_binary_accuracy: 0.6822\n",
      "Epoch 15/120\n",
      "2213733/2213733 [==============================] - 186s 84us/step - loss: 1.2690 - binary_accuracy: 0.6827 - val_loss: 1.2671 - val_binary_accuracy: 0.6832\n",
      "Epoch 16/120\n",
      "2213733/2213733 [==============================] - 187s 84us/step - loss: 1.2636 - binary_accuracy: 0.6841 - val_loss: 1.2662 - val_binary_accuracy: 0.6834\n",
      "Epoch 17/120\n",
      "2213733/2213733 [==============================] - 185s 84us/step - loss: 1.2625 - binary_accuracy: 0.6843 - val_loss: 1.2638 - val_binary_accuracy: 0.6840\n",
      "Epoch 18/120\n",
      "2213733/2213733 [==============================] - 185s 84us/step - loss: 1.2582 - binary_accuracy: 0.6854 - val_loss: 1.2605 - val_binary_accuracy: 0.6849\n",
      "Epoch 19/120\n",
      "2213733/2213733 [==============================] - 186s 84us/step - loss: 1.2596 - binary_accuracy: 0.6851 - val_loss: 1.2625 - val_binary_accuracy: 0.6844\n",
      "Epoch 20/120\n",
      "2213733/2213733 [==============================] - 186s 84us/step - loss: 1.2550 - binary_accuracy: 0.6862 - val_loss: 1.2620 - val_binary_accuracy: 0.6845\n",
      "Epoch 21/120\n",
      "2213733/2213733 [==============================] - 188s 85us/step - loss: 1.2611 - binary_accuracy: 0.6847 - val_loss: 1.2613 - val_binary_accuracy: 0.6847\n",
      "Epoch 22/120\n",
      "2213733/2213733 [==============================] - 186s 84us/step - loss: 1.2626 - binary_accuracy: 0.6843 - val_loss: 1.2576 - val_binary_accuracy: 0.6856\n",
      "Epoch 23/120\n",
      "2213733/2213733 [==============================] - 188s 85us/step - loss: 1.2556 - binary_accuracy: 0.6861 - val_loss: 1.2600 - val_binary_accuracy: 0.6850\n",
      "Epoch 24/120\n",
      "2213733/2213733 [==============================] - 190s 86us/step - loss: 1.2568 - binary_accuracy: 0.6858 - val_loss: 1.2537 - val_binary_accuracy: 0.6865\n",
      "Epoch 25/120\n",
      "2213733/2213733 [==============================] - 188s 85us/step - loss: 1.2525 - binary_accuracy: 0.6868 - val_loss: 1.2571 - val_binary_accuracy: 0.6857\n",
      "Epoch 26/120\n",
      "2213733/2213733 [==============================] - 190s 86us/step - loss: 1.2533 - binary_accuracy: 0.6866 - val_loss: 1.2548 - val_binary_accuracy: 0.6863\n",
      "Epoch 27/120\n",
      "2213733/2213733 [==============================] - 185s 84us/step - loss: 1.2520 - binary_accuracy: 0.6870 - val_loss: 1.2548 - val_binary_accuracy: 0.6863\n",
      "Epoch 28/120\n",
      "2213733/2213733 [==============================] - 186s 84us/step - loss: 1.2543 - binary_accuracy: 0.6864 - val_loss: 1.2551 - val_binary_accuracy: 0.6862\n",
      "Epoch 29/120\n",
      "2213733/2213733 [==============================] - 185s 84us/step - loss: 1.2470 - binary_accuracy: 0.6882 - val_loss: 1.2539 - val_binary_accuracy: 0.6865\n",
      "Epoch 30/120\n",
      "2213733/2213733 [==============================] - 186s 84us/step - loss: 1.2515 - binary_accuracy: 0.6871 - val_loss: 1.2532 - val_binary_accuracy: 0.6867\n",
      "Epoch 31/120\n",
      "2213733/2213733 [==============================] - 187s 84us/step - loss: 1.2496 - binary_accuracy: 0.6876 - val_loss: 1.2500 - val_binary_accuracy: 0.6875\n",
      "Epoch 32/120\n",
      "2213733/2213733 [==============================] - 187s 85us/step - loss: 1.2482 - binary_accuracy: 0.6879 - val_loss: 1.2486 - val_binary_accuracy: 0.6878\n",
      "Epoch 33/120\n",
      "2213733/2213733 [==============================] - 189s 85us/step - loss: 1.2494 - binary_accuracy: 0.6876 - val_loss: 1.2452 - val_binary_accuracy: 0.6887\n",
      "Epoch 34/120\n",
      "2213733/2213733 [==============================] - 187s 84us/step - loss: 1.2432 - binary_accuracy: 0.6892 - val_loss: 1.2477 - val_binary_accuracy: 0.6881\n",
      "Epoch 35/120\n",
      "2213733/2213733 [==============================] - 186s 84us/step - loss: 1.2453 - binary_accuracy: 0.6886 - val_loss: 1.2483 - val_binary_accuracy: 0.6879\n",
      "Epoch 36/120\n",
      "2213733/2213733 [==============================] - 187s 84us/step - loss: 1.2438 - binary_accuracy: 0.6890 - val_loss: 1.2473 - val_binary_accuracy: 0.6882\n",
      "Epoch 37/120\n",
      "2213733/2213733 [==============================] - 186s 84us/step - loss: 1.2472 - binary_accuracy: 0.6882 - val_loss: 1.2433 - val_binary_accuracy: 0.6891\n",
      "Epoch 38/120\n",
      "2213733/2213733 [==============================] - 188s 85us/step - loss: 1.2430 - binary_accuracy: 0.6892 - val_loss: 1.2454 - val_binary_accuracy: 0.6886\n",
      "Epoch 39/120\n",
      "2213733/2213733 [==============================] - 185s 84us/step - loss: 1.2434 - binary_accuracy: 0.6891 - val_loss: 1.2413 - val_binary_accuracy: 0.6896\n",
      "Epoch 40/120\n",
      "2213733/2213733 [==============================] - 188s 85us/step - loss: 1.2420 - binary_accuracy: 0.6895 - val_loss: 1.2412 - val_binary_accuracy: 0.6897\n",
      "Epoch 41/120\n",
      "2213733/2213733 [==============================] - 185s 84us/step - loss: 1.2384 - binary_accuracy: 0.6904 - val_loss: 1.2400 - val_binary_accuracy: 0.6900\n",
      "Epoch 42/120\n",
      "2213733/2213733 [==============================] - 185s 84us/step - loss: 1.2397 - binary_accuracy: 0.6900 - val_loss: 1.2430 - val_binary_accuracy: 0.6892\n",
      "Epoch 43/120\n",
      "2213733/2213733 [==============================] - 191s 86us/step - loss: 1.2417 - binary_accuracy: 0.6895 - val_loss: 1.2443 - val_binary_accuracy: 0.6889\n",
      "Epoch 44/120\n",
      "2213733/2213733 [==============================] - 189s 85us/step - loss: 1.2412 - binary_accuracy: 0.6897 - val_loss: 1.2415 - val_binary_accuracy: 0.6896\n",
      "Epoch 45/120\n",
      "2213733/2213733 [==============================] - 201s 91us/step - loss: 1.2397 - binary_accuracy: 0.6900 - val_loss: 1.2382 - val_binary_accuracy: 0.6904\n",
      "Epoch 46/120\n",
      "2213733/2213733 [==============================] - 196s 88us/step - loss: 1.2402 - binary_accuracy: 0.6899 - val_loss: 1.2376 - val_binary_accuracy: 0.6906\n",
      "Epoch 47/120\n",
      "2213733/2213733 [==============================] - 233s 105us/step - loss: 1.2360 - binary_accuracy: 0.6910 - val_loss: 1.2381 - val_binary_accuracy: 0.6904\n",
      "Epoch 48/120\n",
      "2213733/2213733 [==============================] - 205s 93us/step - loss: 1.2227 - binary_accuracy: 0.6943 - val_loss: 1.2268 - val_binary_accuracy: 0.6933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/120\n",
      "2213733/2213733 [==============================] - 208s 94us/step - loss: 1.2243 - binary_accuracy: 0.6939 - val_loss: 1.2189 - val_binary_accuracy: 0.6952\n",
      "Epoch 50/120\n",
      "2213733/2213733 [==============================] - 197s 89us/step - loss: 1.2170 - binary_accuracy: 0.6957 - val_loss: 1.2168 - val_binary_accuracy: 0.6958\n",
      "Epoch 51/120\n",
      "2213733/2213733 [==============================] - 210s 95us/step - loss: 1.2279 - binary_accuracy: 0.6930 - val_loss: 1.2321 - val_binary_accuracy: 0.6919\n",
      "Epoch 52/120\n",
      "2213733/2213733 [==============================] - 200s 90us/step - loss: 1.2167 - binary_accuracy: 0.6958 - val_loss: 1.2167 - val_binary_accuracy: 0.6958\n",
      "Epoch 53/120\n",
      "2213733/2213733 [==============================] - 187s 85us/step - loss: 1.2168 - binary_accuracy: 0.6958 - val_loss: 1.2176 - val_binary_accuracy: 0.6956\n",
      "Epoch 54/120\n",
      "2213733/2213733 [==============================] - 188s 85us/step - loss: 1.2161 - binary_accuracy: 0.6960 - val_loss: 1.2144 - val_binary_accuracy: 0.6964\n",
      "Epoch 55/120\n",
      "2213733/2213733 [==============================] - 186s 84us/step - loss: 1.2131 - binary_accuracy: 0.6967 - val_loss: 1.2187 - val_binary_accuracy: 0.6953\n",
      "Epoch 56/120\n",
      "2213733/2213733 [==============================] - 188s 85us/step - loss: 1.2200 - binary_accuracy: 0.6950 - val_loss: 1.2157 - val_binary_accuracy: 0.6961\n",
      "Epoch 57/120\n",
      "2213733/2213733 [==============================] - 187s 84us/step - loss: 1.2165 - binary_accuracy: 0.6959 - val_loss: 1.2107 - val_binary_accuracy: 0.6973\n",
      "Epoch 58/120\n",
      "2213733/2213733 [==============================] - 187s 84us/step - loss: 1.2089 - binary_accuracy: 0.6978 - val_loss: 1.2165 - val_binary_accuracy: 0.6959\n",
      "Epoch 59/120\n",
      "2213733/2213733 [==============================] - 184s 83us/step - loss: 1.2121 - binary_accuracy: 0.6970 - val_loss: 1.2119 - val_binary_accuracy: 0.6970\n",
      "Epoch 60/120\n",
      "2213733/2213733 [==============================] - 187s 85us/step - loss: 1.2099 - binary_accuracy: 0.6975 - val_loss: 1.2145 - val_binary_accuracy: 0.6964\n",
      "Epoch 61/120\n",
      " 550000/2213733 [======>.......................] - ETA: 1:54 - loss: 1.3084 - binary_accuracy: 0.6729"
     ]
    }
   ],
   "source": [
    "if doTrain:\n",
    "    model,nparray_accuracyBinary_Train,nparray_accuracyBinary_Test,nparray_loss_Train,nparray_loss_Test=train_model(model,modelName=modelName,numberOfEpochs=numberOfEpochs,batchSize=batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model,modelName,numberOfEpochs,batchSize):\n",
    "    print(\"*** Start load_model for modeName\",modelName,\"***\")\n",
    "    # load the loss, accuracies, weights + biases of the trained model to a file\n",
    "    \n",
    "    # create the name stem, specific for this training\n",
    "    outputFileNameStem=\"NN_3_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    \n",
    "    # create the file name for the weights and biases\n",
    "    outputFileNameWeights=outputFolderName+\"/\"+outputFileNameStem+\"_weights.hdf5\"\n",
    "    \n",
    "    # load the weights and biases\n",
    "    model.load_weights(outputFileNameWeights)\n",
    "\n",
    "    # ready to return\n",
    "    return model\n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doLoadModel:\n",
    "    model=load_model(model,modelName,numberOfEpochs,batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metrics(modelName,numberOfEpochs,batchSize):\n",
    "    print(\"*** Start load_metrics for modeName\",modelName,\"***\")\n",
    "    # load the loss, accuracies, weights + biases of the trained model to a file\n",
    "    \n",
    "    # create the name stem, specific for this training\n",
    "    outputFileNameStem=\"NN_3_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    \n",
    "    # create the file names for accuracy and loss, train and test\n",
    "    outputFileNameAccuracyBinaryTrain=outputFolderName+\"/\"+outputFileNameStem+\"_accuracyBinary_Train.npy\"\n",
    "    outputFileNameAccuracyBinaryTest=outputFolderName+\"/\"+outputFileNameStem+\"_accuracyBinary_Test.npy\"\n",
    "    outputFileNameLossTrain=outputFolderName+\"/\"+outputFileNameStem+\"_loss_Train.npy\"\n",
    "    outputFileNameLossTest=outputFolderName+\"/\"+outputFileNameStem+\"_loss_Test.npy\"\n",
    "    \n",
    "    # retrieve from it the accuracy, loss, train, test\n",
    "    nparray_accuracyBinary_Train=np.load(outputFileNameAccuracyBinaryTrain)\n",
    "    nparray_accuracyBinary_Test=np.load(outputFileNameAccuracyBinaryTest)\n",
    "    nparray_loss_Train=np.load(outputFileNameLossTrain)\n",
    "    nparray_loss_Test=np.load(outputFileNameLossTest)\n",
    "\n",
    "    # ready to return\n",
    "    return (nparray_accuracyBinary_Train,nparray_accuracyBinary_Test,nparray_loss_Train,nparray_loss_Test)\n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doLoadMetrics:\n",
    "    nparray_accuracyBinary_Train,nparray_accuracyBinary_Test,nparray_loss_Train,nparray_loss_Test=load_metrics(modelName=modelName,numberOfEpochs=numberOfEpochs,batchSize=batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay accuracy train and test\n",
    "def plot_accuracy_Train_vs_Test(nparray_accuracyBinary_Train,nparray_accuracyBinary_Test,modelName):\n",
    "    plt.plot(nparray_accuracyBinary_Train)\n",
    "    plt.plot(nparray_accuracyBinary_Test)\n",
    "    plt.title('Model_'+modelName+' accuracy')\n",
    "    plt.ylabel('Binary accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    #plt.xlim(left=0,right=10)\n",
    "    # plt.ylim(bottom=0,top=0.8)\n",
    "    #plt.ylim(bottom=0.75,top=0.80)\n",
    "    plt.legend(['Train', 'Test'], loc='best')\n",
    "    # plt.show()\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_graph_accuracy.\"+extension)\n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doPlotMetrics:\n",
    "    plot_accuracy_Train_vs_Test(nparray_accuracyBinary_Train,nparray_accuracyBinary_Test,\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay loss train and test\n",
    "def plot_loss_Train_vs_Test(nparray_loss_Train,nparray_loss_Test,modelName):\n",
    "    plt.plot(nparray_loss_Train)\n",
    "    plt.plot(nparray_loss_Test)\n",
    "    plt.title('Model_'+modelName+' loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train','Test'],loc=\"upper left\")\n",
    "    #plt.show()\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_graph_loss.\"+extension)\n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doPlotMetrics:\n",
    "    plot_loss_Train_vs_Test(nparray_loss_Train,nparray_loss_Test,\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_model(model,nparray_Input_Train,nparray_Input_Test,nparray_Output_Train,nparray_Output_Test):\n",
    "    outputFileNameStem=\"NN_4_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    \n",
    "    # Train\n",
    "    nparray_PredictedOutput_Train=model.predict(nparray_Input_Train)\n",
    "    p(\"PredictedOutput_Train\",nparray_PredictedOutput_Train)\n",
    "    p(\"Output_Train\",nparray_Output_Train)\n",
    "    nparray_Diff_Train=nparray_PredictedOutput_Train-nparray_Output_Train\n",
    "    p(\"Diff_Train\",nparray_Diff_Train)\n",
    "    # save numpy arrays to npy files\n",
    "    outputFileNamePredictedOutput_Train=outputFolderName+\"/\"+outputFileNameStem+\"_PredictedOutput_Train.npy\"\n",
    "    np.save(outputFileNamePredictedOutput_Train,nparray_PredictedOutput_Train)\n",
    "    outputFileNameDiff_Train=outputFolderName+\"/\"+outputFileNameStem+\"_Diff_Train.npy\"\n",
    "    np.save(outputFileNameDiff_Train,nparray_Diff_Train)\n",
    "    \n",
    "    # Test\n",
    "    nparray_PredictedOutput_Test=model.predict(nparray_Input_Test)\n",
    "    p(\"PredictedOutput_Test\",nparray_PredictedOutput_Test)\n",
    "    p(\"Output_Test\",nparray_Output_Test)\n",
    "    nparray_Diff_Test=nparray_PredictedOutput_Test-nparray_Output_Test\n",
    "    p(\"Diff_Test\",nparray_Diff_Test)\n",
    "    # save numpy arrays to npy files\n",
    "    outputFileNamePredictedOutput_Test=outputFolderName+\"/\"+outputFileNameStem+\"_PredictedOutput_Test.npy\"\n",
    "    np.save(outputFileNamePredictedOutput_Test,nparray_PredictedOutput_Test)\n",
    "    outputFileNameDiff_Test=outputFolderName+\"/\"+outputFileNameStem+\"_Diff_Test.npy\"\n",
    "    np.save(outputFileNameDiff_Test,nparray_Diff_Test)\n",
    "    \n",
    "# done function    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doPredict:\n",
    "    predict_from_model(model,nparray_Input_Train,nparray_Input_Test,nparray_Output_Train,nparray_Output_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predicted():\n",
    "    outputFileNameStem=\"NN_4_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    \n",
    "    # Train\n",
    "    outputFileNamePredictedOutput_Train=outputFolderName+\"/\"+outputFileNameStem+\"_PredictedOutput_Train.npy\"\n",
    "    nparray_PredictedOutput_Train=np.load(outputFileNamePredictedOutput_Train)\n",
    "    outputFileNameDiff_Train=outputFolderName+\"/\"+outputFileNameStem+\"_Diff_Train.npy\"\n",
    "    nparray_Diff_Train=np.load(outputFileNameDiff_Train)\n",
    "    \n",
    "    # Test\n",
    "    outputFileNamePredictedOutput_Test=outputFolderName+\"/\"+outputFileNameStem+\"_PredictedOutput_Test.npy\"\n",
    "    nparray_PredictedOutput_Test=np.load(outputFileNamePredictedOutput_Test)\n",
    "    outputFileNameDiff_Test=outputFolderName+\"/\"+outputFileNameStem+\"_Diff_Test.npy\"\n",
    "    nparray_Diff_Test=np.load(outputFileNameDiff_Test)\n",
    "    \n",
    "    return nparray_PredictedOutput_Train, nparray_Diff_Train, nparray_PredictedOutput_Test, nparray_Diff_Test\n",
    "# done function   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doLoadPredict:\n",
    "    nparray_PredictedOutput_Train, nparray_Diff_Train, nparray_PredictedOutput_Test, nparray_Diff_Test=load_predicted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics2(TrainOrTest, nparray_Output, nparray_PredictedOutput, nparray_VolumeID):\n",
    "    #p(\"nparray_Output\",nparray_Output)\n",
    "    #p(\"nparray_PredictedOutput\",nparray_PredictedOutput)\n",
    "    \n",
    "    # for loop over i (buckets)\n",
    "    # each of this for each bucket have only one value\n",
    "    list_bucket_OutputPositive=[]\n",
    "    list_bucket_OutputNegative=[]\n",
    "    list_bucket_PredictedOutputPositive=[]\n",
    "    list_bucket_PredictedOutputNegative=[]\n",
    "    list_bucket_TruePositive=[]\n",
    "    list_bucket_FalsePositive=[]\n",
    "    list_bucket_FalseNegative=[]\n",
    "    list_bucket_TrueNegative=[]\n",
    "    list_bucket_accuracy=[]\n",
    "    list_bucket_precision=[]\n",
    "    list_bucket_recall=[]\n",
    "    list_bucket_negativePredictedValue=[]\n",
    "    list_bucket_trueNegativeRate=[]\n",
    "    # store for each bucket four values, from which all else can be computed again (TP, FP, FN, TN)\n",
    "    #list_bucket_MetricBasic=[]\n",
    "    \n",
    "    # for each VolumeID we sum all the buckets in that VolumeID to get the total TP,FP,FN,TN in the bucket\n",
    "    # first create for each VolumeID a numpy array of MetricBasic with 4 values set to zero\n",
    "    dict_VolumeID_MetricBasic={}            \n",
    "    \n",
    "    # loop over all the buckets\n",
    "    nbBucketTotal=len(nparray_Output)\n",
    "    for i in range(nbBucketTotal):\n",
    "        if i%100000==0:\n",
    "            print(TrainOrTest+\" bucket i\",i,\"/\",nbBucketTotal)\n",
    "        nparray_bucket_Output=nparray_Output[i]\n",
    "        nparray_bucket_PredictedOutput=nparray_PredictedOutput[i]\n",
    "        #p(\"nparray_bucket_Output\",nparray_bucket_Output)\n",
    "        #p(\"nparray_bucket_PredictedOutput\",nparray_bucket_PredictedOutput)\n",
    "        \n",
    "        # for loop over j (hit)\n",
    "        counter_hit_TP=0\n",
    "        counter_hit_FP=0\n",
    "        counter_hit_FN=0\n",
    "        counter_hit_TN=0\n",
    "        for j in range(len(nparray_bucket_Output)):\n",
    "            #print (\"hit j\",j)\n",
    "            hit_Output=nparray_bucket_Output[j]\n",
    "            hit_PredictedOutput=nparray_bucket_PredictedOutput[j]\n",
    "            #print(\"j\",j,\"hit_PredictedOutput - hit_Output\",hit_PredictedOutput, hit_Output)\n",
    "               \n",
    "            # confusion matrix\n",
    "            # TP FP\n",
    "            # FN TN\n",
    "            TP=0 # True Positive\n",
    "            FP=0 # False Positive (type I error)\n",
    "            FN=0 # False Negative (type II error)\n",
    "            TN=0 # True Negative\n",
    " \n",
    "            \n",
    "            \n",
    "            \n",
    "            # for this hit ask conditions\n",
    "            if hit_PredictedOutput>0:\n",
    "                # the value is predicted positive\n",
    "                if hit_Output>0:\n",
    "                    # the value is actually positive\n",
    "                    TP=1\n",
    "                else:\n",
    "                    # the value is actually negative \n",
    "                    FP=1\n",
    "                # done if \n",
    "            else:\n",
    "                # the value is predicted negative\n",
    "                if hit_Output>0:\n",
    "                    # the value is actually positive\n",
    "                    FN=1\n",
    "                else:\n",
    "                    # the value is actually negative\n",
    "                    TN=1\n",
    "                # done if\n",
    "            # done if \n",
    "            # for this hit only one of these four values is 1, the rest of three are zero\n",
    "            #print(\"i\",i,\"j\",j,\"TP\",TP,\"FP\",FP,\"FN\",FN,\"TN\",TN)\n",
    "            \n",
    "            # increment counters for hits\n",
    "            counter_hit_TP+=TP\n",
    "            counter_hit_FP+=FP\n",
    "            counter_hit_FN+=FN\n",
    "            counter_hit_TN+=TN  \n",
    "            \n",
    "            # put the 4 basic together to create the BasicMatric for this hit as a nparray\n",
    "            MetricBasic=np.array([TP,FP,FN,TN])\n",
    "            # add the basic metric for this hist to the dict_VolumeID_BasicMetric depending on the VolumeID of this hit\n",
    "            # this is for the i bucket and inside the j hit\n",
    "            VolumeID=nparray_VolumeID[i][j]\n",
    "            if VolumeID not in dict_VolumeID_MetricBasic.keys():\n",
    "                dict_VolumeID_MetricBasic[VolumeID]=MetricBasic\n",
    "            else:\n",
    "                dict_VolumeID_MetricBasic[VolumeID]+=MetricBasic\n",
    "        # done for loop over j (hit)\n",
    "        #print(\"i\",i,\"counter_hit_TP\",counter_hit_TP,\"counter_hit_FP\",counter_hit_FP,\"counter_hit_FN\",counter_hit_FN,\"counter_hit_TN\",counter_hit_TN)\n",
    "        \n",
    "        bucket_OutputPositive=counter_hit_TP+counter_hit_FN \n",
    "        bucket_OutputNegative=counter_hit_FP+counter_hit_TN\n",
    "        bucket_PredictedOutputPositive=counter_hit_TP+counter_hit_FP\n",
    "        bucket_PredictedOutputNegative=counter_hit_FN+counter_hit_TN\n",
    "        \n",
    "        #print(\"i\",i,\"bucket_OutputPositive\",bucket_OutputPositive)\n",
    "        #print(\"i\",i,\"bucket_OutputNegative\",bucket_OutputNegative)\n",
    "        #print(\"i\",i,\"bucket_PredictedOutputPositive\",bucket_PredictedOutputPositive)\n",
    "        #print(\"i\",i,\"bucket_PredictedOutputNegative\",bucket_PredictedOutputNegative)\n",
    "        \n",
    "        \n",
    "        bucket_TruePositive=counter_hit_TP\n",
    "        bucket_FalsePositive=counter_hit_FP\n",
    "        bucket_FalseNegative=counter_hit_FN\n",
    "        bucket_TrueNegative=counter_hit_TN\n",
    "\n",
    "        # accuracy=(TP+TN)/(TP+FP+FN+TN)=(TP+TN)/ALL, ALL=20 (20 hits in a bucket)\n",
    "        # precision=(TP)/(TP+FP)=(TP)/(all that are in reality positive)=efficiency from CERN \n",
    "        # e.g. there are 100 truth electrons, efficiency = what fraction of them are also reconstricted as electrons? \n",
    "        # recall=(TP)/(TP+FN)=(TP)/(all that are predicted positive) = one minus fake rate from CERN\n",
    "        # e.g. fake rate = I have reconstructed 100 electrons. What fraction of these are not in reality truth electrons\n",
    "        # fake rate = What fraction of reconstructed electrons are fake electrons?\n",
    "        # fake rate = (FN)/(TP+FN) = 1 - recall\n",
    "        \n",
    "        # accuracy\n",
    "        bucket_accuracy=(counter_hit_TP+counter_hit_TN)/(counter_hit_TP+counter_hit_FP+counter_hit_FN+counter_hit_TN)\n",
    "        \n",
    "        # https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "        \n",
    "        # precision\n",
    "        if (counter_hit_TP+counter_hit_FP)==0:\n",
    "            bucket_precision=0\n",
    "        else:\n",
    "            bucket_precision=(counter_hit_TP)/(counter_hit_TP+counter_hit_FP)\n",
    "        # done if\n",
    "        \n",
    "        # recall\n",
    "        if counter_hit_TP+counter_hit_FN==0:\n",
    "            bucket_recall=0\n",
    "        else:\n",
    "            bucket_recall=(counter_hit_TP)/(counter_hit_TP+counter_hit_FN)\n",
    "        # done if\n",
    "        #print(\"i\",i,\"bucket_accuracy\",bucket_accuracy,\"bucket_precision\",bucket_precision,\"bucket_recall\",bucket_recall)\n",
    "        \n",
    "        # Negative predicted values\n",
    "        if counter_hit_TN+counter_hit_FN==0:\n",
    "            bucket_negativePredictedValue=0\n",
    "        else:\n",
    "            bucket_negativePredictedValue=(counter_hit_TN)/(counter_hit_TN+counter_hit_FN)\n",
    "        # done if\n",
    "        \n",
    "        # True negative rate\n",
    "        if counter_hit_TN+counter_hit_FP==0:\n",
    "            bucket_trueNegativeRate=0\n",
    "        else:\n",
    "            bucket_trueNegativeRate=(counter_hit_TN)/(counter_hit_TN+counter_hit_FP)\n",
    "        # done if \n",
    "        \n",
    "        # the four basic metric froms which everythis calculated again are all integers with max value 20\n",
    "        # are put in a list and the list made a numpy array of positive integers uint8\n",
    "        #bucket_MetricBasic=np.array([counter_hit_TP,counter_hit_FP,counter_hit_FN,counter_hit_TN]).astype(np.uint8)\n",
    "        \n",
    "        \n",
    "        # add to current bucket to the list across all buckets\n",
    "        list_bucket_OutputPositive.append(bucket_OutputPositive)\n",
    "        list_bucket_OutputNegative.append(bucket_OutputNegative)\n",
    "        list_bucket_PredictedOutputPositive.append(bucket_PredictedOutputPositive)\n",
    "        list_bucket_PredictedOutputNegative.append(bucket_PredictedOutputNegative)\n",
    "        list_bucket_TruePositive.append(bucket_TruePositive)\n",
    "        list_bucket_FalsePositive.append(bucket_FalsePositive)\n",
    "        list_bucket_FalseNegative.append(bucket_FalseNegative)\n",
    "        list_bucket_TrueNegative.append(bucket_TrueNegative)\n",
    "        list_bucket_accuracy.append(bucket_accuracy)\n",
    "        list_bucket_precision.append(bucket_precision)\n",
    "        list_bucket_recall.append(bucket_recall)\n",
    "        list_bucket_negativePredictedValue.append(bucket_negativePredictedValue)\n",
    "        list_bucket_trueNegativeRate.append(bucket_trueNegativeRate)\n",
    "        #\n",
    "        #list_bucket_MetricBasic.append(bucket_MetricBasic)\n",
    "        \n",
    "    # done for loop over i (bucket)\n",
    "    \n",
    "    #print(\"list_bucket_accuracy\",list_bucket_accuracy)\n",
    "    #print(\"list_bucket_precision\",list_bucket_precision)\n",
    "    #print(\"list_bucket_recall\",list_bucket_recall)\n",
    "    \n",
    "    # convert list to numpy array\n",
    "    nparray_bucket_OutputPositive=np.array(list_bucket_OutputPositive)\n",
    "    nparray_bucket_OutputNegative=np.array(list_bucket_OutputNegative)\n",
    "    nparray_bucket_PredictedOutputPositive=np.array(list_bucket_PredictedOutputPositive)\n",
    "    nparray_bucket_PredictedOutputNegative=np.array(list_bucket_PredictedOutputNegative)\n",
    "    nparray_bucket_TruePositive=np.array(list_bucket_TruePositive)\n",
    "    nparray_bucket_FalsePositive=np.array(list_bucket_FalsePositive)\n",
    "    nparray_bucket_FalseNegative=np.array(list_bucket_FalseNegative)\n",
    "    nparray_bucket_TrueNegative=np.array(list_bucket_TrueNegative)\n",
    "    nparray_bucket_accuracy=np.array(list_bucket_accuracy)\n",
    "    nparray_bucket_precision=np.array(list_bucket_precision)\n",
    "    nparray_bucket_recall=np.array(list_bucket_recall)\n",
    "    nparray_bucket_negativePredictedValue=np.array(list_bucket_negativePredictedValue)\n",
    "    nparray_bucket_trueNegativeRate=np.array(list_bucket_trueNegativeRate)\n",
    "    #\n",
    "    #nparray_bucket_MetricBasic=np.array(list_bucket_MetricBasic)\n",
    "    \n",
    "    p(\"nparray_bucket_OutputPositive\",nparray_bucket_OutputPositive)\n",
    "    p(\"nparray_bucket_OutputNegative\",nparray_bucket_OutputNegative)\n",
    "    p(\"nparray_bucket_PredictedOutputPositive\",nparray_bucket_PredictedOutputPositive)\n",
    "    p(\"nparray_bucket_PredictedOutputNegative\",nparray_bucket_PredictedOutputNegative)\n",
    "    p(\"nparray_bucket_TruePositive\",nparray_bucket_TruePositive)\n",
    "    p(\"nparray_bucket_FalsePositive\",nparray_bucket_FalsePositive)\n",
    "    p(\"nparray_bucket_FalseNegative\",nparray_bucket_FalseNegative)\n",
    "    p(\"nparray_bucket_TrueNegative\",nparray_bucket_TrueNegative)\n",
    "    p(\"nparray_bucket_accuracy\",nparray_bucket_accuracy)\n",
    "    p(\"nparray_bucket_precision\",nparray_bucket_precision)\n",
    "    p(\"nparray_bucket_recall\",nparray_bucket_recall)\n",
    "    p(\"nparray_bucket_negativePredictedValue\",nparray_bucket_negativePredictedValue)\n",
    "    p(\"nparray_bucket_trueNegativeRate\",nparray_bucket_trueNegativeRate)\n",
    "    #\n",
    "    #p(\"nparray_bucket_MetricBasic\",nparray_bucket_MetricBasic)\n",
    "    \n",
    "    # save numpy arrays to file, first create the common part of the name based on the current model\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    \n",
    "    # create the name of each numpy array and save it\n",
    "    outputFileName_OutputPositive=outputFolderName+\"/\"+outputFileNameStem+\"_OutputPositive_\"+TrainOrTest+\".npy\"\n",
    "    np.save(outputFileName_OutputPositive,nparray_bucket_OutputPositive)\n",
    "    outputFileName_OutputNegative=outputFolderName+\"/\"+outputFileNameStem+\"_OutputNegative_\"+TrainOrTest+\".npy\"\n",
    "    np.save(outputFileName_OutputNegative,nparray_bucket_OutputNegative)\n",
    "    outputFileName_OutputNegative=outputFolderName+\"/\"+outputFileNameStem+\"_OutputNegative_\"+TrainOrTest+\".npy\"\n",
    "    np.save(outputFileName_OutputNegative,nparray_bucket_OutputNegative)\n",
    "    outputFileName_PredictedOutputPositive=outputFolderName+\"/\"+outputFileNameStem+\"_PredictedOutputPositive_\"+TrainOrTest+\".npy\"\n",
    "    np.save(outputFileName_PredictedOutputPositive,nparray_bucket_PredictedOutputPositive)\n",
    "    outputFileName_PredictedOutputNegative=outputFolderName+\"/\"+outputFileNameStem+\"_PredictedOutputNegative_\"+TrainOrTest+\".npy\"\n",
    "    np.save(outputFileName_PredictedOutputNegative,nparray_bucket_PredictedOutputNegative)\n",
    "    outputFileName_TruePositive=outputFolderName+\"/\"+outputFileNameStem+\"_TruePositive_\"+TrainOrTest+\".npy\"\n",
    "    np.save(outputFileName_TruePositive,nparray_bucket_TruePositive)\n",
    "    outputFileName_FalsePositive=outputFolderName+\"/\"+outputFileNameStem+\"_FalsePositive_\"+TrainOrTest+\".npy\"\n",
    "    np.save(outputFileName_FalsePositive,nparray_bucket_FalsePositive)\n",
    "    outputFileName_FalseNegative=outputFolderName+\"/\"+outputFileNameStem+\"_FalseNegative_\"+TrainOrTest+\".npy\"\n",
    "    np.save(outputFileName_FalseNegative,nparray_bucket_FalseNegative)\n",
    "    outputFileName_TrueNegative=outputFolderName+\"/\"+outputFileNameStem+\"_TrueNegative_\"+TrainOrTest+\".npy\"\n",
    "    np.save(outputFileName_TrueNegative,nparray_bucket_TrueNegative)\n",
    "    outputFileName_accuracy=outputFolderName+\"/\"+outputFileNameStem+\"_Accuracy_\"+TrainOrTest+\".npy\"\n",
    "    np.save(outputFileName_accuracy,nparray_bucket_accuracy)\n",
    "    outputFileName_precision=outputFolderName+\"/\"+outputFileNameStem+\"_Precision_\"+TrainOrTest+\".npy\"\n",
    "    np.save(outputFileName_precision,nparray_bucket_precision)\n",
    "    outputFileName_recall=outputFolderName+\"/\"+outputFileNameStem+\"_Recall_\"+TrainOrTest+\".npy\"\n",
    "    np.save(outputFileName_recall,nparray_bucket_recall)\n",
    "    outputFileName_negativePredictedValue=outputFolderName+\"/\"+outputFileNameStem+\"_NegativePredictedValue_\"+TrainOrTest+\".npy\"\n",
    "    np.save(outputFileName_negativePredictedValue,nparray_bucket_negativePredictedValue)\n",
    "    outputFileName_trueNegativeRate=outputFolderName+\"/\"+outputFileNameStem+\"_TrueNegativeRate_\"+TrainOrTest+\".npy\"\n",
    "    np.save(outputFileName_trueNegativeRate,nparray_bucket_trueNegativeRate)\n",
    "    #\n",
    "    #outputFileName_MetricBasic=outputFolderName+\"/\"+outputFileNameStem+\"_MetricBasic_\"+TrainOrTest+\".npy\"\n",
    "    #np.save(outputFileName_MetricBasic,nparray_bucket_MetricBasic)\n",
    "    \n",
    "    # done all, ready to return\n",
    "    return nparray_bucket_OutputPositive,nparray_bucket_OutputNegative,nparray_bucket_PredictedOutputPositive,nparray_bucket_PredictedOutputNegative,nparray_bucket_TruePositive,nparray_bucket_FalsePositive,nparray_bucket_FalseNegative,nparray_bucket_TrueNegative,nparray_bucket_accuracy,nparray_bucket_precision,nparray_bucket_recall,nparray_bucket_negativePredictedValue,nparray_bucket_trueNegativeRate,dict_VolumeID_MetricBasic\n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doCalculateMetrics2:\n",
    "    print(\"\")\n",
    "    print(\"Train\")\n",
    "    nparray_bucket_OutputPositive_Train,nparray_bucket_OutputNegative_Train,nparray_bucket_PredictedOutputPositive_Train,nparray_bucket_PredictedOutputNegative_Train,nparray_bucket_TruePositive_Train,nparray_bucket_FalsePositive_Train,nparray_bucket_FalseNegative_Train,nparray_bucket_TrueNegative_Train,nparray_bucket_accuracy_Train,nparray_bucket_precision_Train,nparray_bucket_recall_Train,nparray_bucket_negativePredictedValue_Train,nparray_bucket_trueNegativeRate_Train,dict_VolumeID_MetricBasic_Train=calculate_metrics2(\"Train\", nparray_Output_Train, nparray_PredictedOutput_Train, nparray_VolumeID_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doCalculateMetrics2:\n",
    "    print(\"\")\n",
    "    print(\"Test\")\n",
    "    nparray_bucket_OutputPositive_Test,nparray_bucket_OutputNegative_Test,nparray_bucket_PredictedOutputPositive_Test,nparray_bucket_PredictedOutputNegative_Test,nparray_bucket_TruePositive_Test,nparray_bucket_FalsePositive_Test,nparray_bucket_FalseNegative_Test,nparray_bucket_TrueNegative_Test,nparray_bucket_accuracy_Test,nparray_bucket_precision_Test,nparray_bucket_recall_Test,nparray_bucket_negativePredictedValue_Test,nparray_bucket_trueNegativeRate_Test,dict_VolumeID_MetricBasic_Test=calculate_metrics2(\"Test\", nparray_Output_Test, nparray_PredictedOutput_Test, nparray_VolumeID_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# overlay train and test we expect train to be better than test \n",
    "def plot_train_test_OutputPositive(nparray_bucket_OutputPositive_Train,nparray_bucket_OutputPositive_Test,modelName):\n",
    "    plt.hist(nparray_bucket_OutputPositive_Train,bins=[i for i in range(22)],density=True,alpha=1,color=\"blue\",histtype='step',label=\"Train\")\n",
    "    plt.hist(nparray_bucket_OutputPositive_Test,bins=[i for i in range(22)],density=True,alpha=1,color=\"red\",histtype='step',label=\"Test\") \n",
    "    plt.ylabel('Nr of buckets')\n",
    "    plt.xlabel('Bucket OutputPositive')\n",
    "    plt.title(modelName)\n",
    "    plt.legend(loc='upper left')\n",
    "    #plt.ylim(bottom=0,top=250)\n",
    "    #plt.show()\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_OutputPositive.\"+extension)\n",
    "# done function\n",
    "if doPlotMetrics2:\n",
    "    plot_train_test_OutputPositive(nparray_bucket_OutputPositive_Train,nparray_bucket_OutputPositive_Test,modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay train and test we expect train to be better than test \n",
    "def plot_train_test_OutputNegative(nparray_bucket_OutputNegative_Train,nparray_bucket_OutputNegative_Test,modelName):\n",
    "    plt.hist(nparray_bucket_OutputNegative_Train,bins=[i for i in range(22)],density=True,alpha=1,color=\"blue\",histtype='step',label=\"Train\")\n",
    "    plt.hist(nparray_bucket_OutputNegative_Test,bins=[i for i in range(22)],density=True,alpha=1,color=\"red\",histtype='step',label=\"Test\")\n",
    "    plt.ylabel('Nr of buckets')\n",
    "    plt.xlabel('Bucket OutputNegative')\n",
    "    plt.title(modelName)\n",
    "    plt.legend(loc='upper left')\n",
    "    #plt.ylim(bottom=0,top=250)\n",
    "    #plt.show()\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_OutputNegative.\"+extension)\n",
    "# done function\n",
    "if doPlotMetrics2:\n",
    "    plot_train_test_OutputNegative(nparray_bucket_OutputNegative_Train,nparray_bucket_OutputNegative_Test,modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay train and test we expect train to be better than test \n",
    "def plot_train_test_PredictedOutputPositive(nparray_bucket_PredictedOutputPositive_Train,nparray_bucket_PredictedOutputPositive_Test,modelName):\n",
    "    plt.hist(nparray_bucket_PredictedOutputPositive_Train,bins=[i for i in range(22)],density=True,alpha=1,color=\"blue\",histtype='step',label=\"Train\")\n",
    "    plt.hist(nparray_bucket_PredictedOutputPositive_Test,bins=[i for i in range(22)],density=True,alpha=1,color=\"red\",histtype='step',label=\"Test\")\n",
    "    plt.ylabel('Nr of buckets')\n",
    "    plt.xlabel('Bucket PredictedOutputPositive')\n",
    "    plt.title(modelName)\n",
    "    plt.legend(loc='upper left')\n",
    "    #plt.ylim(bottom=0,top=250)\n",
    "    #plt.show()\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_PredictedOutputPositive.\"+extension)\n",
    "# done function\n",
    "if doPlotMetrics2:\n",
    "    plot_train_test_PredictedOutputPositive(nparray_bucket_PredictedOutputPositive_Train,nparray_bucket_PredictedOutputPositive_Test,modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay train and test we expect train to be better than test \n",
    "def plot_train_test_PredictedOutputNegative(nparray_bucket_PredictedOutputNegative_Train,nparray_bucket_PredictedOutputNegative_Test,modelName):\n",
    "    plt.hist(nparray_bucket_PredictedOutputNegative_Train,bins=[i for i in range(22)],density=True,alpha=1,color=\"blue\",histtype='step',label=\"Train\")\n",
    "    plt.hist(nparray_bucket_PredictedOutputNegative_Test,bins=[i for i in range(22)],density=True,alpha=1,color=\"red\",histtype='step',label=\"Test\")\n",
    "    plt.ylabel('Nr of buckets')\n",
    "    plt.xlabel('Bucket PredictedOutputNegative')\n",
    "    plt.title(modelName)\n",
    "    plt.legend(loc='upper left')\n",
    "    #plt.ylim(bottom=0,top=250)\n",
    "    #plt.show()\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_PredictedOutputNegative.\"+extension)\n",
    "# done function\n",
    "if doPlotMetrics2:\n",
    "    plot_train_test_PredictedOutputNegative(nparray_bucket_PredictedOutputNegative_Train,nparray_bucket_PredictedOutputNegative_Test,modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay train and test we expect train to be better than test \n",
    "def plot_train_test_TruePositive(nparray_bucket_TruePositive_Train,nparray_bucket_TruePositive_Test,modelName):\n",
    "    plt.hist(nparray_bucket_TruePositive_Train,bins=[i for i in range(22)],density=True,alpha=1,color=\"blue\",histtype='step',label=\"Train\")\n",
    "    plt.hist(nparray_bucket_TruePositive_Test,bins=[i for i in range(22)],density=True,alpha=1,color=\"red\",histtype='step',label=\"Test\")\n",
    "    plt.ylabel('Nr of buckets')\n",
    "    plt.xlabel('Bucket TruePositive')\n",
    "    plt.title(modelName)\n",
    "    plt.legend(loc='upper left')\n",
    "    #plt.ylim(bottom=0,top=250)\n",
    "    #plt.show()\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_TruePositive.\"+extension)\n",
    "# done function\n",
    "if doPlotMetrics2:\n",
    "    plot_train_test_TruePositive(nparray_bucket_TruePositive_Train,nparray_bucket_TruePositive_Test,modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay train and test we expect train to be better than test \n",
    "def plot_train_test_FalsePositive(nparray_bucket_FalsePositive_Train,nparray_bucket_FalsePositive_Test,modelName):\n",
    "    plt.hist(nparray_bucket_FalsePositive_Train,bins=[i for i in range(22)],density=True,alpha=1,color=\"blue\",histtype='step',label=\"Train\")\n",
    "    plt.hist(nparray_bucket_FalsePositive_Test,bins=[i for i in range(22)],density=True,alpha=1,color=\"red\",histtype='step',label=\"Test\")\n",
    "    plt.ylabel('Nr of buckets')\n",
    "    plt.xlabel('Bucket FalsePositive')\n",
    "    plt.title(modelName)\n",
    "    plt.legend(loc='upper left')\n",
    "    #plt.ylim(bottom=0,top=250)\n",
    "    #plt.show()\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_FalsePositive.\"+extension)\n",
    "# done function\n",
    "if doPlotMetrics2:\n",
    "    plot_train_test_FalsePositive(nparray_bucket_FalsePositive_Train,nparray_bucket_FalsePositive_Test,modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay train and test we expect train to be better than test \n",
    "def plot_train_test_FalseNegative(nparray_bucket_FalseNegative_Train,nparray_bucket_FalseNegative_Test,modelName):\n",
    "    plt.hist(nparray_bucket_FalseNegative_Train,bins=[i for i in range(22)],density=True,alpha=1,color=\"blue\",histtype='step',label=\"Train\")\n",
    "    plt.hist(nparray_bucket_FalseNegative_Test,bins=[i for i in range(22)],density=True,alpha=1,color=\"red\",histtype='step',label=\"Test\")\n",
    "    plt.ylabel('Nr of buckets')\n",
    "    plt.xlabel('Bucket FalseNegative')\n",
    "    plt.title(modelName)\n",
    "    plt.legend(loc='upper left')\n",
    "    #plt.ylim(bottom=0,top=250)\n",
    "    #plt.show()\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_FalseNegative.\"+extension)\n",
    "# done function\n",
    "if doPlotMetrics2:\n",
    "    plot_train_test_FalseNegative(nparray_bucket_FalseNegative_Train,nparray_bucket_FalseNegative_Test,modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay train and test we expect train to be better than test \n",
    "def plot_train_test_TrueNegative(nparray_bucket_TrueNegative_Train,nparray_bucket_TrueNegative_Test,modelName):\n",
    "    plt.hist(nparray_bucket_TrueNegative_Train,bins=[i for i in range(22)],density=True,alpha=1,color=\"blue\",histtype='step',label=\"Train\")\n",
    "    plt.hist(nparray_bucket_TrueNegative_Test,bins=[i for i in range(22)],density=True,alpha=1,color=\"red\",histtype='step',label=\"Test\")\n",
    "    plt.ylabel('Nr of buckets')\n",
    "    plt.xlabel('Bucket TrueNegative')\n",
    "    plt.title(modelName)\n",
    "    plt.legend(loc='upper left')\n",
    "    #plt.ylim(bottom=0,top=250)\n",
    "    #plt.show()\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_TrueNegative.\"+extension)\n",
    "# done function\n",
    "if doPlotMetrics2:\n",
    "    plot_train_test_TrueNegative(nparray_bucket_TrueNegative_Train,nparray_bucket_TrueNegative_Test,modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay train and test we expect train to be better than test \n",
    "def plot_train_test_accuracy(nparray_bucket_accuracy_Train,nparray_bucket_accuracy_Test,modelName):\n",
    "    plt.hist(nparray_bucket_accuracy_Train,bins=[i/20 for i in range(22)],density=True,alpha=1,color=\"blue\",histtype='step',label=\"Train\")\n",
    "    plt.hist(nparray_bucket_accuracy_Test,bins=[i/20 for i in range(22)],density=True,alpha=1,color=\"red\",histtype='step',label=\"Test\")\n",
    "    plt.ylabel('Nr of buckets')\n",
    "    plt.xlabel('Bucket accuracy')\n",
    "    plt.title(modelName)\n",
    "    plt.legend(loc='upper left')\n",
    "    #plt.ylim(bottom=0,top=250)\n",
    "    #plt.show()\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_accuracy.\"+extension)\n",
    "# done function\n",
    "if doPlotMetrics2:\n",
    "    plot_train_test_accuracy(nparray_bucket_accuracy_Train,nparray_bucket_accuracy_Test,modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay train and test we expect train to be better than test \n",
    "def plot_train_test_precision(nparray_bucket_precision_Train,nparray_bucket_precision_Test,modelName):\n",
    "    plt.hist(nparray_bucket_precision_Train,bins=[i/20 for i in range(22)],density=True,alpha=1,color=\"blue\",histtype='step',label=\"Train\")\n",
    "    plt.hist(nparray_bucket_precision_Test,bins=[i/20 for i in range(22)],density=True,alpha=1,color=\"red\",histtype='step',label=\"Test\")\n",
    "    plt.ylabel('Nr of buckets')\n",
    "    plt.xlabel('Bucket precision')\n",
    "    plt.title(modelName)\n",
    "    plt.legend(loc='upper left')\n",
    "    #plt.ylim(bottom=0,top=250)\n",
    "    #plt.show()\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_precision.\"+extension)\n",
    "# done function\n",
    "if doPlotMetrics2:\n",
    "    plot_train_test_precision(nparray_bucket_precision_Train,nparray_bucket_precision_Test,modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay train and test we expect train to be better than test \n",
    "def plot_train_test_recall(nparray_bucket_recall_Train,nparray_bucket_recall_Test,modelName):\n",
    "    plt.hist(nparray_bucket_recall_Train,bins=[i/20 for i in range(22)],density=True,alpha=1,color=\"blue\",histtype='step',label=\"Train\")\n",
    "    plt.hist(nparray_bucket_recall_Test,bins=[i/20 for i in range(22)],density=True,alpha=1,color=\"red\",histtype='step',label=\"Test\")  \n",
    "    plt.ylabel('Nr of buckets')\n",
    "    plt.xlabel('Bucket recall')\n",
    "    plt.title(modelName)\n",
    "    plt.legend(loc='upper left')\n",
    "    #plt.ylim(bottom=0,top=250)\n",
    "    #plt.show()\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_recall.\"+extension)\n",
    "# done function\n",
    "if doPlotMetrics2:\n",
    "    plot_train_test_recall(nparray_bucket_recall_Train,nparray_bucket_recall_Test,modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay train and test we expect train to be better than test \n",
    "def plot_train_test_negativePredictedValue(nparray_bucket_negativePredictedValue_Train,nparray_bucket_negativePredictedValue_Test,modelName):\n",
    "    plt.hist(nparray_bucket_negativePredictedValue_Train,bins=[i/20 for i in range(22)],density=True,alpha=1,color=\"blue\",histtype='step',label=\"Train\")\n",
    "    plt.hist(nparray_bucket_negativePredictedValue_Test,bins=[i/20 for i in range(22)],density=True,alpha=1,color=\"red\",histtype='step',label=\"Test\")\n",
    "    plt.ylabel('Nr of buckets')\n",
    "    plt.xlabel('Bucket negativePredictedValue')\n",
    "    plt.title(modelName)\n",
    "    plt.legend(loc='upper left')\n",
    "    #plt.ylim(bottom=0,top=250)\n",
    "    #plt.show()\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_negativePredictedValue.\"+extension)\n",
    "# done function\n",
    "if doPlotMetrics2:\n",
    "    plot_train_test_negativePredictedValue(nparray_bucket_negativePredictedValue_Train,nparray_bucket_negativePredictedValue_Test,modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay train and test we expect train to be better than test \n",
    "def plot_train_test_trueNegativeRate(nparray_bucket_trueNegativeRate_Train,nparray_bucket_trueNegativeRate_Test,modelName):\n",
    "    plt.hist(nparray_bucket_trueNegativeRate_Train,bins=[i/20 for i in range(22)],density=True,alpha=1,color=\"blue\",histtype='step',label=\"Train\")\n",
    "    plt.hist(nparray_bucket_trueNegativeRate_Test,bins=[i/20 for i in range(22)],density=True,alpha=1,color=\"red\",histtype='step',label=\"Test\")\n",
    "    plt.ylabel('Nr of buckets')\n",
    "    plt.xlabel('Bucket trueNegativeRate')\n",
    "    plt.title(modelName)\n",
    "    plt.legend(loc='upper left')\n",
    "    #plt.ylim(bottom=0,top=250)\n",
    "    #plt.show()\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_trueNegativeRate.\"+extension)\n",
    "# done function\n",
    "if doPlotMetrics2:\n",
    "    plot_train_test_trueNegativeRate(nparray_bucket_trueNegativeRate_Train,nparray_bucket_trueNegativeRate_Test,modelName)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doCalculateMetrics2:\n",
    "    fig, ax = plt.subplots()\n",
    "    h=ax.hist2d(\n",
    "        nparray_bucket_OutputPositive_Train,\n",
    "        nparray_bucket_PredictedOutputPositive_Train,\n",
    "        bins=[range(0,22),range(0,22)],\n",
    "        cmin=1\n",
    "        )\n",
    "    plt.colorbar(h[3], ax=ax)\n",
    "    plt.xlabel(\"Output Positive\")\n",
    "    plt.ylabel(\"Predicted Output Positive\")\n",
    "    plt.title(modelName+\" Train\")\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_OutputPositive_vs_PredictedOutputPositive_Train.\"+extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doCalculateMetrics2:\n",
    "    fig, ax = plt.subplots()\n",
    "    h=ax.hist2d(\n",
    "        nparray_bucket_OutputPositive_Test,\n",
    "        nparray_bucket_PredictedOutputPositive_Test,\n",
    "        bins=[range(0,22),range(0,22)],\n",
    "        cmin=1\n",
    "        )\n",
    "    plt.colorbar(h[3], ax=ax)\n",
    "    plt.xlabel(\"Output Positive\")\n",
    "    plt.ylabel(\"Predicted Output Positive\")\n",
    "    plt.title(modelName+\" Test\")\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_OutputPositive_vs_PredictedOutputPositive_Test.\"+extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doCalculateMetrics2:\n",
    "    fig, ax = plt.subplots()\n",
    "    h=ax.hist2d(\n",
    "        nparray_bucket_OutputNegative_Train,\n",
    "        nparray_bucket_PredictedOutputNegative_Train,\n",
    "        bins=[range(0,22),range(0,22)],\n",
    "        cmin=1,\n",
    "        # norm=LogNorm()\n",
    "        )\n",
    "    plt.colorbar(h[3], ax=ax)\n",
    "    plt.xlabel(\"Output Negative\")\n",
    "    plt.ylabel(\"Predicted Output Negative\")\n",
    "    plt.title(modelName+\" Train\")\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_OutputNegative_vs_PredictedOutputNegative_Train.\"+extension) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doCalculateMetrics2:\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    h=ax.hist2d(\n",
    "        nparray_bucket_OutputNegative_Test,\n",
    "        nparray_bucket_PredictedOutputNegative_Test,\n",
    "        bins=[range(0,22),range(0,22)],\n",
    "        cmin=1\n",
    "        )\n",
    "    plt.colorbar(h[3], ax=ax)\n",
    "    plt.xlabel(\"Output Negative\")\n",
    "    plt.ylabel(\"Predicted Output Negative\")\n",
    "    plt.title(modelName+\" Test\")\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_OutputNegative_vs_PredictedOutputNegative_Test.\"+extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doCalculateMetrics2:\n",
    "    # Confusion matrix Train with no reweighting\n",
    "    nbHitTP=np.sum(nparray_bucket_TruePositive_Train)\n",
    "    nbHitFP=np.sum(nparray_bucket_FalsePositive_Train)\n",
    "    nbHitFN=np.sum(nparray_bucket_FalseNegative_Train)\n",
    "    nbHitTN=np.sum(nparray_bucket_TrueNegative_Train)\n",
    "    nrHitsAll=nbHitTP+nbHitFP+nbHitFN+nbHitTN\n",
    "    nrHitsPercentTP=100*nbHitTP/nrHitsAll\n",
    "    nrHitsPercentFP=100*nbHitFP/nrHitsAll\n",
    "    nrHitsPercentFN=100*nbHitFN/nrHitsAll\n",
    "    nrHitsPercentTN=100*nbHitTN/nrHitsAll\n",
    "    print(\"Train Hits. Percent TP=%.1f FP=%.1f FN=%.1f TN=%.1f\"%(nrHitsPercentTP,nrHitsPercentFP,nrHitsPercentFN,nrHitsPercentTN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doCalculateMetrics2:\n",
    "    # Confusion matrix Train with no reweighting\n",
    "    nbHitTP=np.sum(nparray_bucket_TruePositive_Test)\n",
    "    nbHitFP=np.sum(nparray_bucket_FalsePositive_Test)\n",
    "    nbHitFN=np.sum(nparray_bucket_FalseNegative_Test)\n",
    "    nbHitTN=np.sum(nparray_bucket_TrueNegative_Test)\n",
    "    nrHitsAll=nbHitTP+nbHitFP+nbHitFN+nbHitTN\n",
    "    nrHitsPercentTP=100*nbHitTP/nrHitsAll\n",
    "    nrHitsPercentFP=100*nbHitFP/nrHitsAll\n",
    "    nrHitsPercentFN=100*nbHitFN/nrHitsAll\n",
    "    nrHitsPercentTN=100*nbHitTN/nrHitsAll\n",
    "    print(\"Test Hits. Percent TP=%.1f FP=%.1f FN=%.1f TN=%.1f\"%(nrHitsPercentTP,nrHitsPercentFP,nrHitsPercentFN,nrHitsPercentTN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(TP,FP,FN,TN,debug=False):\n",
    "    \n",
    "    if debug:\n",
    "        print(\"TP\",TP,\"FP\",FP,\"FN\",FN,\"TN\",TN)\n",
    "        \n",
    "    nbTotal=TP+FP+FN+TN\n",
    "    #\n",
    "    TPPercent=100*TP/nbTotal\n",
    "    FPPercent=100*FP/nbTotal\n",
    "    FNPercent=100*FN/nbTotal\n",
    "    TNPercent=100*TN/nbTotal\n",
    "    if debug:\n",
    "        print(\"TPPercent\",TPPercent,\"FPPercent\",FPPercent,\"FNPercent\",FNPercent,\"TNPercent\",TNPercent)\n",
    "        \n",
    "    #\n",
    "    OutputPositive=TP+FN \n",
    "    OutputNegative=FP+TN\n",
    "    PredictedOutputPositive=TP+FP\n",
    "    PredictedOutputNegative=FN+TN\n",
    "    if debug:\n",
    "        print(\"OutputPositive\",OutputPositive,\"OutputNegative\",OutputNegative,\"PredictedOutputPositive\",PredictedOutputPositive,\"PredictedOutputNegative\",PredictedOutputNegative)\n",
    "        \n",
    "    OutputPositivePercent=100*OutputPositive/nbTotal\n",
    "    OutputNegativePercent=100*OutputNegative/nbTotal    \n",
    "    PredictedOutputPositivePercent=100*PredictedOutputPositive/nbTotal        \n",
    "    PredictedOutputNegativePercent=100*PredictedOutputNegative/nbTotal\n",
    "    if debug:\n",
    "        print(\"OutputPositivePercent\",OutputPositivePercent,\"OutputNegativePercent\",OutputNegativePercent,\"PredictedOutputPositivePercent\",PredictedOutputPositivePercent,\"PredictedOutputNegativePercent\",PredictedOutputNegativePercent)\n",
    "        \n",
    "    # https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "    # accuracy=(TP+TN)/(TP+FP+FN+TN)=(TP+TN)/ALL, ALL=20 (20 hits in a bucket)\n",
    "    # precision=(TP)/(TP+FP)=(TP)/(all that are in reality positive)=efficiency from CERN \n",
    "    # e.g. there are 100 truth electrons, efficiency = what fraction of them are also reconstricted as electrons? \n",
    "    # recall=(TP)/(TP+FN)=(TP)/(all that are predicted positive) = one minus fake rate from CERN\n",
    "    # e.g. fake rate = I have reconstructed 100 electrons. What fraction of these are not in reality truth electrons\n",
    "    # fake rate = What fraction of reconstructed electrons are fake electrons?\n",
    "    # fake rate = (FN)/(TP+FN) = 1 - recall\n",
    "        \n",
    "    # accuracy\n",
    "    accuracy=(TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "    # precision\n",
    "    if (TP+FP)==0:\n",
    "        precision=0\n",
    "    else:\n",
    "        precision=(TP)/(TP+FP)\n",
    "    # done if\n",
    "        \n",
    "    # recall\n",
    "    if TP+FN==0:\n",
    "        recall=0\n",
    "    else:\n",
    "        recall=(TP)/(TP+FN)\n",
    "    # done if\n",
    "    \n",
    "    # precision for negative: negative predicted value\n",
    "    if TN+FN==0:\n",
    "        negativePredictedValue=0\n",
    "    else:\n",
    "        negativePredictedValue=(TN)/(TN+FN)\n",
    "    # done if\n",
    "        \n",
    "    # recall for negative: true negative rate\n",
    "    if TN+FP==0:\n",
    "        trueNegativeRate=0\n",
    "    else:\n",
    "        trueNegativeRate=(TN)/(TN+FP)\n",
    "    # done if \n",
    "    \n",
    "    if debug:\n",
    "        print(\"accuracy\",accuracy,\"precision\",precision,\"recall\",recall,\"negativePredictedValue\",negativePredictedValue,\"trueNegativeRate\",trueNegativeRate)\n",
    "        \n",
    "    # return only one dictionary\n",
    "    dict_var_value={}\n",
    "    dict_var_value[\"NbTotal\"]=nbTotal\n",
    "    dict_var_value[\"TP\"]=TP\n",
    "    dict_var_value[\"FP\"]=FP\n",
    "    dict_var_value[\"FN\"]=FN\n",
    "    dict_var_value[\"TN\"]=TN\n",
    "    dict_var_value[\"TPPercent\"]=TPPercent\n",
    "    dict_var_value[\"FPPercent\"]=FPPercent\n",
    "    dict_var_value[\"FNPercent\"]=FNPercent\n",
    "    dict_var_value[\"TNPercent\"]=TNPercent\n",
    "    dict_var_value[\"OutputPositive\"]=OutputPositive\n",
    "    dict_var_value[\"OutputNegative\"]=OutputNegative\n",
    "    dict_var_value[\"PredictedOutputPositive\"]=PredictedOutputPositive\n",
    "    dict_var_value[\"PredictedOutputNegative\"]=PredictedOutputNegative\n",
    "    dict_var_value[\"OutputPositivePercent\"]=OutputPositivePercent\n",
    "    dict_var_value[\"OutputNegativePercent\"]=OutputNegativePercent\n",
    "    dict_var_value[\"PredictedOutputPositivePercent\"]=PredictedOutputPositivePercent\n",
    "    dict_var_value[\"PredictedOutputNegativePercent\"]=PredictedOutputNegativePercent\n",
    "    dict_var_value[\"Accuracy\"]=accuracy\n",
    "    dict_var_value[\"Precision\"]=precision\n",
    "    dict_var_value[\"Recall\"]=recall\n",
    "    dict_var_value[\"NegativePredictedValue\"]=negativePredictedValue\n",
    "    dict_var_value[\"TrueNegativeRate\"]=trueNegativeRate\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Print dict_var_value\")\n",
    "        for var in sorted(dict_var_value.keys()):\n",
    "            print(\"var\",var,\"value\",dict_var_value[var])\n",
    "    \n",
    "    # all done, ready to return\n",
    "    return dict_var_value\n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics3(TrainOrTest, dict_VolumeID_MetricBasic):\n",
    "    if debug or verbose:\n",
    "        print(\"Start calculate_metrics3 for \",TrainOrTest)\n",
    "    \n",
    "    # calculate list of VolumeID in increasing order\n",
    "    list_VolumeID=sorted(dict_VolumeID_MetricBasic.keys())\n",
    "    \n",
    "    # now for each VolumeID we calculate all the metrics\n",
    "    # so for each metric we put the values for each VolumeID one after the other\n",
    "    # so we can put in a list and from the list make a nparray and save that nparray, for later to overlay plots\n",
    "    dict_var_list_value={}\n",
    "    # loop over volumes in their order\n",
    "    for VolumeID in list_VolumeID:\n",
    "        MetricBasic=dict_VolumeID_MetricBasic[VolumeID]\n",
    "        TP=MetricBasic[0]\n",
    "        FP=MetricBasic[1]\n",
    "        FN=MetricBasic[2]\n",
    "        TN=MetricBasic[3]\n",
    "        # from these 4 values, calculate the other metrics and figures of merit \n",
    "        dict_var_value=get_metrics(TP,FP,FN,TN,debug=debug)\n",
    "        for var in dict_var_value.keys():\n",
    "            value=dict_var_value[var]\n",
    "            if var not in dict_var_list_value.keys():\n",
    "                # create a list with one value, value for the current VolumeID\n",
    "                dict_var_list_value[var]=[value]\n",
    "            else:\n",
    "                # to the already existing list add value for the current VolumeID\n",
    "                dict_var_list_value[var].append(value)\n",
    "            # done if\n",
    "        # done for loop over var\n",
    "    # done for loop over VolumeID\n",
    "    \n",
    "    #\n",
    "    outputFileNameStem=outputFolderName+\"/\"+\"NN_7_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    \n",
    "    # save the npparray_volume_id\n",
    "    nparray_volume_id=np.array(list_VolumeID)\n",
    "    p(\"nparray_volume_id\",nparray_volume_id)\n",
    "    var=\"VolumeID\"\n",
    "    outputFileNameNpy=outputFileNameStem+\"_nparray_\"+var+\"_\"+TrainOrTest+\".npy\"\n",
    "    np.save(outputFileNameNpy,nparray_volume_id)\n",
    "    \n",
    "    # for loop over var\n",
    "    for var in sorted(dict_var_list_value.keys()):\n",
    "        list_value=dict_var_list_value[var]\n",
    "        nparray_value=np.array(list_value)\n",
    "        #p(\"nparray_\"+var,nparray_value)\n",
    "        print(\"var\",var)\n",
    "        outputFileNameNpy=outputFileNameStem+\"_nparray_\"+var+\"_VolumeID_\"+TrainOrTest+\".npy\"\n",
    "        np.save(outputFileNameNpy,nparray_value)\n",
    "        # create plot here\n",
    "        plt.plot(nparray_value,marker=\"o\")\n",
    "        plt.xlabel(\"volume_id\")\n",
    "        plt.xticks(range(len(nparray_volume_id)),nparray_volume_id)\n",
    "        plt.ylabel(var)\n",
    "        plt.title(TrainOrTest)\n",
    "        #plt.legend(loc='best')\n",
    "        #plt.show()\n",
    "        for extension in \"png,pdf\".split(\",\"):\n",
    "            plt.savefig(outputFileNameStem+\"_plot_graph_\"+var+\"_VolumeID_\"+TrainOrTest+\".\"+extension)\n",
    "        # done for loop\n",
    "        plt.close()\n",
    "    # done for loop over var\n",
    "    \n",
    "    # calculate the percentage of the numbrer of hits in each VolumeID\n",
    "    nparray_nbTotal=np.array(dict_var_list_value[\"NbTotal\"])\n",
    "    sum_nbTotal=np.sum(nparray_nbTotal)\n",
    "    nparray_nbTotalPercent=100*nparray_nbTotal/sum_nbTotal\n",
    "    p(\"nparray_nbTotalPercent\",nparray_nbTotalPercent)\n",
    "    var=\"NbTotalPercent\"\n",
    "    outputFileNameNpy=outputFileNameStem+\"_nparray_\"+var+\"_VolumeID_\"+TrainOrTest+\".npy\"\n",
    "    np.save(outputFileNameNpy,nparray_nbTotalPercent)\n",
    "    # create plot here\n",
    "    plt.plot(nparray_value,marker=\"o\")\n",
    "    plt.xlabel(\"volume_id\")\n",
    "    plt.xticks(range(len(nparray_volume_id)),nparray_volume_id)\n",
    "    plt.ylabel(var)\n",
    "    plt.title(TrainOrTest)\n",
    "    #plt.legend(loc='best')\n",
    "    #plt.show()\n",
    "    for extension in \"png,pdf\".split(\",\"):\n",
    "        plt.savefig(outputFileNameStem+\"_plot_graph_\"+var+\"_VolumeID_\"+TrainOrTest+\".\"+extension)\n",
    "    # done for loop\n",
    "    plt.close()\n",
    "    \n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if doPlotMetrics3:\n",
    "    calculate_metrics3(\"Train\",dict_VolumeID_MetricBasic_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doPlotMetrics3:\n",
    "    calculate_metrics3(\"Test\",dict_VolumeID_MetricBasic_Train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
