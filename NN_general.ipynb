{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility, before importing keras \n",
    "# we need to set random numbers in both numpy and tensorflow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "np.random.seed(98383822)\n",
    "tf.random.set_seed(278732344)\n",
    "import keras\n",
    "\n",
    "import os\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import pandas as pd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug=False\n",
    "verbose=True\n",
    "doTrain=True\n",
    "doLoadModel=True\n",
    "doLoadMetrics=True\n",
    "doPlotMetrics=True\n",
    "doPredict=True\n",
    "doLoadPredict=True\n",
    "doCalculateMetrics2=True\n",
    "doPlotMetrics2=True\n",
    "doCalculateMetrics3=True\n",
    "doPlotMetrics3=True\n",
    "# to distinguish between models, in order of positions:\n",
    "# all options are chosen to have the same number of letters so that files names appear of the same zise with \"ls\"\n",
    "# 0: optionNbHiddenLayers: 2, 3, 4\n",
    "# 1: optionNbNodesPerHiddenLayer: 5\n",
    "# 2: optionActivationFunctionHiddenLayer: E (elu), R (relu)\n",
    "# 3: optionActivationFunctionLastLayer: TANH (tanh), SQNL (SQNL), SOSI (SoftSign)\n",
    "# 4: optionOptimizer: Adam (Adam), AdaD (AdaDelta)\n",
    "# 5: optionLossFunction: SH (squared hinge), RH (hinge, or regular hinge at power one)\n",
    "#option=\"2_5_E_TANH_Adam_SH_A\" # before\n",
    "option=\"3_10_R_TANH_Adam_RH_B\" # after comparing several this seems to be the best\n",
    "#\n",
    "flatMax=17\n",
    "numberOfEpochs=1200\n",
    "batchSize=50000\n",
    "extensions=\"png,pdf\"\n",
    "modelName=\"Balanced\"+str(flatMax)+\"_\"+option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p(name,nparray):\n",
    "    print(\"Start\",name)\n",
    "    print(nparray)\n",
    "    print(\"End\",name,\"shape\",nparray.shape,\"dtype\",nparray.dtype,\"type\",type(nparray))\n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folderStem=\"/Volumes/Luiza_SSD\"\n",
    "#folderStem=\"/Users/luizaadelinaciucu/Work\"\n",
    "#\n",
    "minNbPositiveHit=\"10\"\n",
    "inputFolderNameUnbalanced=folderStem+\"/ATLAS/TrackML/output_new_ev_000_100\"\n",
    "inputFolderNameBalanced=inputFolderNameUnbalanced+\"_min_\"+minNbPositiveHit+\"_balanced\"+str(flatMax)\n",
    "outputFolderName=inputFolderNameUnbalanced+\"_min_\"+minNbPositiveHit+\"_NN_B_HP\"\n",
    "# if output folder does not exist, create it\n",
    "if not os.path.exists(outputFolderName):\n",
    "    os.makedirs(outputFolderName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eventNumber=\"000000082\"\n",
    "eventNumber=\"all\"\n",
    "\n",
    "\n",
    "if modelName==\"Unbalanced\":\n",
    "    # without Balanced in the names\n",
    "    #\n",
    "    nparray_Input_Train=np.load(inputFolderNameUnbalanced+\"/NN_2_data_Input_Train_\"+eventNumber+\".npy\")\n",
    "    nparray_Input_Test=np.load(inputFolderNameUnbalanced+\"/NN_2_data_Input_Test_\"+eventNumber+\".npy\")\n",
    "    #\n",
    "    nparray_Output_Train=np.load(inputFolderNameUnbalanced+\"/NN_2_data_OutputMin\"+minNbPositiveHit+\"_Train_\"+eventNumber+\".npy\")\n",
    "    nparray_Output_Test=np.load(inputFolderNameUnbalanced+\"/NN_2_data_OutputMin\"+minNbPositiveHit+\"_Test_\"+eventNumber+\".npy\")\n",
    "    #\n",
    "    nparray_VolumeID_Train=np.load(inputFolderNameUnbalanced+\"/NN_2_data_VolumeID_Train_\"+eventNumber+\".npy\")\n",
    "    nparray_VolumeID_Test=np.load(inputFolderNameUnbalanced+\"/NN_2_data_VolumeID_Test_\"+eventNumber+\".npy\")\n",
    "elif modelName.startswith(\"Balanced\"):\n",
    "    # with Balanced in the names for input and then for Test you have two choice, also take from balanced or unbalanced\n",
    "    \n",
    "    # Train\n",
    "    if True:\n",
    "        # Train balanced\n",
    "        nparray_Input_Train=np.load(inputFolderNameBalanced+\"/NN_2_data_InputBalanced_Train_\"+eventNumber+\".npy\")\n",
    "        nparray_Output_Train=np.load(inputFolderNameBalanced+\"/NN_2_data_OutputBalanced_Train_\"+eventNumber+\".npy\")\n",
    "        nparray_VolumeID_Train=np.load(inputFolderNameBalanced+\"/NN_2_data_VolumeIDBalanced_Train_\"+eventNumber+\".npy\")\n",
    "    \n",
    "    # Test two options, output also balanced or unbalanced\n",
    "    if True:\n",
    "        # Test unbalanced\n",
    "        nparray_Input_Test=np.load(inputFolderNameUnbalanced+\"/NN_2_data_Input_Test_\"+eventNumber+\".npy\")\n",
    "        nparray_Output_Test=np.load(inputFolderNameUnbalanced+\"/NN_2_data_OutputMin\"+minNbPositiveHit+\"_Test_\"+eventNumber+\".npy\")\n",
    "        nparray_VolumeID_Test=np.load(inputFolderNameUnbalanced+\"/NN_2_data_VolumeID_Test_\"+eventNumber+\".npy\")\n",
    "        \n",
    "    if False:\n",
    "        # Test balanced\n",
    "        nparray_Input_Test=np.load(inputFolderNameBalanced+\"/NN_2_data_InputBalanced_Test_\"+eventNumber+\".npy\")\n",
    "        nparray_Output_Test=np.load(inputFolderNameBalanced+\"/NN_2_data_OutputBalanced_Test_\"+eventNumber+\".npy\")\n",
    "        nparray_VolumeID_Test=np.load(inputFolderNameBalanced+\"/NN_2_data_VolumeIDBalanced_Test_\"+eventNumber+\".npy\")\n",
    "else:\n",
    "    print(\"modelName\",modelName,\"not known. ERROR!\")\n",
    "    assert(False)\n",
    "# done if\n",
    "\n",
    "# convert output from int8 to float32, maybe it will give more precision in the results\n",
    "# although you would expect Keras to convert internally in the format best for it\n",
    "nparray_Output_Train = nparray_Output_Train.astype(np.float32)\n",
    "nparray_Output_Test = nparray_Output_Test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p(\"Input_Train\",nparray_Input_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p(\"Input_Test\",nparray_Input_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p (\"Output_Train\", nparray_Output_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p (\"Output_Test\",nparray_Output_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p(\"VolumeID_Train\",nparray_VolumeID_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p(\"VolumeID_Test\",nparray_VolumeID_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility, before importing keras \n",
    "# we need to set random numbers in both numpy and tensorflow\n",
    "#np.random.seed(98383822)\n",
    "#tf.random.set_seed(2)\n",
    "#import keras\n",
    "\n",
    "nrNodesInputLayer=nparray_Input_Train.shape[1] # three inputs (x, y, z) for each hit in the batch\n",
    "nrNodesOutputLayer=nparray_Output_Train.shape[1] # one output for each hit in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_modelOld():\n",
    "    \n",
    "    # nr nodes on the hidden layers\n",
    "    k=5\n",
    "    nrNodesHiddenLayer=nrNodesOutputLayer*k\n",
    "\n",
    "    # create empty model\n",
    "    model=keras.models.Sequential()\n",
    "\n",
    "    # add first layer \n",
    "    model.add(keras.layers.Dense(nrNodesInputLayer,activation='linear',input_shape=(nrNodesInputLayer,1)))\n",
    "\n",
    "    # flatten first layer\n",
    "    model.add(keras.layers.Flatten())\n",
    "\n",
    "    # add hidden layers\n",
    "    model.add(keras.layers.Dense(nrNodesHiddenLayer,activation='elu'))\n",
    "    model.add(keras.layers.Dense(nrNodesHiddenLayer,activation='elu'))\n",
    "    \n",
    "    # add output layer\n",
    "    model.add(keras.layers.Dense(nrNodesOutputLayer,activation='tanh'))\n",
    "\n",
    "    # model geometry done\n",
    "\n",
    "    # choosing how the NN learns\n",
    "    # https://keras.io/models/sequential/\n",
    "    # learning method squared hinge\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False),\n",
    "        loss=keras.losses.squared_hinge,\n",
    "        metrics=['binary_accuracy'],\n",
    "        ),\n",
    "    # done if\n",
    "\n",
    "   # now model is done we are ready to train \n",
    "    return model\n",
    "# done function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use as activation function between -1 and 1, so instead of tanh\n",
    "# code from the answer to\n",
    "# https://stackoverflow.com/questions/57023350/implementing-the-square-non-linearity-sqnl-activation-function-in-keras\n",
    "def square_non_linear(x):\n",
    "    orig = x\n",
    "    x = tf.where(orig >2.0, (tf.ones_like(x)) , x)\n",
    "    x = tf.where(tf.logical_and(0.0 <= orig, orig <=2.0), (x - tf.math.square(x)/4.), x)\n",
    "    x = tf.where(tf.logical_and(-2.0 <= orig, orig < 0), (x + tf.math.square(x)/4.), x)\n",
    "    return tf.where(orig < -2.0, 0*x-1.0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(option):\n",
    "    \n",
    "    list_option = option.split(\"_\")\n",
    "    if len(list_option)!=7:\n",
    "        print(\"option\",option,\"list_option\",list_option,\"does not have 7 elements. Will ABORT!!!\")\n",
    "        assert(False)\n",
    "    optionNbHiddenLayers=list_option[0]\n",
    "    optionNbNodesPerHiddenLayer=list_option[1]\n",
    "    optionActivationFunctionHiddenLayer=list_option[2]\n",
    "    optionActivationFunctionLastLayer=list_option[3]\n",
    "    optionOptimizer=list_option[4]\n",
    "    optionLossFunction=list_option[5]\n",
    "    optionDropoutLayer=list_option[6]\n",
    "    \n",
    "    if debug or verbose:\n",
    "        print(\"optionNbHiddenLayers\",optionNbHiddenLayers)\n",
    "        print(\"optionNbNodesPerHiddenLayer\",optionNbNodesPerHiddenLayer)\n",
    "        print(\"optionActivationFunctionHiddenLayer\",optionActivationFunctionHiddenLayer)\n",
    "        print(\"optionActivationFunctionLastLayer\",optionActivationFunctionLastLayer)\n",
    "        print(\"optionOptimizer\",optionOptimizer)\n",
    "        print(\"optionLossFunction\",optionLossFunction)\n",
    "        print(\"optionDropoutLayer\",optionDropoutLayer) # not used so far\n",
    "    \n",
    "    # nr nodes on the hidden layers\n",
    "    k=int(optionNbNodesPerHiddenLayer) # e.g. 5\n",
    "    nrNodesHiddenLayer=nrNodesOutputLayer*k\n",
    "\n",
    "    # create empty model\n",
    "    model=keras.models.Sequential()\n",
    "\n",
    "    # add first layer \n",
    "    model.add(keras.layers.Dense(nrNodesInputLayer,activation='linear',input_shape=(nrNodesInputLayer,1)))\n",
    "\n",
    "    # flatten first layer\n",
    "    model.add(keras.layers.Flatten())\n",
    "    \n",
    "    if optionDropoutLayer == \"D\":\n",
    "        model.add(keras.layers.Dropout(0.2))\n",
    "    \n",
    "    \n",
    "    # calculate activationNameHiddenLayer\n",
    "    if optionActivationFunctionHiddenLayer==\"E\":\n",
    "        activationNameHiddenLayer=\"elu\"\n",
    "    elif optionActivationFunctionHiddenLayer==\"R\":\n",
    "        activationNameHiddenLayer=\"relu\"\n",
    "    else:\n",
    "        print(\"optionActivationFunctionHiddenLayer\",optionActivationFunctionHiddenLayer, \"not known. Will ABORT!!!\")\n",
    "        assert(False)\n",
    "    # done if\n",
    "\n",
    "    # add hidden layers\n",
    "    if optionNbHiddenLayers==\"1\":\n",
    "        model.add(keras.layers.Dense(nrNodesHiddenLayer,activation=activationNameHiddenLayer))\n",
    "        if optionDropoutLayer == \"C\" or optionDropoutLayer == \"D\":\n",
    "            model.add(keras.layers.Dropout(0.2))\n",
    "    elif optionNbHiddenLayers==\"2\":\n",
    "        model.add(keras.layers.Dense(nrNodesHiddenLayer,activation=activationNameHiddenLayer))\n",
    "        if optionDropoutLayer == \"C\" or optionDropoutLayer == \"D\":\n",
    "            model.add(keras.layers.Dropout(0.2))\n",
    "        model.add(keras.layers.Dense(nrNodesHiddenLayer,activation=activationNameHiddenLayer))\n",
    "        if optionDropoutLayer == \"C\" or optionDropoutLayer == \"D\":\n",
    "            model.add(keras.layers.Dropout(0.2))\n",
    "    elif optionNbHiddenLayers==\"3\":\n",
    "        model.add(keras.layers.Dense(nrNodesHiddenLayer,activation=activationNameHiddenLayer))\n",
    "        if optionDropoutLayer == \"C\" or optionDropoutLayer == \"D\":\n",
    "            model.add(keras.layers.Dropout(0.2))\n",
    "        model.add(keras.layers.Dense(nrNodesHiddenLayer,activation=activationNameHiddenLayer))\n",
    "        if optionDropoutLayer == \"C\" or optionDropoutLayer == \"D\":\n",
    "            model.add(keras.layers.Dropout(0.2))\n",
    "        model.add(keras.layers.Dense(nrNodesHiddenLayer,activation=activationNameHiddenLayer))\n",
    "        if optionDropoutLayer == \"C\" or optionDropoutLayer == \"D\":\n",
    "            model.add(keras.layers.Dropout(0.2))\n",
    "    elif optionNbHiddenLayers==\"4\":\n",
    "        model.add(keras.layers.Dense(nrNodesHiddenLayer,activation=activationNameHiddenLayer))\n",
    "        if optionDropoutLayer == \"C\" or optionDropoutLayer == \"D\":\n",
    "            model.add(keras.layers.Dropout(0.2))\n",
    "        model.add(keras.layers.Dense(nrNodesHiddenLayer,activation=activationNameHiddenLayer))\n",
    "        if optionDropoutLayer == \"C\" or optionDropoutLayer == \"D\":\n",
    "            model.add(keras.layers.Dropout(0.2))\n",
    "        model.add(keras.layers.Dense(nrNodesHiddenLayer,activation=activationNameHiddenLayer))\n",
    "        if optionDropoutLayer == \"C\" or optionDropoutLayer == \"D\":\n",
    "            model.add(keras.layers.Dropout(0.2))\n",
    "        model.add(keras.layers.Dense(nrNodesHiddenLayer,activation=activationNameHiddenLayer))\n",
    "        if optionDropoutLayer == \"C\" or optionDropoutLayer == \"D\":\n",
    "            model.add(keras.layers.Dropout(0.2))\n",
    "    elif optionNbHiddenLayers==\"5\":\n",
    "        model.add(keras.layers.Dense(nrNodesHiddenLayer,activation=activationNameHiddenLayer))\n",
    "        if optionDropoutLayer == \"C\" or optionDropoutLayer == \"D\":\n",
    "            model.add(keras.layers.Dropout(0.2))\n",
    "        model.add(keras.layers.Dense(nrNodesHiddenLayer,activation=activationNameHiddenLayer))\n",
    "        if optionDropoutLayer == \"C\" or optionDropoutLayer == \"D\":\n",
    "            model.add(keras.layers.Dropout(0.2))\n",
    "        model.add(keras.layers.Dense(nrNodesHiddenLayer,activation=activationNameHiddenLayer))\n",
    "        if optionDropoutLayer == \"C\" or optionDropoutLayer == \"D\":\n",
    "            model.add(keras.layers.Dropout(0.2))\n",
    "        model.add(keras.layers.Dense(nrNodesHiddenLayer,activation=activationNameHiddenLayer))\n",
    "        if optionDropoutLayer == \"C\" or optionDropoutLayer == \"D\":\n",
    "            model.add(keras.layers.Dropout(0.2))\n",
    "        model.add(keras.layers.Dense(nrNodesHiddenLayer,activation=activationNameHiddenLayer))\n",
    "        if optionDropoutLayer == \"C\" or optionDropoutLayer == \"D\":\n",
    "            model.add(keras.layers.Dropout(0.2))\n",
    "    else:\n",
    "        print(\"optionNbHiddenLayers\",optionNbHiddenLayers, \"not known. Will ABORT!!!\")\n",
    "        assert(False)\n",
    "    # done if \n",
    "    \n",
    "    if optionDropoutLayer == \"A\":\n",
    "        pass\n",
    "    elif optionDropoutLayer == \"B\":\n",
    "        # add a dropout layer in the hidden layers\n",
    "        model.add(keras.layers.Dropout(0.2))\n",
    "    elif optionDropoutLayer == \"C\":\n",
    "        pass\n",
    "    elif optionDropoutLayer == \"D\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"optionDropoutLayer\",optionDropoutLayer, \"not known. Will ABORT!!!\")\n",
    "        assert(False)\n",
    "    # done if    \n",
    "    \n",
    "    # add output layer\n",
    "    if optionActivationFunctionLastLayer==\"TANH\":\n",
    "        my_activation='tanh'\n",
    "    elif optionActivationFunctionLastLayer==\"SQNL\":\n",
    "        my_activation=square_non_linear\n",
    "    elif optionActivationFunctionLastLayer==\"SOSI\":\n",
    "        my_activation='softsign'\n",
    "    else:\n",
    "        print(\"optionActivationFunctionLastLayer\",optionActivationFunctionLastLayer,\"not known. Will ABORT!!!\")\n",
    "        assert(False)\n",
    "    # fi\n",
    "    # add the layer with the chosen my_activation\n",
    "    model.add(keras.layers.Dense(nrNodesOutputLayer,activation=my_activation))\n",
    "    \n",
    "    # model geometry done\n",
    "\n",
    "    # choosing how the NN learns\n",
    "    # https://keras.io/models/sequential/\n",
    "   \n",
    "    # set my_optimizer\n",
    "    if optionOptimizer==\"Adam\":\n",
    "        # learning method squared hinge\n",
    "        my_optimizer=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "        # my_optimizer=keras.optimizers.Adam(learning_rate=0.002, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "    elif optionOptimizer==\"AdaD\":\n",
    "        # learning method hinge (regular hinge)\n",
    "        my_optimizer=keras.optimizers.Adadelta(learning_rate=0.001, rho=0.95, epsilon=1e-07)\n",
    "    else:\n",
    "        print(\"optionOptimizer\",optionOptimizer,\"not known. Will ABORT!!!\")\n",
    "        asert(False)\n",
    "    # done if  \n",
    "    \n",
    "    # set my_loss\n",
    "    if optionLossFunction==\"SH\":\n",
    "        # learning method squared hinge\n",
    "        my_loss=keras.losses.squared_hinge\n",
    "    elif optionLossFunction==\"RH\":\n",
    "        # learning method hinge (regular hinge)\n",
    "        my_loss=keras.losses.hinge\n",
    "    else:\n",
    "        print(\"optionLossFunction\",optionLossFunction,\"not known. Will ABORT!!!\")\n",
    "        asert(False)\n",
    "    # done if\n",
    "    \n",
    "    # compile the model with my chosen my_optimizer and my_loss\n",
    "    model.compile(\n",
    "        optimizer=my_optimizer,\n",
    "        loss=my_loss,\n",
    "        metrics=['binary_accuracy'],\n",
    "        ),\n",
    "    # done if\n",
    "    \n",
    "   # now model is done we are ready to train \n",
    "    return model\n",
    "# done function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=get_model(option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,modelName,numberOfEpochs,batchSize):\n",
    "    print(\"*** Start train_model for modeName\",modelName,\"***\")\n",
    "    # train the model and return for each epoch the accuracy and loss values\n",
    "    # in a variable called history\n",
    "    # https://keras.io/models/sequential\n",
    "    history=model.fit(\n",
    "            nparray_Input_Train,\n",
    "            nparray_Output_Train,\n",
    "            batchSize,\n",
    "            numberOfEpochs,\n",
    "            validation_data=(nparray_Input_Test,nparray_Output_Test),\n",
    "            shuffle=False,\n",
    "            )\n",
    "    # done if\n",
    "    # the train (fit) function outputs a history\n",
    "    # retrieve from it the accuracy, loss, train, test\n",
    "    nparray_accuracyBinary_Train=history.history[\"binary_accuracy\"]\n",
    "    nparray_accuracyBinary_Test=history.history[\"val_binary_accuracy\"]\n",
    "    nparray_loss_Train=history.history[\"loss\"]\n",
    "    nparray_loss_Test=history.history[\"val_loss\"]\n",
    "    \n",
    "    # save the loss, accuracies, weights + biases of the trained model to a file\n",
    "    # create the name stem, specific for this training\n",
    "    outputFileNameStem=\"NN_3_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    # create the file names for accuracy and loss, train and test\n",
    "    outputFileNameAccuracyBinaryTrain=outputFolderName+\"/\"+outputFileNameStem+\"_accuracyBinary_Train.npy\"\n",
    "    outputFileNameAccuracyBinaryTest=outputFolderName+\"/\"+outputFileNameStem+\"_accuracyBinary_Test.npy\"\n",
    "    outputFileNameLossTrain=outputFolderName+\"/\"+outputFileNameStem+\"_loss_Train.npy\"\n",
    "    outputFileNameLossTest=outputFolderName+\"/\"+outputFileNameStem+\"_loss_Test.npy\"\n",
    "    # create the file name for the weights and biases\n",
    "    outputFileNameWeights=outputFolderName+\"/\"+outputFileNameStem+\"_weights.hdf5\"\n",
    "    \n",
    "    # save to files\n",
    "    #\n",
    "    np.save(outputFileNameAccuracyBinaryTrain,nparray_accuracyBinary_Train)\n",
    "    np.save(outputFileNameAccuracyBinaryTest,nparray_accuracyBinary_Test)\n",
    "    np.save(outputFileNameLossTrain,nparray_loss_Train)\n",
    "    np.save(outputFileNameLossTest,nparray_loss_Test)\n",
    "    #\n",
    "    model.save_weights(outputFileNameWeights)\n",
    "\n",
    "    # ready to return\n",
    "    return (model,nparray_accuracyBinary_Train,nparray_accuracyBinary_Test,nparray_loss_Train,nparray_loss_Test)\n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doTrain:\n",
    "    model,nparray_accuracyBinary_Train,nparray_accuracyBinary_Test,nparray_loss_Train,nparray_loss_Test=train_model(model,modelName=modelName,numberOfEpochs=numberOfEpochs,batchSize=batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model,modelName,numberOfEpochs,batchSize):\n",
    "    print(\"*** Start load_model for modeName\",modelName,\"***\")\n",
    "    # load the loss, accuracies, weights + biases of the trained model to a file\n",
    "    \n",
    "    # create the name stem, specific for this training\n",
    "    outputFileNameStem=\"NN_3_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    \n",
    "    # create the file name for the weights and biases\n",
    "    outputFileNameWeights=outputFolderName+\"/\"+outputFileNameStem+\"_weights.hdf5\"\n",
    "    \n",
    "    # load the weights and biases\n",
    "    model.load_weights(outputFileNameWeights)\n",
    "\n",
    "    # ready to return\n",
    "    return model\n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doLoadModel:\n",
    "    model=load_model(model,modelName,numberOfEpochs,batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metrics(modelName,numberOfEpochs,batchSize):\n",
    "    print(\"*** Start load_metrics for modeName\",modelName,\"***\")\n",
    "    # load the loss, accuracies, weights + biases of the trained model to a file\n",
    "    \n",
    "    # create the name stem, specific for this training\n",
    "    outputFileNameStem=\"NN_3_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    \n",
    "    # create the file names for accuracy and loss, train and test\n",
    "    outputFileNameAccuracyBinaryTrain=outputFolderName+\"/\"+outputFileNameStem+\"_accuracyBinary_Train.npy\"\n",
    "    outputFileNameAccuracyBinaryTest=outputFolderName+\"/\"+outputFileNameStem+\"_accuracyBinary_Test.npy\"\n",
    "    outputFileNameLossTrain=outputFolderName+\"/\"+outputFileNameStem+\"_loss_Train.npy\"\n",
    "    outputFileNameLossTest=outputFolderName+\"/\"+outputFileNameStem+\"_loss_Test.npy\"\n",
    "    \n",
    "    # retrieve from it the accuracy, loss, train, test\n",
    "    nparray_accuracyBinary_Train=np.load(outputFileNameAccuracyBinaryTrain)\n",
    "    nparray_accuracyBinary_Test=np.load(outputFileNameAccuracyBinaryTest)\n",
    "    nparray_loss_Train=np.load(outputFileNameLossTrain)\n",
    "    nparray_loss_Test=np.load(outputFileNameLossTest)\n",
    "\n",
    "    # ready to return\n",
    "    return (nparray_accuracyBinary_Train,nparray_accuracyBinary_Test,nparray_loss_Train,nparray_loss_Test)\n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doLoadMetrics:\n",
    "    nparray_accuracyBinary_Train,nparray_accuracyBinary_Test,nparray_loss_Train,nparray_loss_Test=load_metrics(modelName=modelName,numberOfEpochs=numberOfEpochs,batchSize=batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay accuracy train and test\n",
    "def plot_accuracy_Train_vs_Test(nparray_accuracyBinary_Train,nparray_accuracyBinary_Test,modelName):\n",
    "    plt.plot(nparray_accuracyBinary_Train)\n",
    "    plt.plot(nparray_accuracyBinary_Test)\n",
    "    plt.title('Model_'+modelName+' accuracy')\n",
    "    plt.ylabel('Binary accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    #plt.xlim(left=0,right=10)\n",
    "    # plt.ylim(bottom=0,top=0.8)\n",
    "    #plt.ylim(bottom=0.75,top=0.80)\n",
    "    plt.legend(['Train', 'Test'], loc='best')\n",
    "    # plt.show()\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_graph_accuracy.\"+extension)\n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doPlotMetrics:\n",
    "    plot_accuracy_Train_vs_Test(nparray_accuracyBinary_Train,nparray_accuracyBinary_Test,modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay loss train and test\n",
    "def plot_loss_Train_vs_Test(nparray_loss_Train,nparray_loss_Test,modelName):\n",
    "    plt.plot(nparray_loss_Train)\n",
    "    plt.plot(nparray_loss_Test)\n",
    "    plt.title('Model_'+modelName+' loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train','Test'],loc=\"upper left\")\n",
    "    #plt.show()\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_graph_loss.\"+extension)\n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doPlotMetrics:\n",
    "    plot_loss_Train_vs_Test(nparray_loss_Train,nparray_loss_Test,modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_model(model,nparray_Input_Train,nparray_Input_Test,nparray_Output_Train,nparray_Output_Test):\n",
    "    outputFileNameStem=\"NN_4_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    \n",
    "    # Train\n",
    "    nparray_PredictedOutput_Train=model.predict(nparray_Input_Train)\n",
    "    p(\"PredictedOutput_Train\",nparray_PredictedOutput_Train)\n",
    "    p(\"Output_Train\",nparray_Output_Train)\n",
    "    nparray_Diff_Train=nparray_PredictedOutput_Train-nparray_Output_Train\n",
    "    p(\"Diff_Train\",nparray_Diff_Train)\n",
    "    # save numpy arrays to npy files\n",
    "    outputFileNamePredictedOutput_Train=outputFolderName+\"/\"+outputFileNameStem+\"_PredictedOutput_Train.npy\"\n",
    "    np.save(outputFileNamePredictedOutput_Train,nparray_PredictedOutput_Train)\n",
    "    outputFileNameDiff_Train=outputFolderName+\"/\"+outputFileNameStem+\"_Diff_Train.npy\"\n",
    "    np.save(outputFileNameDiff_Train,nparray_Diff_Train)\n",
    "    \n",
    "    # Test\n",
    "    nparray_PredictedOutput_Test=model.predict(nparray_Input_Test)\n",
    "    p(\"PredictedOutput_Test\",nparray_PredictedOutput_Test)\n",
    "    p(\"Output_Test\",nparray_Output_Test)\n",
    "    nparray_Diff_Test=nparray_PredictedOutput_Test-nparray_Output_Test\n",
    "    p(\"Diff_Test\",nparray_Diff_Test)\n",
    "    # save numpy arrays to npy files\n",
    "    outputFileNamePredictedOutput_Test=outputFolderName+\"/\"+outputFileNameStem+\"_PredictedOutput_Test.npy\"\n",
    "    np.save(outputFileNamePredictedOutput_Test,nparray_PredictedOutput_Test)\n",
    "    outputFileNameDiff_Test=outputFolderName+\"/\"+outputFileNameStem+\"_Diff_Test.npy\"\n",
    "    np.save(outputFileNameDiff_Test,nparray_Diff_Test)\n",
    "    \n",
    "# done function    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doPredict:\n",
    "    predict_from_model(model,nparray_Input_Train,nparray_Input_Test,nparray_Output_Train,nparray_Output_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predicted():\n",
    "    outputFileNameStem=\"NN_4_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    \n",
    "    # Train\n",
    "    outputFileNamePredictedOutput_Train=outputFolderName+\"/\"+outputFileNameStem+\"_PredictedOutput_Train.npy\"\n",
    "    nparray_PredictedOutput_Train=np.load(outputFileNamePredictedOutput_Train)\n",
    "    outputFileNameDiff_Train=outputFolderName+\"/\"+outputFileNameStem+\"_Diff_Train.npy\"\n",
    "    nparray_Diff_Train=np.load(outputFileNameDiff_Train)\n",
    "    \n",
    "    # Test\n",
    "    outputFileNamePredictedOutput_Test=outputFolderName+\"/\"+outputFileNameStem+\"_PredictedOutput_Test.npy\"\n",
    "    nparray_PredictedOutput_Test=np.load(outputFileNamePredictedOutput_Test)\n",
    "    outputFileNameDiff_Test=outputFolderName+\"/\"+outputFileNameStem+\"_Diff_Test.npy\"\n",
    "    nparray_Diff_Test=np.load(outputFileNameDiff_Test)\n",
    "    \n",
    "    return nparray_PredictedOutput_Train, nparray_Diff_Train, nparray_PredictedOutput_Test, nparray_Diff_Test\n",
    "# done function   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doLoadPredict:\n",
    "    nparray_PredictedOutput_Train, nparray_Diff_Train, nparray_PredictedOutput_Test, nparray_Diff_Test=load_predicted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i*0.8 for i in range(21)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics4(TrainOrTest, nparray_Output, nparray_PredictedOutput):\n",
    "    nbBucket = len(nparray_Output)\n",
    "    assert(nbBucket==len(nparray_PredictedOutput))\n",
    "    print(\"nbBucket\",nbBucket)\n",
    "    bucketSize=20\n",
    "    print(\"bucketSize\",bucketSize)\n",
    "    counter_particleTruth=0\n",
    "    counter_particleRecon=0\n",
    "    # loop over buckets\n",
    "    for i in range(nbBucket):\n",
    "        #if (4004<i<=4006)==False:\n",
    "        #    continue\n",
    "        if debug or i%100000==0:\n",
    "            print(\"i\",i)\n",
    "        nparray_output=nparray_Output[i]\n",
    "        nparray_outputPredicted=nparray_PredictedOutput[i]\n",
    "        if debug:\n",
    "            p(\"nparray_output\",nparray_output)\n",
    "            p(\"nparray_outputPredicted\",nparray_outputPredicted)\n",
    "        counter_Positive=0\n",
    "        counter_truePositive=0\n",
    "        # loop over hits in the bucket\n",
    "        for j in range(bucketSize):\n",
    "            output=nparray_output[j]\n",
    "            outputPredicted=nparray_outputPredicted[j]\n",
    "            if debug:\n",
    "                print(\"j\",j,\"output\",output,\"outputPredicted\",outputPredicted)\n",
    "            if output>0:\n",
    "                counter_Positive+=1\n",
    "                if outputPredicted>0:\n",
    "                    counter_truePositive+=1\n",
    "                else:\n",
    "                    pass\n",
    "                # done if\n",
    "            else:\n",
    "                pass\n",
    "            # done if\n",
    "        # done for loop over hit (j)\n",
    "        if debug:\n",
    "            print(\"counter_Positive\",counter_Positive,\"counter_truePositive\",counter_truePositive)\n",
    "        if counter_Positive>=10:\n",
    "            counter_particleTruth+=1\n",
    "            if counter_truePositive/counter_Positive>0.8:\n",
    "                counter_particleRecon+=1\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "    # done for loop over bucket (i)\n",
    "    efficiency=100*counter_particleRecon/counter_particleTruth\n",
    "    print(TrainOrTest,\"efficiency=%.1f percent\"%efficiency,\", counter_particleTruth=%.0f\"%counter_particleTruth,\", counter_particleRecon=%.0f\"%counter_particleRecon)\n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doCalculateMetrics4:\n",
    "    print(\"\")\n",
    "    print(\"Train\")\n",
    "    npparay_bla_Train=calculate_metrics4(\"Train\", nparray_Output_Train, nparray_PredictedOutput_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doCalculateMetrics4:\n",
    "    print(\"\")\n",
    "    print(\"Test\")\n",
    "    npparay_bla_Test=calculate_metrics4(\"Test\", nparray_Output_Test, nparray_PredictedOutput_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics2(TrainOrTest, nparray_Output, nparray_PredictedOutput, nparray_VolumeID):\n",
    "    #p(\"nparray_Output\",nparray_Output)\n",
    "    #p(\"nparray_PredictedOutput\",nparray_PredictedOutput)\n",
    "    \n",
    "    # for loop over i (buckets)\n",
    "    # each of this for each bucket have only one value\n",
    "    list_bucket_OutputPositive=[]\n",
    "    list_bucket_OutputNegative=[]\n",
    "    list_bucket_PredictedOutputPositive=[]\n",
    "    list_bucket_PredictedOutputNegative=[]\n",
    "    list_bucket_TruePositive=[]\n",
    "    list_bucket_FalsePositive=[]\n",
    "    list_bucket_FalseNegative=[]\n",
    "    list_bucket_TrueNegative=[]\n",
    "    list_bucket_accuracy=[]\n",
    "    list_bucket_precision=[]\n",
    "    list_bucket_recall=[]\n",
    "    list_bucket_negativePredictedValue=[]\n",
    "    list_bucket_trueNegativeRate=[]\n",
    "    # store for each bucket four values, from which all else can be computed again (TP, FP, FN, TN)\n",
    "    #list_bucket_MetricBasic=[]\n",
    "    \n",
    "    # for each VolumeID we sum all the buckets in that VolumeID to get the total TP,FP,FN,TN in the bucket\n",
    "    # first create for each VolumeID a numpy array of MetricBasic with 4 values set to zero\n",
    "    dict_VolumeID_MetricBasic={}            \n",
    "    \n",
    "    # loop over all the buckets\n",
    "    nbBucketTotal=len(nparray_Output)\n",
    "    for i in range(nbBucketTotal):\n",
    "        if i%100000==0:\n",
    "            print(TrainOrTest+\" bucket i\",i,\"/\",nbBucketTotal)\n",
    "        nparray_bucket_Output=nparray_Output[i]\n",
    "        nparray_bucket_PredictedOutput=nparray_PredictedOutput[i]\n",
    "        #p(\"nparray_bucket_Output\",nparray_bucket_Output)\n",
    "        #p(\"nparray_bucket_PredictedOutput\",nparray_bucket_PredictedOutput)\n",
    "        \n",
    "        # for loop over j (hit)\n",
    "        counter_hit_TP=0\n",
    "        counter_hit_FP=0\n",
    "        counter_hit_FN=0\n",
    "        counter_hit_TN=0\n",
    "        for j in range(len(nparray_bucket_Output)):\n",
    "            #print (\"hit j\",j)\n",
    "            hit_Output=nparray_bucket_Output[j]\n",
    "            hit_PredictedOutput=nparray_bucket_PredictedOutput[j]\n",
    "            #print(\"j\",j,\"hit_PredictedOutput - hit_Output\",hit_PredictedOutput, hit_Output)\n",
    "               \n",
    "            # confusion matrix\n",
    "            # TP FP\n",
    "            # FN TN\n",
    "            TP=0 # True Positive\n",
    "            FP=0 # False Positive (type I error)\n",
    "            FN=0 # False Negative (type II error)\n",
    "            TN=0 # True Negative\n",
    " \n",
    "            \n",
    "            \n",
    "            \n",
    "            # for this hit ask conditions\n",
    "            if hit_PredictedOutput>0:\n",
    "                # the value is predicted positive\n",
    "                if hit_Output>0:\n",
    "                    # the value is actually positive\n",
    "                    TP=1\n",
    "                else:\n",
    "                    # the value is actually negative \n",
    "                    FP=1\n",
    "                # done if \n",
    "            else:\n",
    "                # the value is predicted negative\n",
    "                if hit_Output>0:\n",
    "                    # the value is actually positive\n",
    "                    FN=1\n",
    "                else:\n",
    "                    # the value is actually negative\n",
    "                    TN=1\n",
    "                # done if\n",
    "            # done if \n",
    "            # for this hit only one of these four values is 1, the rest of three are zero\n",
    "            #print(\"i\",i,\"j\",j,\"TP\",TP,\"FP\",FP,\"FN\",FN,\"TN\",TN)\n",
    "            \n",
    "            # increment counters for hits\n",
    "            counter_hit_TP+=TP\n",
    "            counter_hit_FP+=FP\n",
    "            counter_hit_FN+=FN\n",
    "            counter_hit_TN+=TN  \n",
    "            \n",
    "            # put the 4 basic together to create the BasicMatric for this hit as a nparray\n",
    "            MetricBasic=np.array([TP,FP,FN,TN])\n",
    "            # add the basic metric for this hist to the dict_VolumeID_BasicMetric depending on the VolumeID of this hit\n",
    "            # this is for the i bucket and inside the j hit\n",
    "            VolumeID=nparray_VolumeID[i][j]\n",
    "            if VolumeID not in dict_VolumeID_MetricBasic.keys():\n",
    "                dict_VolumeID_MetricBasic[VolumeID]=MetricBasic\n",
    "            else:\n",
    "                dict_VolumeID_MetricBasic[VolumeID]+=MetricBasic\n",
    "        # done for loop over j (hit)\n",
    "        #print(\"i\",i,\"counter_hit_TP\",counter_hit_TP,\"counter_hit_FP\",counter_hit_FP,\"counter_hit_FN\",counter_hit_FN,\"counter_hit_TN\",counter_hit_TN)\n",
    "        \n",
    "        bucket_OutputPositive=counter_hit_TP+counter_hit_FN \n",
    "        bucket_OutputNegative=counter_hit_FP+counter_hit_TN\n",
    "        bucket_PredictedOutputPositive=counter_hit_TP+counter_hit_FP\n",
    "        bucket_PredictedOutputNegative=counter_hit_FN+counter_hit_TN\n",
    "        \n",
    "        #print(\"i\",i,\"bucket_OutputPositive\",bucket_OutputPositive)\n",
    "        #print(\"i\",i,\"bucket_OutputNegative\",bucket_OutputNegative)\n",
    "        #print(\"i\",i,\"bucket_PredictedOutputPositive\",bucket_PredictedOutputPositive)\n",
    "        #print(\"i\",i,\"bucket_PredictedOutputNegative\",bucket_PredictedOutputNegative)\n",
    "        \n",
    "        \n",
    "        bucket_TruePositive=counter_hit_TP\n",
    "        bucket_FalsePositive=counter_hit_FP\n",
    "        bucket_FalseNegative=counter_hit_FN\n",
    "        bucket_TrueNegative=counter_hit_TN\n",
    "\n",
    "        # accuracy=(TP+TN)/(TP+FP+FN+TN)=(TP+TN)/ALL, ALL=20 (20 hits in a bucket)\n",
    "        # precision=(TP)/(TP+FP)=(TP)/(all that are in reality positive)=efficiency from CERN \n",
    "        # e.g. there are 100 truth electrons, efficiency = what fraction of them are also reconstricted as electrons? \n",
    "        # recall=(TP)/(TP+FN)=(TP)/(all that are predicted positive) = one minus fake rate from CERN\n",
    "        # e.g. fake rate = I have reconstructed 100 electrons. What fraction of these are not in reality truth electrons\n",
    "        # fake rate = What fraction of reconstructed electrons are fake electrons?\n",
    "        # fake rate = (FN)/(TP+FN) = 1 - recall\n",
    "        \n",
    "        # accuracy\n",
    "        bucket_accuracy=(counter_hit_TP+counter_hit_TN)/(counter_hit_TP+counter_hit_FP+counter_hit_FN+counter_hit_TN)\n",
    "        \n",
    "        # https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "        \n",
    "        # precision\n",
    "        if (counter_hit_TP+counter_hit_FP)==0:\n",
    "            bucket_precision=0\n",
    "        else:\n",
    "            bucket_precision=(counter_hit_TP)/(counter_hit_TP+counter_hit_FP)\n",
    "        # done if\n",
    "        \n",
    "        # recall\n",
    "        if counter_hit_TP+counter_hit_FN==0:\n",
    "            bucket_recall=0\n",
    "        else:\n",
    "            bucket_recall=(counter_hit_TP)/(counter_hit_TP+counter_hit_FN)\n",
    "        # done if\n",
    "        #print(\"i\",i,\"bucket_accuracy\",bucket_accuracy,\"bucket_precision\",bucket_precision,\"bucket_recall\",bucket_recall)\n",
    "        \n",
    "        # Negative predicted values\n",
    "        if counter_hit_TN+counter_hit_FN==0:\n",
    "            bucket_negativePredictedValue=0\n",
    "        else:\n",
    "            bucket_negativePredictedValue=(counter_hit_TN)/(counter_hit_TN+counter_hit_FN)\n",
    "        # done if\n",
    "        \n",
    "        # True negative rate\n",
    "        if counter_hit_TN+counter_hit_FP==0:\n",
    "            bucket_trueNegativeRate=0\n",
    "        else:\n",
    "            bucket_trueNegativeRate=(counter_hit_TN)/(counter_hit_TN+counter_hit_FP)\n",
    "        # done if \n",
    "        \n",
    "        # the four basic metric froms which everythis calculated again are all integers with max value 20\n",
    "        # are put in a list and the list made a numpy array of positive integers uint8\n",
    "        #bucket_MetricBasic=np.array([counter_hit_TP,counter_hit_FP,counter_hit_FN,counter_hit_TN]).astype(np.uint8)\n",
    "        \n",
    "        \n",
    "        # add to current bucket to the list across all buckets\n",
    "        list_bucket_OutputPositive.append(bucket_OutputPositive)\n",
    "        list_bucket_OutputNegative.append(bucket_OutputNegative)\n",
    "        list_bucket_PredictedOutputPositive.append(bucket_PredictedOutputPositive)\n",
    "        list_bucket_PredictedOutputNegative.append(bucket_PredictedOutputNegative)\n",
    "        list_bucket_TruePositive.append(bucket_TruePositive)\n",
    "        list_bucket_FalsePositive.append(bucket_FalsePositive)\n",
    "        list_bucket_FalseNegative.append(bucket_FalseNegative)\n",
    "        list_bucket_TrueNegative.append(bucket_TrueNegative)\n",
    "        list_bucket_accuracy.append(bucket_accuracy)\n",
    "        list_bucket_precision.append(bucket_precision)\n",
    "        list_bucket_recall.append(bucket_recall)\n",
    "        list_bucket_negativePredictedValue.append(bucket_negativePredictedValue)\n",
    "        list_bucket_trueNegativeRate.append(bucket_trueNegativeRate)\n",
    "        #\n",
    "        #list_bucket_MetricBasic.append(bucket_MetricBasic)\n",
    "        \n",
    "    # done for loop over i (bucket)\n",
    "    \n",
    "    #print(\"list_bucket_accuracy\",list_bucket_accuracy)\n",
    "    #print(\"list_bucket_precision\",list_bucket_precision)\n",
    "    #print(\"list_bucket_recall\",list_bucket_recall)\n",
    "    \n",
    "    # convert list to numpy array\n",
    "    nparray_bucket_OutputPositive=np.array(list_bucket_OutputPositive)\n",
    "    nparray_bucket_OutputNegative=np.array(list_bucket_OutputNegative)\n",
    "    nparray_bucket_PredictedOutputPositive=np.array(list_bucket_PredictedOutputPositive)\n",
    "    nparray_bucket_PredictedOutputNegative=np.array(list_bucket_PredictedOutputNegative)\n",
    "    nparray_bucket_TruePositive=np.array(list_bucket_TruePositive)\n",
    "    nparray_bucket_FalsePositive=np.array(list_bucket_FalsePositive)\n",
    "    nparray_bucket_FalseNegative=np.array(list_bucket_FalseNegative)\n",
    "    nparray_bucket_TrueNegative=np.array(list_bucket_TrueNegative)\n",
    "    nparray_bucket_accuracy=np.array(list_bucket_accuracy)\n",
    "    nparray_bucket_precision=np.array(list_bucket_precision)\n",
    "    nparray_bucket_recall=np.array(list_bucket_recall)\n",
    "    nparray_bucket_negativePredictedValue=np.array(list_bucket_negativePredictedValue)\n",
    "    nparray_bucket_trueNegativeRate=np.array(list_bucket_trueNegativeRate)\n",
    "    #\n",
    "    #nparray_bucket_MetricBasic=np.array(list_bucket_MetricBasic)\n",
    "    \n",
    "    p(\"nparray_bucket_OutputPositive\",nparray_bucket_OutputPositive)\n",
    "    p(\"nparray_bucket_OutputNegative\",nparray_bucket_OutputNegative)\n",
    "    p(\"nparray_bucket_PredictedOutputPositive\",nparray_bucket_PredictedOutputPositive)\n",
    "    p(\"nparray_bucket_PredictedOutputNegative\",nparray_bucket_PredictedOutputNegative)\n",
    "    p(\"nparray_bucket_TruePositive\",nparray_bucket_TruePositive)\n",
    "    p(\"nparray_bucket_FalsePositive\",nparray_bucket_FalsePositive)\n",
    "    p(\"nparray_bucket_FalseNegative\",nparray_bucket_FalseNegative)\n",
    "    p(\"nparray_bucket_TrueNegative\",nparray_bucket_TrueNegative)\n",
    "    p(\"nparray_bucket_accuracy\",nparray_bucket_accuracy)\n",
    "    p(\"nparray_bucket_precision\",nparray_bucket_precision)\n",
    "    p(\"nparray_bucket_recall\",nparray_bucket_recall)\n",
    "    p(\"nparray_bucket_negativePredictedValue\",nparray_bucket_negativePredictedValue)\n",
    "    p(\"nparray_bucket_trueNegativeRate\",nparray_bucket_trueNegativeRate)\n",
    "    #\n",
    "    #p(\"nparray_bucket_MetricBasic\",nparray_bucket_MetricBasic)\n",
    "    \n",
    "    # save numpy arrays to file, first create the common part of the name based on the current model\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    \n",
    "    # create the name of each numpy array and save it\n",
    "    outputFileName_OutputPositive=outputFolderName+\"/\"+outputFileNameStem+\"_OutputPositive_\"+TrainOrTest+\".npy\"\n",
    "    np.save(outputFileName_OutputPositive,nparray_bucket_OutputPositive)\n",
    "    outputFileName_OutputNegative=outputFolderName+\"/\"+outputFileNameStem+\"_OutputNegative_\"+TrainOrTest+\".npy\"\n",
    "    np.save(outputFileName_OutputNegative,nparray_bucket_OutputNegative)\n",
    "    outputFileName_OutputNegative=outputFolderName+\"/\"+outputFileNameStem+\"_OutputNegative_\"+TrainOrTest+\".npy\"\n",
    "    np.save(outputFileName_OutputNegative,nparray_bucket_OutputNegative)\n",
    "    outputFileName_PredictedOutputPositive=outputFolderName+\"/\"+outputFileNameStem+\"_PredictedOutputPositive_\"+TrainOrTest+\".npy\"\n",
    "    np.save(outputFileName_PredictedOutputPositive,nparray_bucket_PredictedOutputPositive)\n",
    "    outputFileName_PredictedOutputNegative=outputFolderName+\"/\"+outputFileNameStem+\"_PredictedOutputNegative_\"+TrainOrTest+\".npy\"\n",
    "    np.save(outputFileName_PredictedOutputNegative,nparray_bucket_PredictedOutputNegative)\n",
    "    outputFileName_TruePositive=outputFolderName+\"/\"+outputFileNameStem+\"_TruePositive_\"+TrainOrTest+\".npy\"\n",
    "    np.save(outputFileName_TruePositive,nparray_bucket_TruePositive)\n",
    "    outputFileName_FalsePositive=outputFolderName+\"/\"+outputFileNameStem+\"_FalsePositive_\"+TrainOrTest+\".npy\"\n",
    "    np.save(outputFileName_FalsePositive,nparray_bucket_FalsePositive)\n",
    "    outputFileName_FalseNegative=outputFolderName+\"/\"+outputFileNameStem+\"_FalseNegative_\"+TrainOrTest+\".npy\"\n",
    "    np.save(outputFileName_FalseNegative,nparray_bucket_FalseNegative)\n",
    "    outputFileName_TrueNegative=outputFolderName+\"/\"+outputFileNameStem+\"_TrueNegative_\"+TrainOrTest+\".npy\"\n",
    "    np.save(outputFileName_TrueNegative,nparray_bucket_TrueNegative)\n",
    "    outputFileName_accuracy=outputFolderName+\"/\"+outputFileNameStem+\"_Accuracy_\"+TrainOrTest+\".npy\"\n",
    "    np.save(outputFileName_accuracy,nparray_bucket_accuracy)\n",
    "    outputFileName_precision=outputFolderName+\"/\"+outputFileNameStem+\"_Precision_\"+TrainOrTest+\".npy\"\n",
    "    np.save(outputFileName_precision,nparray_bucket_precision)\n",
    "    outputFileName_recall=outputFolderName+\"/\"+outputFileNameStem+\"_Recall_\"+TrainOrTest+\".npy\"\n",
    "    np.save(outputFileName_recall,nparray_bucket_recall)\n",
    "    outputFileName_negativePredictedValue=outputFolderName+\"/\"+outputFileNameStem+\"_NegativePredictedValue_\"+TrainOrTest+\".npy\"\n",
    "    np.save(outputFileName_negativePredictedValue,nparray_bucket_negativePredictedValue)\n",
    "    outputFileName_trueNegativeRate=outputFolderName+\"/\"+outputFileNameStem+\"_TrueNegativeRate_\"+TrainOrTest+\".npy\"\n",
    "    np.save(outputFileName_trueNegativeRate,nparray_bucket_trueNegativeRate)\n",
    "    #\n",
    "    #outputFileName_MetricBasic=outputFolderName+\"/\"+outputFileNameStem+\"_MetricBasic_\"+TrainOrTest+\".npy\"\n",
    "    #np.save(outputFileName_MetricBasic,nparray_bucket_MetricBasic)\n",
    "    \n",
    "    # done all, ready to return\n",
    "    return nparray_bucket_OutputPositive,nparray_bucket_OutputNegative,nparray_bucket_PredictedOutputPositive,nparray_bucket_PredictedOutputNegative,nparray_bucket_TruePositive,nparray_bucket_FalsePositive,nparray_bucket_FalseNegative,nparray_bucket_TrueNegative,nparray_bucket_accuracy,nparray_bucket_precision,nparray_bucket_recall,nparray_bucket_negativePredictedValue,nparray_bucket_trueNegativeRate,dict_VolumeID_MetricBasic\n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doCalculateMetrics2:\n",
    "    print(\"\")\n",
    "    print(\"Train\")\n",
    "    nparray_bucket_OutputPositive_Train,nparray_bucket_OutputNegative_Train,nparray_bucket_PredictedOutputPositive_Train,nparray_bucket_PredictedOutputNegative_Train,nparray_bucket_TruePositive_Train,nparray_bucket_FalsePositive_Train,nparray_bucket_FalseNegative_Train,nparray_bucket_TrueNegative_Train,nparray_bucket_accuracy_Train,nparray_bucket_precision_Train,nparray_bucket_recall_Train,nparray_bucket_negativePredictedValue_Train,nparray_bucket_trueNegativeRate_Train,dict_VolumeID_MetricBasic_Train=calculate_metrics2(\"Train\", nparray_Output_Train, nparray_PredictedOutput_Train, nparray_VolumeID_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doCalculateMetrics2:\n",
    "    print(\"\")\n",
    "    print(\"Test\")\n",
    "    nparray_bucket_OutputPositive_Test,nparray_bucket_OutputNegative_Test,nparray_bucket_PredictedOutputPositive_Test,nparray_bucket_PredictedOutputNegative_Test,nparray_bucket_TruePositive_Test,nparray_bucket_FalsePositive_Test,nparray_bucket_FalseNegative_Test,nparray_bucket_TrueNegative_Test,nparray_bucket_accuracy_Test,nparray_bucket_precision_Test,nparray_bucket_recall_Test,nparray_bucket_negativePredictedValue_Test,nparray_bucket_trueNegativeRate_Test,dict_VolumeID_MetricBasic_Test=calculate_metrics2(\"Test\", nparray_Output_Test, nparray_PredictedOutput_Test, nparray_VolumeID_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make bins centered around their values by shifting them to the left\n",
    "bins_int_0_21 = [i-0.5 for i in range(22)]\n",
    "bins_float_0_1 = [i/20-0.025 for i in range(22)]\n",
    "print(\"bins_int_0_21\",bins_int_0_21)\n",
    "print(\"bins_float_0_1\",bins_float_0_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# overlay train and test we expect train to be better than test \n",
    "def plot_train_test_OutputPositive(nparray_bucket_OutputPositive_Train,nparray_bucket_OutputPositive_Test,modelName):\n",
    "    plt.hist(nparray_bucket_OutputPositive_Train,bins=bins_int_0_21,density=True,alpha=1,color=\"blue\",histtype='step',label=\"Train\")\n",
    "    plt.hist(nparray_bucket_OutputPositive_Test,bins=bins_int_0_21,density=True,alpha=1,color=\"red\",histtype='step',label=\"Test\") \n",
    "    plt.ylabel('Nr of buckets')\n",
    "    plt.xlabel('Bucket OutputPositive')\n",
    "    plt.title(modelName)\n",
    "    plt.legend(loc='upper left')\n",
    "    #plt.ylim(bottom=0,top=250)\n",
    "    #plt.show()\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_OutputPositive.\"+extension)\n",
    "# done function\n",
    "if doPlotMetrics2:\n",
    "    plot_train_test_OutputPositive(nparray_bucket_OutputPositive_Train,nparray_bucket_OutputPositive_Test,modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay train and test we expect train to be better than test \n",
    "def plot_train_test_PredictedOutputPositive(nparray_bucket_PredictedOutputPositive_Train,nparray_bucket_PredictedOutputPositive_Test,modelName):\n",
    "    plt.hist(nparray_bucket_PredictedOutputPositive_Train,bins=bins_int_0_21,density=True,alpha=1,color=\"blue\",histtype='step',label=\"Train\")\n",
    "    plt.hist(nparray_bucket_PredictedOutputPositive_Test,bins=bins_int_0_21,density=True,alpha=1,color=\"red\",histtype='step',label=\"Test\")\n",
    "    plt.ylabel('Nr of buckets')\n",
    "    plt.xlabel('Bucket PredictedOutputPositive')\n",
    "    plt.title(modelName)\n",
    "    plt.legend(loc='upper left')\n",
    "    #plt.ylim(bottom=0,top=250)\n",
    "    #plt.show()\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_PredictedOutputPositive.\"+extension)\n",
    "# done function\n",
    "if doPlotMetrics2:\n",
    "    plot_train_test_PredictedOutputPositive(nparray_bucket_PredictedOutputPositive_Train,nparray_bucket_PredictedOutputPositive_Test,modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doCalculateMetrics2:\n",
    "    fig, ax = plt.subplots()\n",
    "    h=ax.hist2d(\n",
    "        nparray_bucket_OutputPositive_Train,\n",
    "        nparray_bucket_PredictedOutputPositive_Train,\n",
    "        bins=[bins_int_0_21,bins_int_0_21],\n",
    "        cmin=1\n",
    "        )\n",
    "    plt.colorbar(h[3], ax=ax)\n",
    "    plt.xlabel(\"Output Positive\")\n",
    "    plt.ylabel(\"Predicted Output Positive\")\n",
    "    plt.title(modelName+\" Train\")\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_OutputPositive_vs_PredictedOutputPositive_Train.\"+extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doCalculateMetrics2:\n",
    "    fig, ax = plt.subplots()\n",
    "    h=ax.hist2d(\n",
    "        nparray_bucket_OutputPositive_Test,\n",
    "        nparray_bucket_PredictedOutputPositive_Test,\n",
    "        bins=[bins_int_0_21,bins_int_0_21],\n",
    "        cmin=1\n",
    "        )\n",
    "    plt.colorbar(h[3], ax=ax)\n",
    "    plt.xlabel(\"Output Positive\")\n",
    "    plt.ylabel(\"Predicted Output Positive\")\n",
    "    plt.title(modelName+\" Test\")\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_OutputPositive_vs_PredictedOutputPositive_Test.\"+extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay train and test we expect train to be better than test \n",
    "def plot_train_test_OutputNegative(nparray_bucket_OutputNegative_Train,nparray_bucket_OutputNegative_Test,modelName):\n",
    "    plt.hist(nparray_bucket_OutputNegative_Train,bins=bins_int_0_21,density=True,alpha=1,color=\"blue\",histtype='step',label=\"Train\")\n",
    "    plt.hist(nparray_bucket_OutputNegative_Test,bins=bins_int_0_21,density=True,alpha=1,color=\"red\",histtype='step',label=\"Test\")\n",
    "    plt.ylabel('Nr of buckets')\n",
    "    plt.xlabel('Bucket OutputNegative')\n",
    "    plt.title(modelName)\n",
    "    plt.legend(loc='upper left')\n",
    "    #plt.ylim(bottom=0,top=250)\n",
    "    #plt.show()\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_OutputNegative.\"+extension)\n",
    "# done function\n",
    "if doPlotMetrics2:\n",
    "    plot_train_test_OutputNegative(nparray_bucket_OutputNegative_Train,nparray_bucket_OutputNegative_Test,modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay train and test we expect train to be better than test \n",
    "def plot_train_test_PredictedOutputPositive(nparray_bucket_PredictedOutputPositive_Train,nparray_bucket_PredictedOutputPositive_Test,modelName):\n",
    "    plt.hist(nparray_bucket_PredictedOutputPositive_Train,bins=bins_int_0_21,density=True,alpha=1,color=\"blue\",histtype='step',label=\"Train\")\n",
    "    plt.hist(nparray_bucket_PredictedOutputPositive_Test,bins=bins_int_0_21,density=True,alpha=1,color=\"red\",histtype='step',label=\"Test\")\n",
    "    plt.ylabel('Nr of buckets')\n",
    "    plt.xlabel('Bucket PredictedOutputPositive')\n",
    "    plt.title(modelName)\n",
    "    plt.legend(loc='upper left')\n",
    "    #plt.ylim(bottom=0,top=250)\n",
    "    #plt.show()\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_PredictedOutputPositive.\"+extension)\n",
    "# done function\n",
    "if doPlotMetrics2:\n",
    "    plot_train_test_PredictedOutputPositive(nparray_bucket_PredictedOutputPositive_Train,nparray_bucket_PredictedOutputPositive_Test,modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay train and test we expect train to be better than test \n",
    "def plot_train_test_PredictedOutputNegative(nparray_bucket_PredictedOutputNegative_Train,nparray_bucket_PredictedOutputNegative_Test,modelName):\n",
    "    plt.hist(nparray_bucket_PredictedOutputNegative_Train,bins=bins_int_0_21,density=True,alpha=1,color=\"blue\",histtype='step',label=\"Train\")\n",
    "    plt.hist(nparray_bucket_PredictedOutputNegative_Test,bins=bins_int_0_21,density=True,alpha=1,color=\"red\",histtype='step',label=\"Test\")\n",
    "    plt.ylabel('Nr of buckets')\n",
    "    plt.xlabel('Bucket PredictedOutputNegative')\n",
    "    plt.title(modelName)\n",
    "    plt.legend(loc='upper left')\n",
    "    #plt.ylim(bottom=0,top=250)\n",
    "    #plt.show()\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_PredictedOutputNegative.\"+extension)\n",
    "# done function\n",
    "if doPlotMetrics2:\n",
    "    plot_train_test_PredictedOutputNegative(nparray_bucket_PredictedOutputNegative_Train,nparray_bucket_PredictedOutputNegative_Test,modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay train and test we expect train to be better than test \n",
    "def plot_train_test_TruePositive(nparray_bucket_TruePositive_Train,nparray_bucket_TruePositive_Test,modelName):\n",
    "    plt.hist(nparray_bucket_TruePositive_Train,bins=bins_int_0_21,density=True,alpha=1,color=\"blue\",histtype='step',label=\"Train\")\n",
    "    plt.hist(nparray_bucket_TruePositive_Test,bins=bins_int_0_21,density=True,alpha=1,color=\"red\",histtype='step',label=\"Test\")\n",
    "    plt.ylabel('Nr of buckets')\n",
    "    plt.xlabel('Bucket TruePositive')\n",
    "    plt.title(modelName)\n",
    "    plt.legend(loc='upper left')\n",
    "    #plt.ylim(bottom=0,top=250)\n",
    "    #plt.show()\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_TruePositive.\"+extension)\n",
    "# done function\n",
    "if doPlotMetrics2:\n",
    "    plot_train_test_TruePositive(nparray_bucket_TruePositive_Train,nparray_bucket_TruePositive_Test,modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay train and test we expect train to be better than test \n",
    "def plot_train_test_FalsePositive(nparray_bucket_FalsePositive_Train,nparray_bucket_FalsePositive_Test,modelName):\n",
    "    plt.hist(nparray_bucket_FalsePositive_Train,bins=bins_int_0_21,density=True,alpha=1,color=\"blue\",histtype='step',label=\"Train\")\n",
    "    plt.hist(nparray_bucket_FalsePositive_Test,bins=bins_int_0_21,density=True,alpha=1,color=\"red\",histtype='step',label=\"Test\")\n",
    "    plt.ylabel('Nr of buckets')\n",
    "    plt.xlabel('Bucket FalsePositive')\n",
    "    plt.title(modelName)\n",
    "    plt.legend(loc='upper left')\n",
    "    #plt.ylim(bottom=0,top=250)\n",
    "    #plt.show()\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_FalsePositive.\"+extension)\n",
    "# done function\n",
    "if doPlotMetrics2:\n",
    "    plot_train_test_FalsePositive(nparray_bucket_FalsePositive_Train,nparray_bucket_FalsePositive_Test,modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay train and test we expect train to be better than test \n",
    "def plot_train_test_FalseNegative(nparray_bucket_FalseNegative_Train,nparray_bucket_FalseNegative_Test,modelName):\n",
    "    plt.hist(nparray_bucket_FalseNegative_Train,bins=bins_int_0_21,density=True,alpha=1,color=\"blue\",histtype='step',label=\"Train\")\n",
    "    plt.hist(nparray_bucket_FalseNegative_Test,bins=bins_int_0_21,density=True,alpha=1,color=\"red\",histtype='step',label=\"Test\")\n",
    "    plt.ylabel('Nr of buckets')\n",
    "    plt.xlabel('Bucket FalseNegative')\n",
    "    plt.title(modelName)\n",
    "    plt.legend(loc='upper left')\n",
    "    #plt.ylim(bottom=0,top=250)\n",
    "    #plt.show()\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_FalseNegative.\"+extension)\n",
    "# done function\n",
    "if doPlotMetrics2:\n",
    "    plot_train_test_FalseNegative(nparray_bucket_FalseNegative_Train,nparray_bucket_FalseNegative_Test,modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay train and test we expect train to be better than test \n",
    "def plot_train_test_TrueNegative(nparray_bucket_TrueNegative_Train,nparray_bucket_TrueNegative_Test,modelName):\n",
    "    plt.hist(nparray_bucket_TrueNegative_Train,bins=bins_int_0_21,density=True,alpha=1,color=\"blue\",histtype='step',label=\"Train\")\n",
    "    plt.hist(nparray_bucket_TrueNegative_Test,bins=bins_int_0_21,density=True,alpha=1,color=\"red\",histtype='step',label=\"Test\")\n",
    "    plt.ylabel('Nr of buckets')\n",
    "    plt.xlabel('Bucket TrueNegative')\n",
    "    plt.title(modelName)\n",
    "    plt.legend(loc='upper left')\n",
    "    #plt.ylim(bottom=0,top=250)\n",
    "    #plt.show()\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_TrueNegative.\"+extension)\n",
    "# done function\n",
    "if doPlotMetrics2:\n",
    "    plot_train_test_TrueNegative(nparray_bucket_TrueNegative_Train,nparray_bucket_TrueNegative_Test,modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay train and test we expect train to be better than test \n",
    "def plot_train_test_accuracy(nparray_bucket_accuracy_Train,nparray_bucket_accuracy_Test,modelName):\n",
    "    plt.hist(nparray_bucket_accuracy_Train,bins=bins_float_0_1,density=True,alpha=1,color=\"blue\",histtype='step',label=\"Train\")\n",
    "    plt.hist(nparray_bucket_accuracy_Test,bins=bins_float_0_1,density=True,alpha=1,color=\"red\",histtype='step',label=\"Test\")\n",
    "    plt.ylabel('Nr of buckets')\n",
    "    plt.xlabel('Bucket accuracy')\n",
    "    plt.title(modelName)\n",
    "    plt.legend(loc='upper left')\n",
    "    #plt.ylim(bottom=0,top=250)\n",
    "    #plt.show()\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_accuracy.\"+extension)\n",
    "# done function\n",
    "if doPlotMetrics2:\n",
    "    plot_train_test_accuracy(nparray_bucket_accuracy_Train,nparray_bucket_accuracy_Test,modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay train and test we expect train to be better than test \n",
    "def plot_train_test_precision(nparray_bucket_precision_Train,nparray_bucket_precision_Test,modelName):\n",
    "    plt.hist(nparray_bucket_precision_Train,bins=bins_float_0_1,density=True,alpha=1,color=\"blue\",histtype='step',label=\"Train\")\n",
    "    plt.hist(nparray_bucket_precision_Test,bins=bins_float_0_1,density=True,alpha=1,color=\"red\",histtype='step',label=\"Test\")\n",
    "    plt.ylabel('Nr of buckets')\n",
    "    plt.xlabel('Bucket precision')\n",
    "    plt.title(modelName)\n",
    "    plt.legend(loc='upper left')\n",
    "    #plt.ylim(bottom=0,top=250)\n",
    "    #plt.show()\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_precision.\"+extension)\n",
    "# done function\n",
    "if doPlotMetrics2:\n",
    "    plot_train_test_precision(nparray_bucket_precision_Train,nparray_bucket_precision_Test,modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay train and test we expect train to be better than test \n",
    "def plot_train_test_recall(nparray_bucket_recall_Train,nparray_bucket_recall_Test,modelName):\n",
    "    plt.hist(nparray_bucket_recall_Train,bins=bins_float_0_1,density=True,alpha=1,color=\"blue\",histtype='step',label=\"Train\")\n",
    "    plt.hist(nparray_bucket_recall_Test,bins=bins_float_0_1,density=True,alpha=1,color=\"red\",histtype='step',label=\"Test\")  \n",
    "    plt.ylabel('Nr of buckets')\n",
    "    plt.xlabel('Bucket recall')\n",
    "    plt.title(modelName)\n",
    "    plt.legend(loc='upper left')\n",
    "    #plt.ylim(bottom=0,top=250)\n",
    "    #plt.show()\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_recall.\"+extension)\n",
    "# done function\n",
    "if doPlotMetrics2:\n",
    "    plot_train_test_recall(nparray_bucket_recall_Train,nparray_bucket_recall_Test,modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay train and test we expect train to be better than test \n",
    "def plot_train_test_negativePredictedValue(nparray_bucket_negativePredictedValue_Train,nparray_bucket_negativePredictedValue_Test,modelName):\n",
    "    plt.hist(nparray_bucket_negativePredictedValue_Train,bins=bins_float_0_1,density=True,alpha=1,color=\"blue\",histtype='step',label=\"Train\")\n",
    "    plt.hist(nparray_bucket_negativePredictedValue_Test,bins=bins_float_0_1,density=True,alpha=1,color=\"red\",histtype='step',label=\"Test\")\n",
    "    plt.ylabel('Nr of buckets')\n",
    "    plt.xlabel('Bucket negativePredictedValue')\n",
    "    plt.title(modelName)\n",
    "    plt.legend(loc='upper left')\n",
    "    #plt.ylim(bottom=0,top=250)\n",
    "    #plt.show()\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_negativePredictedValue.\"+extension)\n",
    "# done function\n",
    "if doPlotMetrics2:\n",
    "    plot_train_test_negativePredictedValue(nparray_bucket_negativePredictedValue_Train,nparray_bucket_negativePredictedValue_Test,modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay train and test we expect train to be better than test \n",
    "def plot_train_test_trueNegativeRate(nparray_bucket_trueNegativeRate_Train,nparray_bucket_trueNegativeRate_Test,modelName):\n",
    "    plt.hist(nparray_bucket_trueNegativeRate_Train,bins=bins_float_0_1,density=True,alpha=1,color=\"blue\",histtype='step',label=\"Train\")\n",
    "    plt.hist(nparray_bucket_trueNegativeRate_Test,bins=bins_float_0_1,density=True,alpha=1,color=\"red\",histtype='step',label=\"Test\")\n",
    "    plt.ylabel('Nr of buckets')\n",
    "    plt.xlabel('Bucket trueNegativeRate')\n",
    "    plt.title(modelName)\n",
    "    plt.legend(loc='upper left')\n",
    "    #plt.ylim(bottom=0,top=250)\n",
    "    #plt.show()\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_trueNegativeRate.\"+extension)\n",
    "# done function\n",
    "if doPlotMetrics2:\n",
    "    plot_train_test_trueNegativeRate(nparray_bucket_trueNegativeRate_Train,nparray_bucket_trueNegativeRate_Test,modelName)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doCalculateMetrics2:\n",
    "    fig, ax = plt.subplots()\n",
    "    h=ax.hist2d(\n",
    "        nparray_bucket_OutputPositive_Train,\n",
    "        nparray_bucket_PredictedOutputPositive_Train,\n",
    "        bins=[bins_int_0_21,bins_int_0_21],\n",
    "        cmin=1\n",
    "        )\n",
    "    plt.colorbar(h[3], ax=ax)\n",
    "    plt.xlabel(\"Output Positive\")\n",
    "    plt.ylabel(\"Predicted Output Positive\")\n",
    "    plt.title(modelName+\" Train\")\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_OutputPositive_vs_PredictedOutputPositive_Train.\"+extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doCalculateMetrics2:\n",
    "    fig, ax = plt.subplots()\n",
    "    h=ax.hist2d(\n",
    "        nparray_bucket_OutputPositive_Test,\n",
    "        nparray_bucket_PredictedOutputPositive_Test,\n",
    "        bins=[bins_int_0_21,bins_int_0_21],\n",
    "        cmin=1\n",
    "        )\n",
    "    plt.colorbar(h[3], ax=ax)\n",
    "    plt.xlabel(\"Output Positive\")\n",
    "    plt.ylabel(\"Predicted Output Positive\")\n",
    "    plt.title(modelName+\" Test\")\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_OutputPositive_vs_PredictedOutputPositive_Test.\"+extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doCalculateMetrics2:\n",
    "    fig, ax = plt.subplots()\n",
    "    h=ax.hist2d(\n",
    "        nparray_bucket_OutputNegative_Train,\n",
    "        nparray_bucket_PredictedOutputNegative_Train,\n",
    "        bins=[bins_int_0_21,bins_int_0_21],\n",
    "        cmin=1,\n",
    "        # norm=LogNorm()\n",
    "        )\n",
    "    plt.colorbar(h[3], ax=ax)\n",
    "    plt.xlabel(\"Output Negative\")\n",
    "    plt.ylabel(\"Predicted Output Negative\")\n",
    "    plt.title(modelName+\" Train\")\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_OutputNegative_vs_PredictedOutputNegative_Train.\"+extension) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doCalculateMetrics2:\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    h=ax.hist2d(\n",
    "        nparray_bucket_OutputNegative_Test,\n",
    "        nparray_bucket_PredictedOutputNegative_Test,\n",
    "        bins=[bins_int_0_21,bins_int_0_21],\n",
    "        cmin=1\n",
    "        )\n",
    "    plt.colorbar(h[3], ax=ax)\n",
    "    plt.xlabel(\"Output Negative\")\n",
    "    plt.ylabel(\"Predicted Output Negative\")\n",
    "    plt.title(modelName+\" Test\")\n",
    "    outputFileNameStem=\"NN_5_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    for extension in extensions.split(\",\"):\n",
    "        plt.savefig(outputFolderName+\"/\"+outputFileNameStem+\"_histo_OutputNegative_vs_PredictedOutputNegative_Test.\"+extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doCalculateMetrics2:\n",
    "    # Confusion matrix Train with no reweighting\n",
    "    nbHitTP=np.sum(nparray_bucket_TruePositive_Train)\n",
    "    nbHitFP=np.sum(nparray_bucket_FalsePositive_Train)\n",
    "    nbHitFN=np.sum(nparray_bucket_FalseNegative_Train)\n",
    "    nbHitTN=np.sum(nparray_bucket_TrueNegative_Train)\n",
    "    nrHitsAll=nbHitTP+nbHitFP+nbHitFN+nbHitTN\n",
    "    nrHitsPercentTP=100*nbHitTP/nrHitsAll\n",
    "    nrHitsPercentFP=100*nbHitFP/nrHitsAll\n",
    "    nrHitsPercentFN=100*nbHitFN/nrHitsAll\n",
    "    nrHitsPercentTN=100*nbHitTN/nrHitsAll\n",
    "    print(\"Train Hits. Percent TP=%.1f FP=%.1f FN=%.1f TN=%.1f\"%(nrHitsPercentTP,nrHitsPercentFP,nrHitsPercentFN,nrHitsPercentTN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doCalculateMetrics2:\n",
    "    # Confusion matrix Train with no reweighting\n",
    "    nbHitTP=np.sum(nparray_bucket_TruePositive_Test)\n",
    "    nbHitFP=np.sum(nparray_bucket_FalsePositive_Test)\n",
    "    nbHitFN=np.sum(nparray_bucket_FalseNegative_Test)\n",
    "    nbHitTN=np.sum(nparray_bucket_TrueNegative_Test)\n",
    "    nrHitsAll=nbHitTP+nbHitFP+nbHitFN+nbHitTN\n",
    "    nrHitsPercentTP=100*nbHitTP/nrHitsAll\n",
    "    nrHitsPercentFP=100*nbHitFP/nrHitsAll\n",
    "    nrHitsPercentFN=100*nbHitFN/nrHitsAll\n",
    "    nrHitsPercentTN=100*nbHitTN/nrHitsAll\n",
    "    print(\"Test Hits. Percent TP=%.1f FP=%.1f FN=%.1f TN=%.1f\"%(nrHitsPercentTP,nrHitsPercentFP,nrHitsPercentFN,nrHitsPercentTN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(TP,FP,FN,TN,debug=False):\n",
    "    \n",
    "    if debug:\n",
    "        print(\"TP\",TP,\"FP\",FP,\"FN\",FN,\"TN\",TN)\n",
    "        \n",
    "    nbTotal=TP+FP+FN+TN\n",
    "    #\n",
    "    TPPercent=100*TP/nbTotal\n",
    "    FPPercent=100*FP/nbTotal\n",
    "    FNPercent=100*FN/nbTotal\n",
    "    TNPercent=100*TN/nbTotal\n",
    "    if debug:\n",
    "        print(\"TPPercent\",TPPercent,\"FPPercent\",FPPercent,\"FNPercent\",FNPercent,\"TNPercent\",TNPercent)\n",
    "        \n",
    "    #\n",
    "    OutputPositive=TP+FN \n",
    "    OutputNegative=FP+TN\n",
    "    PredictedOutputPositive=TP+FP\n",
    "    PredictedOutputNegative=FN+TN\n",
    "    if debug:\n",
    "        print(\"OutputPositive\",OutputPositive,\"OutputNegative\",OutputNegative,\"PredictedOutputPositive\",PredictedOutputPositive,\"PredictedOutputNegative\",PredictedOutputNegative)\n",
    "        \n",
    "    OutputPositivePercent=100*OutputPositive/nbTotal\n",
    "    OutputNegativePercent=100*OutputNegative/nbTotal    \n",
    "    PredictedOutputPositivePercent=100*PredictedOutputPositive/nbTotal        \n",
    "    PredictedOutputNegativePercent=100*PredictedOutputNegative/nbTotal\n",
    "    if debug:\n",
    "        print(\"OutputPositivePercent\",OutputPositivePercent,\"OutputNegativePercent\",OutputNegativePercent,\"PredictedOutputPositivePercent\",PredictedOutputPositivePercent,\"PredictedOutputNegativePercent\",PredictedOutputNegativePercent)\n",
    "        \n",
    "    # https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "    # accuracy=(TP+TN)/(TP+FP+FN+TN)=(TP+TN)/ALL, ALL=20 (20 hits in a bucket)\n",
    "    # precision=(TP)/(TP+FP)=(TP)/(all that are in reality positive)=efficiency from CERN \n",
    "    # e.g. there are 100 truth electrons, efficiency = what fraction of them are also reconstricted as electrons? \n",
    "    # recall=(TP)/(TP+FN)=(TP)/(all that are predicted positive) = one minus fake rate from CERN\n",
    "    # e.g. fake rate = I have reconstructed 100 electrons. What fraction of these are not in reality truth electrons\n",
    "    # fake rate = What fraction of reconstructed electrons are fake electrons?\n",
    "    # fake rate = (FN)/(TP+FN) = 1 - recall\n",
    "        \n",
    "    # accuracy\n",
    "    accuracy=(TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "    # precision\n",
    "    if (TP+FP)==0:\n",
    "        precision=0\n",
    "    else:\n",
    "        precision=(TP)/(TP+FP)\n",
    "    # done if\n",
    "        \n",
    "    # recall\n",
    "    if TP+FN==0:\n",
    "        recall=0\n",
    "    else:\n",
    "        recall=(TP)/(TP+FN)\n",
    "    # done if\n",
    "    \n",
    "    # precision for negative: negative predicted value\n",
    "    if TN+FN==0:\n",
    "        negativePredictedValue=0\n",
    "    else:\n",
    "        negativePredictedValue=(TN)/(TN+FN)\n",
    "    # done if\n",
    "        \n",
    "    # recall for negative: true negative rate\n",
    "    if TN+FP==0:\n",
    "        trueNegativeRate=0\n",
    "    else:\n",
    "        trueNegativeRate=(TN)/(TN+FP)\n",
    "    # done if \n",
    "    \n",
    "    if debug:\n",
    "        print(\"accuracy\",accuracy,\"precision\",precision,\"recall\",recall,\"negativePredictedValue\",negativePredictedValue,\"trueNegativeRate\",trueNegativeRate)\n",
    "        \n",
    "    # return only one dictionary\n",
    "    dict_var_value={}\n",
    "    dict_var_value[\"NbTotal\"]=nbTotal\n",
    "    dict_var_value[\"TP\"]=TP\n",
    "    dict_var_value[\"FP\"]=FP\n",
    "    dict_var_value[\"FN\"]=FN\n",
    "    dict_var_value[\"TN\"]=TN\n",
    "    dict_var_value[\"TPPercent\"]=TPPercent\n",
    "    dict_var_value[\"FPPercent\"]=FPPercent\n",
    "    dict_var_value[\"FNPercent\"]=FNPercent\n",
    "    dict_var_value[\"TNPercent\"]=TNPercent\n",
    "    dict_var_value[\"OutputPositive\"]=OutputPositive\n",
    "    dict_var_value[\"OutputNegative\"]=OutputNegative\n",
    "    dict_var_value[\"PredictedOutputPositive\"]=PredictedOutputPositive\n",
    "    dict_var_value[\"PredictedOutputNegative\"]=PredictedOutputNegative\n",
    "    dict_var_value[\"OutputPositivePercent\"]=OutputPositivePercent\n",
    "    dict_var_value[\"OutputNegativePercent\"]=OutputNegativePercent\n",
    "    dict_var_value[\"PredictedOutputPositivePercent\"]=PredictedOutputPositivePercent\n",
    "    dict_var_value[\"PredictedOutputNegativePercent\"]=PredictedOutputNegativePercent\n",
    "    dict_var_value[\"Accuracy\"]=accuracy\n",
    "    dict_var_value[\"Precision\"]=precision\n",
    "    dict_var_value[\"Recall\"]=recall\n",
    "    dict_var_value[\"NegativePredictedValue\"]=negativePredictedValue\n",
    "    dict_var_value[\"TrueNegativeRate\"]=trueNegativeRate\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Print dict_var_value\")\n",
    "        for var in sorted(dict_var_value.keys()):\n",
    "            print(\"var\",var,\"value\",dict_var_value[var])\n",
    "    \n",
    "    # all done, ready to return\n",
    "    return dict_var_value\n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics3(TrainOrTest, dict_VolumeID_MetricBasic):\n",
    "    if debug or verbose:\n",
    "        print(\"Start calculate_metrics3 for \",TrainOrTest)\n",
    "    \n",
    "    # calculate list of VolumeID in increasing order\n",
    "    list_VolumeID=sorted(dict_VolumeID_MetricBasic.keys())\n",
    "    \n",
    "    # now for each VolumeID we calculate all the metrics\n",
    "    # so for each metric we put the values for each VolumeID one after the other\n",
    "    # so we can put in a list and from the list make a nparray and save that nparray, for later to overlay plots\n",
    "    dict_var_list_value={}\n",
    "    # loop over volumes in their order\n",
    "    for VolumeID in list_VolumeID:\n",
    "        MetricBasic=dict_VolumeID_MetricBasic[VolumeID]\n",
    "        TP=MetricBasic[0]\n",
    "        FP=MetricBasic[1]\n",
    "        FN=MetricBasic[2]\n",
    "        TN=MetricBasic[3]\n",
    "        # from these 4 values, calculate the other metrics and figures of merit \n",
    "        dict_var_value=get_metrics(TP,FP,FN,TN,debug=debug)\n",
    "        for var in dict_var_value.keys():\n",
    "            value=dict_var_value[var]\n",
    "            if var not in dict_var_list_value.keys():\n",
    "                # create a list with one value, value for the current VolumeID\n",
    "                dict_var_list_value[var]=[value]\n",
    "            else:\n",
    "                # to the already existing list add value for the current VolumeID\n",
    "                dict_var_list_value[var].append(value)\n",
    "            # done if\n",
    "        # done for loop over var\n",
    "    # done for loop over VolumeID\n",
    "    \n",
    "    #\n",
    "    outputFileNameStem=outputFolderName+\"/\"+\"NN_7_\"+modelName+\"_\"+str(numberOfEpochs)+\"_\"+str(batchSize)\n",
    "    \n",
    "    # save the npparray_volume_id\n",
    "    nparray_volume_id=np.array(list_VolumeID)\n",
    "    p(\"nparray_volume_id\",nparray_volume_id)\n",
    "    var=\"VolumeID\"\n",
    "    outputFileNameNpy=outputFileNameStem+\"_nparray_\"+var+\"_\"+TrainOrTest+\".npy\"\n",
    "    np.save(outputFileNameNpy,nparray_volume_id)\n",
    "    \n",
    "    # for loop over var\n",
    "    for var in sorted(dict_var_list_value.keys()):\n",
    "        list_value=dict_var_list_value[var]\n",
    "        nparray_value=np.array(list_value)\n",
    "        #p(\"nparray_\"+var,nparray_value)\n",
    "        print(\"var\",var)\n",
    "        outputFileNameNpy=outputFileNameStem+\"_nparray_\"+var+\"_VolumeID_\"+TrainOrTest+\".npy\"\n",
    "        np.save(outputFileNameNpy,nparray_value)\n",
    "        # create plot here\n",
    "        plt.plot(nparray_value,marker=\"o\")\n",
    "        plt.xlabel(\"volume_id\")\n",
    "        plt.xticks(range(len(nparray_volume_id)),nparray_volume_id)\n",
    "        plt.ylabel(var)\n",
    "        plt.title(TrainOrTest)\n",
    "        #plt.legend(loc='best')\n",
    "        #plt.show()\n",
    "        for extension in \"png,pdf\".split(\",\"):\n",
    "            plt.savefig(outputFileNameStem+\"_plot_graph_\"+var+\"_VolumeID_\"+TrainOrTest+\".\"+extension)\n",
    "        # done for loop\n",
    "        plt.close()\n",
    "    # done for loop over var\n",
    "    \n",
    "    # calculate the percentage of the numbrer of hits in each VolumeID\n",
    "    nparray_nbTotal=np.array(dict_var_list_value[\"NbTotal\"])\n",
    "    sum_nbTotal=np.sum(nparray_nbTotal)\n",
    "    nparray_nbTotalPercent=100*nparray_nbTotal/sum_nbTotal\n",
    "    p(\"nparray_nbTotalPercent\",nparray_nbTotalPercent)\n",
    "    var=\"NbTotalPercent\"\n",
    "    outputFileNameNpy=outputFileNameStem+\"_nparray_\"+var+\"_VolumeID_\"+TrainOrTest+\".npy\"\n",
    "    np.save(outputFileNameNpy,nparray_nbTotalPercent)\n",
    "    # create plot here\n",
    "    plt.plot(nparray_value,marker=\"o\")\n",
    "    plt.xlabel(\"volume_id\")\n",
    "    plt.xticks(range(len(nparray_volume_id)),nparray_volume_id)\n",
    "    plt.ylabel(var)\n",
    "    plt.title(TrainOrTest)\n",
    "    #plt.legend(loc='best')\n",
    "    #plt.show()\n",
    "    for extension in \"png,pdf\".split(\",\"):\n",
    "        plt.savefig(outputFileNameStem+\"_plot_graph_\"+var+\"_VolumeID_\"+TrainOrTest+\".\"+extension)\n",
    "    # done for loop\n",
    "    plt.close()\n",
    "    \n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if doPlotMetrics3:\n",
    "    calculate_metrics3(\"Train\",dict_VolumeID_MetricBasic_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doPlotMetrics3:\n",
    "    calculate_metrics3(\"Test\",dict_VolumeID_MetricBasic_Test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
