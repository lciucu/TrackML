{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug=False\n",
    "verbose=True\n",
    "doBucketPlots=False\n",
    "doIndividual=True\n",
    "doProcessIndividual=True # applies minimum cut for output: 00, 04, 07, 10\n",
    "doMerge=FTrue\n",
    "doBalanceOld=False\n",
    "doProcessBalancedOld=False # for OutputBalanced do Min04, Min07, etc\n",
    "doBalance=True\n",
    "#\n",
    "list_minNbPositiveHit=[\n",
    "    #0,\n",
    "    #4,\n",
    "    #7,\n",
    "    10,\n",
    "]\n",
    "list_mergeOption=[\n",
    "    #[0,10],\n",
    "    #[10,20],\n",
    "    #[20,30],\n",
    "    #[30,40],\n",
    "    #[40,50],\n",
    "    #[50,60],\n",
    "    #[60,70],\n",
    "    #[70,80],\n",
    "    #[80,90],\n",
    "    #[90,100],\n",
    "    #[0,20],\n",
    "    #[0,30],\n",
    "    #[0,40],\n",
    "    #[0,50],\n",
    "    #[0,60],\n",
    "    #[0,70],\n",
    "    #[0,80],\n",
    "    #[0,90],\n",
    "    [0,100],\n",
    "]\n",
    "bucketSize=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folderStem=\"/Volumes/Luiza_SSD\"\n",
    "\n",
    "#\n",
    "inputFolderName=folderStem+\"/ATLAS/TrackML/data/ttbar_mu200-generic\"\n",
    "#outputFolderName=folderStem+\"/ATLAS/TrackML/output_general\"\n",
    "#outputFolderName=folderStem+\"/ATLAS/TrackML/output_again\"\n",
    "outputFolderName=folderStem+\"/ATLAS/TrackML/output_new\"\n",
    "# if output folder does not exist, create it\n",
    "if not os.path.exists(outputFolderName):\n",
    "    os.makedirs(outputFolderName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eventNumber_from_fileName(fileName):\n",
    "    eventNumber=fileName.replace(\"event\",\"\").replace(\"-hits.csv\",\"\")\n",
    "    return eventNumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate event numbers from my folder\n",
    "# lista goala in care sa pun numerele evenimentelor\n",
    "list_eventNumber=[]\n",
    "# The sorted() function returns a sorted list of the specified iterable object \n",
    "# Strings are sorted alphabetically, and numbers are sorted numerically\n",
    "# os operating system \n",
    "# hits ca sa nu am de doura ori evenimentul in lista\n",
    "for fileName in sorted(os.listdir(inputFolderName)):\n",
    "    if fileName.endswith(\"-hits.csv\"):\n",
    "        #print(fileName)\n",
    "        eventNumber=get_eventNumber_from_fileName(fileName)\n",
    "        #print(eventNumber)\n",
    "        list_eventNumber.append(eventNumber)\n",
    "# done for loop\n",
    "#list_eventNumber=list_eventNumber[0:20] # keep only first 10 events\n",
    "# list_eventNumber=[\"000000007\"] # keep only one event for test\n",
    "print(\"All events available in my folder. list_eventNumber\", list_eventNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p(name,nparray):\n",
    "    print(\"Start\",name)\n",
    "    print(nparray)\n",
    "    print(\"End\",name,\"shape\",nparray.shape,\"dtype\",nparray.dtype,\"type\",type(nparray))\n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_eta(px,py,pz,m):\n",
    "    p=np.sqrt(px*px+py*py+pz*pz)\n",
    "    theta=np.arccos(pz/p)\n",
    "    eta=-np.log(np.tan(theta/2))\n",
    "    return eta\n",
    "# done function\n",
    "\n",
    "# example\n",
    "eta=calculate_eta(3,4,100000,0)\n",
    "eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this runs only once per event\n",
    "def buildAnnoyIndex(nparray_position,metric=\"angular\",ntrees=10,debug=False): \n",
    "    \n",
    "    numberDimension=nparray_position.shape[1] # 3 (x,y,z)\n",
    "    if debug:\n",
    "        print(\"numberDimension\",numberDimension,\"metric\",metric)\n",
    "    index=AnnoyIndex(numberDimension,metric)\n",
    "    if debug:\n",
    "        print(\"type(index)\",type(index))\n",
    "        print(\"enumerate data\")\n",
    "    # add each hit to the index\n",
    "    for i,position in enumerate(nparray_position):\n",
    "        if debug:\n",
    "            print(\"i\",i,\"position\",position)\n",
    "        index.add_item(i,position)\n",
    "    # done for loop over hits\n",
    "    # build the index with 10 trees\n",
    "    index.build(ntrees) # 10 trees\n",
    "    return index\n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_output_from_df_hits(df_hits,index):\n",
    "    list_nparray_input=[]\n",
    "    list_nparray_output=[]\n",
    "    list_nparray_eta=[]\n",
    "    list_nparray_volumeID=[]\n",
    "    list_nparray_layerID=[]\n",
    "    list_nparray_queryIndex=[]\n",
    "    list_nparray_nbPositiveHit=[]\n",
    "    \n",
    "    nparray_volume_id=df_hits[\"volume_id\"].values\n",
    "    nparray_layer_id=df_hits[\"layer_id\"].values \n",
    "\n",
    "    counterBucket=0\n",
    "    \n",
    "    #my_title=\"FirstLayer\"\n",
    "    my_title=\"All\"\n",
    "    for i in range(len(df_hits)):\n",
    "        if (\n",
    "            True # for each hit create a bucket\n",
    "            #(nparray_volume_id[i]==8 and nparray_layer_id[i]==2) \n",
    "            #or (nparray_volume_id[i]==7 and nparray_layer_id[i]==2)\n",
    "            #or (nparray_volume_id[i]==9 and nparray_layer_id[i]==14)\n",
    "            #or (nparray_volume_id[i]==16 and nparray_layer_id[i]==2)\n",
    "            #or (nparray_volume_id[i]==12 and nparray_layer_id[i]==2)\n",
    "            #or (nparray_volume_id[i]==18 and nparray_layer_id[i]==12)\n",
    "            #or (nparray_volume_id[i]==14 and nparray_layer_id[i]==12)\n",
    "            #or (nparray_volume_id[i]==17 and nparray_layer_id[i]==4)\n",
    "            )==False:\n",
    "            continue\n",
    "\n",
    "        counterBucket+=1\n",
    "        \n",
    "        #if (counterBucket<1)==False:\n",
    "        #    continue\n",
    "        \n",
    "        #if (counterBucket%10000==1)==False:\n",
    "        #    continue\n",
    "        \n",
    "        if debug or counterBucket%20000==1:\n",
    "            print(\"i\",i,\"counterBucket\",counterBucket)\n",
    "            \n",
    "        # using annoy to find the 20 nearest neighboring hits by angle to this hit\n",
    "        list_index=index.get_nns_by_item(i,bucketSize)\n",
    "        if debug:\n",
    "            print(\"list_index\",list_index)\n",
    "            \n",
    "        # create bucket\n",
    "        df_bucket=df_hits.iloc[list_index]\n",
    "        df_query=df_hits.iloc[i]\n",
    "        \n",
    "        if debug:\n",
    "            print(\"df_bucket\",df_bucket)\n",
    "            print(\"df_query\",df_query)\n",
    "            print(\"df_query_hit_id\",df_query[\"hit_id\"])\n",
    "        \n",
    "        # find the position (index) in the bucket of the query point and store it for later user\n",
    "        query_hitID=df_query[\"hit_id\"].astype(np.uint64)\n",
    "        if debug:\n",
    "            print(\"query_hitID\",query_hitID) \n",
    "        # for loop over hits in bucket and remember at which position (index) inside the bucket we have the query point\n",
    "        nparray_hitID=df_bucket[\"hit_id\"].values.astype(np.uint64)\n",
    "        for j in range(bucketSize):\n",
    "            current_hitID=nparray_hitID[j]\n",
    "            if debug:\n",
    "                print(\"current_hitID\",current_hitID)\n",
    "            if current_hitID==query_hitID:\n",
    "                queryIndex=j\n",
    "                break\n",
    "        # done for loop over hits in bucket\n",
    "        if debug:\n",
    "            print(\"queryIndex\",queryIndex)\n",
    "        # create numpy array for queryIndex\n",
    "        list_queryIndex=[queryIndex]\n",
    "        nparray_queryIndex=np.array(list_queryIndex).astype(np.uint8)\n",
    "        \n",
    "        # create NN input for this one bucket\n",
    "        # convert from default in pandas from np.float64 to np.float32, which is enough for the 4-5 digits it has\n",
    "        nparray_input=df_bucket[[\"x\",\"y\",\"z\"]].values.astype(np.float32).flatten()\n",
    "        if debug:\n",
    "            p(\"nparray_input\",nparray_input)\n",
    "            \n",
    "        # as for input, for volumeID\n",
    "        nparray_volumeID=df_bucket[\"volume_id\"].values.astype(np.uint8)\n",
    "        \n",
    "        # as for input, for layerID\n",
    "        nparray_layerID=df_bucket[\"layer_id\"].values.astype(np.uint8)\n",
    "            \n",
    "        # create NN output for this one bucket\n",
    "        # identify particle with the largest number of hits in this bucket\n",
    "        nparray_particleID=df_bucket[\"particle_id\"].values\n",
    "\n",
    "        if debug:\n",
    "            p(\"nparray_particleID\",nparray_particleID)\n",
    "        dict_particleID_counterParticleID={}\n",
    "        for particleID in nparray_particleID:\n",
    "            if particleID not in dict_particleID_counterParticleID:\n",
    "                dict_particleID_counterParticleID[particleID]=1\n",
    "            else:\n",
    "                dict_particleID_counterParticleID[particleID]+=1\n",
    "        if debug:\n",
    "            print(\"dict_particleID_counterParticleID\",dict_particleID_counterParticleID)\n",
    "          \n",
    "        # find the maximum value of the counters\n",
    "        particleIDWithMaxHits=0\n",
    "        counterParticleIDWithMaxHits=0\n",
    "        for particleID in dict_particleID_counterParticleID:\n",
    "            counterParticleID=dict_particleID_counterParticleID[particleID]\n",
    "            if counterParticleID>counterParticleIDWithMaxHits:\n",
    "                counterParticleIDWithMaxHits=counterParticleID\n",
    "                particleIDWithMaxHits=particleID\n",
    "        if debug:\n",
    "            print(\"counterBucket\",counterBucket,\"particleIDWithMaxHits\",particleIDWithMaxHits,\"counterParticleIDWithMaxHits\",counterParticleIDWithMaxHits)           \n",
    "        \n",
    "        # store the number of positive hits in this bucket, as it will be used a lot later\n",
    "        nparray_nbPositiveHit=np.array([counterParticleIDWithMaxHits]).astype(np.uint8)\n",
    "        \n",
    "        # calculate eta\n",
    "        nparray_tp=df_bucket[[\"tpx\",\"tpy\",\"tpz\"]].values.astype(np.float32)\n",
    "        if debug:\n",
    "            p(\"nparray_tp\",nparray_tp)\n",
    "        \n",
    "        # calculate eta for each hit and store it\n",
    "        if debug:\n",
    "            print(\"Start eta\")\n",
    "            print(\"particleIDWithMaxHits\",particleIDWithMaxHits)\n",
    "            \n",
    "        # for loop over hits in a bucket\n",
    "        # create list of eta for each hit in the bucket\n",
    "        list_eta=[]\n",
    "        for j,particleID in enumerate(nparray_particleID):\n",
    "            #print(\"j\",j,\"particleID\",particleID)\n",
    "            list_tp=nparray_tp[j]\n",
    "            # print(\"list_tp\",list_tp)\n",
    "            tpx=list_tp[0]\n",
    "            tpy=list_tp[1]\n",
    "            tpz=list_tp[2]\n",
    "            # let's assume mass of particle is zero (good approximationf for pions, kaons forming tracks)\n",
    "            m=0.0\n",
    "            #print(\"j\",j,\"tpx=%.4f, tpy=%.4f, tpz=%.4f\"%(tpx,tpy,tpz))\n",
    "            # \n",
    "            eta=calculate_eta(tpx,tpy,tpz,m)\n",
    "            if debug:\n",
    "                print(\"j\",j,\"particleID\",particleID,\"tpx=%.10f, tpy=%.10f, tpz=%.10f, eta=%.10f\"%(tpx,tpy,tpz,eta))\n",
    "            list_eta.append(eta)\n",
    "        # done for loop over hits in bucket\n",
    "        # by printing out I confirmed that all hits belonging to the particle have exactly the same tpx,tpy,tpz\n",
    "        # list_eta has bucketSize elements, so now make it a numpy array and convert it to type float 32\n",
    "        nparray_eta=np.array(list_eta).astype(np.float32)\n",
    "        if debug:\n",
    "            p(\"nparray_eta\",nparray_eta)\n",
    "        \n",
    "        # create nparray_output\n",
    "        list_output=[]\n",
    "        \n",
    "        # loop over every hit in the bucket\n",
    "        for particleID in nparray_particleID:      \n",
    "            if particleID==particleIDWithMaxHits:\n",
    "                list_output.append(1)\n",
    "            else:\n",
    "                list_output.append(-1)\n",
    "            # done if\n",
    "        # done for loop for each hit in the bucket\n",
    "        if debug:        \n",
    "            print(\"list_output\",list_output)\n",
    "        # create nparray for output from list and convert from the default int64 to int8\n",
    "        nparray_output=np.array(list_output).astype(np.int8)\n",
    "        if debug:\n",
    "            print(\"nparray_output\",nparray_output)\n",
    "            \n",
    "        if doBucketPlots and counterBucket%1000==0:\n",
    "            print(\"i\",i,\"counterBucket\",counterBucket)\n",
    "            \n",
    "            outputFileNameStem=outputFolderName+\"/plot_vis_e_\"+eventNumber+\"_i_\"+str(i)+\"_c_\"+str(counterBucket)+\"_\"+my_title+\"_\"\n",
    "            if True:\n",
    "                # plot x vs y\n",
    "                plt.plot(df_bucket.x,df_bucket.y,\"o\",color=\"red\")\n",
    "                plt.plot(df_query.x,df_query.y,\"D\",color=\"blue\")\n",
    "                plt.xlabel(\"x [mm]\")\n",
    "                plt.ylabel(\"y [mm]\")\n",
    "                plt.xlim(left=-1050,right=1050)\n",
    "                plt.ylim(bottom=-1050,top=1050)\n",
    "                #plt.plot(0,0,\"r+\")\n",
    "                plt.title(my_title+\". y vs x. Blue=query. eta=%.2f. i=%.0f\"%(eta,i))\n",
    "                # plt.show()\n",
    "                plotBucketFileNameStem=outputFileNameStem+\"y_vs_x\"\n",
    "                plt.savefig(plotBucketFileNameStem+\".png\")\n",
    "                #plt.savefig(plotBucketFileNameStem+\".pdf\")\n",
    "                plt.close()\n",
    "            # done if\n",
    "            if True:\n",
    "                # plot z vs r\n",
    "                df_bucket_r=np.sqrt(df_bucket.x**2+df_bucket.y**2)\n",
    "                df_query_r=np.sqrt(df_query.x**2+df_query.y**2)\n",
    "                plt.plot(df_bucket.z,df_bucket_r,\"o\",color=\"red\")\n",
    "                plt.plot(df_query.z,df_query_r,\"D\",color=\"blue\")\n",
    "                plt.xlabel(\"z [mm]\")\n",
    "                plt.ylabel(\"radius [mm]\")\n",
    "                plt.xlim(left=-3050,right=3050)\n",
    "                plt.ylim(bottom=-1050,top=1050)\n",
    "                #plt.plot(0,0,\"r+\")\n",
    "                plt.title(my_title+\". r vs z. Blue=query. eta=%.2f. i=%.0f\"%(eta,i))\n",
    "                # plt.show()\n",
    "                plotBucketFileNameStem=outputFileNameStem+\"r_vs_z\"\n",
    "                plt.savefig(plotBucketFileNameStem+\".png\")\n",
    "                #plt.savefig(plotBucketFileNameStem+\".pdf\")\n",
    "                plt.close()\n",
    "            # done if\n",
    "        # done if\n",
    "            \n",
    "        # add for this current bucket to the list for the entire event (which has several buckets)\n",
    "        list_nparray_input.append(nparray_input)\n",
    "        list_nparray_output.append(nparray_output)\n",
    "        list_nparray_eta.append(nparray_eta)\n",
    "        list_nparray_volumeID.append(nparray_volumeID)\n",
    "        list_nparray_layerID.append(nparray_layerID)\n",
    "        list_nparray_queryIndex.append(nparray_queryIndex)\n",
    "        list_nparray_npPositiveHit.append(nparray_nbPositiveHit)\n",
    "    # done for loop over hits in the event, so that for each hit we make a bucket\n",
    "    \n",
    "    # transform list of numpy array into numpy array of dimension 2 \n",
    "    # rows are buckets, columns are intput and output values\n",
    "    nparray_input_all=np.array(list_nparray_input)\n",
    "    nparray_output_all=np.array(list_nparray_output)\n",
    "    nparray_eta_all=np.array(list_nparray_eta)\n",
    "    nparray_volumeID_all=np.array(list_nparray_volumeID)\n",
    "    nparray_layerID_all=np.array(list_nparray_layerID)\n",
    "    nparray_queryIndex_all=np.array(list_nparray_queryIndex)\n",
    "    nparray_nbPositiveHit_all=np.array(list_nparray_nbPositiveHit)\n",
    "    if debug:\n",
    "        p(\"nparray_input_all\",nparray_input_all)\n",
    "        p(\"nparray_output_all\",nparray_output_all)\n",
    "        p(\"nparray_eta_all\",nparray_eta_all)\n",
    "        p(\"nparray_volumeID_all\",nparray_volumeID_all)\n",
    "        p(\"nparray_layerID_all\",nparray_layerID_all)\n",
    "        p(\"nparray_queryIndex_all\",nparray_queryIndex_all)\n",
    "        p(\"nparray_nbPositiveHit_all\",nparray_nbPositiveHit_all)\n",
    "    \n",
    "    return nparray_input_all, nparray_output_all, nparray_eta_all, nparray_volumeID_all, nparray_layerID_all, nparray_queryIndex_all, nparray_nbPositiveHit_all\n",
    "# done function      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def do_run_one_event_at_a_time(list_eventNumber):\n",
    "    # for loop over eventNumber from list_eventNumber and write individual event to files\n",
    "    for i,eventNumber in enumerate(list_eventNumber):\n",
    "        #if i>0:\n",
    "        #    continue\n",
    "    \n",
    "        print()\n",
    "        print()\n",
    "        print(\"******************************\")\n",
    "        print(\"i\",i,\"eventNumber\",eventNumber)\n",
    "        print(\"******************************\")\n",
    "    \n",
    "        inputFileName_hits_recon=inputFolderName+\"/event\"+eventNumber+\"-hits.csv\"\n",
    "        inputFileName_hits_truth=inputFolderName+\"/event\"+eventNumber+\"-truth.csv\"\n",
    "    \n",
    "        if debug or verbose:\n",
    "            print(\"Read csv files ad df and merge them\")\n",
    "        df_hits_recon=pd.read_csv(inputFileName_hits_recon)\n",
    "        df_hits_truth=pd.read_csv(inputFileName_hits_truth)\n",
    "        # hit_id appears in both, erase from one so that when we concatenate below it appears only once\n",
    "        del df_hits_truth[\"hit_id\"]\n",
    "        df_hits=pd.concat([df_hits_recon,df_hits_truth],axis=1,sort=False)\n",
    "    \n",
    "        # build annoy index\n",
    "        if debug or verbose:\n",
    "            print(\"Start do Annoy Index\")\n",
    "        nparray_position=df_hits[[\"x\",\"y\",\"z\"]].values\n",
    "        if debug:\n",
    "            p(\"nparray_position\",nparray_position)\n",
    "\n",
    "        index=buildAnnoyIndex(nparray_position,metric=\"angular\",ntrees=10,debug=False)\n",
    "    \n",
    "        # for loop peste hituri peste hiturile preselectate volume_id=8, layer_id=2\n",
    "        if debug or verbose:\n",
    "            print(\"From df_hits produce nparray_input_all,nparray_output_all\")\n",
    "        nparray_input_all,nparray_output_all,nparray_eta_all,nparray_volumeID_all,nparray_layerID_all,nparray_queryIndex_all,nparray_nbPositiveHit_all=get_input_output_from_df_hits(df_hits,index)\n",
    "    \n",
    "        if debug:\n",
    "            p(\"nparray_input_all\",nparray_input_all)\n",
    "            p(\"nparray_output_all\",nparray_output_all)\n",
    "            p(\"nparray_eta_all\",nparray_eta_all)\n",
    "            p(\"nparray_volumeID_all\",nparray_volumeID_all)\n",
    "            p(\"nparray_layerID_all\",nparray_layerID_all)\n",
    "            p(\"nparray_queryIndex_all\",nparray_queryIndex_all)\n",
    "            p(\"nparray_nbPositiveHit_all\",nparray_nbPositiveHit_all)\n",
    "    \n",
    "        # reshape only input by adding one extra dimension needed by tensorflow, in practice it puts all into one extra bracket\n",
    "        nparray_input_all=nparray_input_all.reshape(nparray_input_all.shape[0],nparray_input_all.shape[1],1)\n",
    "        if debug or verbose:\n",
    "            p(\"nparray_input_all after reshape\",nparray_input_all)\n",
    "\n",
    "        fileNameNNInputAll=outputFolderName+\"/NN_2_data_Input_\"+eventNumber+\".npy\"\n",
    "        fileNameNNOutputAll=outputFolderName+\"/NN_2_data_Output_\"+eventNumber+\".npy\"\n",
    "        fileNameNNEtaAll=outputFolderName+\"/NN_2_data_Eta_\"+eventNumber+\".npy\"\n",
    "        fileNameNNVolumeIDAll=outputFolderName+\"/NN_2_data_VolumeID_\"+eventNumber+\".npy\"\n",
    "        fileNameNNLayerIDAll=outputFolderName+\"/NN_2_data_LayerID_\"+eventNumber+\".npy\"\n",
    "        fileNameNNQueryIndexAll=outputFolderName+\"/NN_2_data_QueryIndex_\"+eventNumber+\".npy\"\n",
    "        fileNameNNNbPositiveHitAll=outputFolderName+\"/NN_2_data_NbPositiveHit_\"+eventNumber+\".npy\"\n",
    "            \n",
    "        np.save(fileNameNNInputAll,nparray_input_all)\n",
    "        np.save(fileNameNNOutputAll,nparray_output_all)\n",
    "        np.save(fileNameNNEtaAll,nparray_eta_all)\n",
    "        np.save(fileNameNNVolumeIDAll,nparray_volumeID_all)\n",
    "        np.save(fileNameNNLayerIDAll,nparray_layerID_all)\n",
    "        np.save(fileNameNNQueryIndexAll,nparray_queryIndex_all)\n",
    "        np.save(fileNameNNNbPositiveHitAll,nparray_nbPositiveHit_all)\n",
    "\n",
    "    # done for loop over events\n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doIndividual:\n",
    "    do_run_one_event_at_a_time(list_eventNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_create_output_based_on_min_nb_of_positive_hits(list_eventNumber):\n",
    "    nparray_output_negative_one_bucket=np.array([-1 for i in range(bucketSize)]).astype(np.int8)\n",
    "    nparray_nbPositiveHit_negative_one_bucket=np.array([0])\n",
    "    # for loop over eventNumber from list_eventNumber\n",
    "    for i,eventNumber in enumerate(list_eventNumber):\n",
    "        if debug or verbose:\n",
    "            print(\"eventNumber\",eventNumber)\n",
    "        # open the files needed for this event: Output and NbPositiveHit\n",
    "        fileNameNNOutputAll=outputFolderName+\"/NN_2_data_Output_\"+eventNumber+\".npy\"\n",
    "        nparray_output_all=np.load(fileNameNNOutputAll)\n",
    "        fileNameNNNbPositiveHitAll=outputFolderName+\"/NN_2_data_NbPositiveHit_\"+eventNumber+\".npy\"\n",
    "        nparray_nbPositiveHit_all=np.load(fileNameNNNbPositiveHitAll)\n",
    "        # \n",
    "        dict_minNbPositiveHit_nparray_output_all={}\n",
    "        dict_minNbPositiveHit_nparray_nbPositiveHit_all={}\n",
    "        # for loop over each val of min number of positive fits\n",
    "        for minNbPositiveHit in list_minNbPositiveHit:\n",
    "            if debug:\n",
    "                print(\"i\",i,\"eventNumber\",eventNumber,\"minNbPositiveHit\",minNbPositiveHit)\n",
    "            # copy the current output in a completely new version with deep copy\n",
    "            # as we will want to replace only the values that need changed for less than needed positive hits\n",
    "            dict_minNbPositiveHit_nparray_output_all[minNbPositiveHit]=copy.deepcopy(nparray_output_all)\n",
    "            dict_minNbPositiveHit_nparray_nbPositiveHit_all[minNbPositiveHit]=copy.deepcopy(nparray_nbPositiveHit_all)\n",
    "        # done for loop over minNbPositiveHit\n",
    "        #\n",
    "        # loop over each row (bucket) in the output\n",
    "        for j in range(len(nparray_output_all)):\n",
    "            bucket=nparray_output_all[j]\n",
    "            if debug:\n",
    "                p(\"j=%.0f, bucket\"%j,bucket)\n",
    "            nbPositiveHit=nparray_nbPositiveHit_all[j][0] # as array with one element\n",
    "            if debug:\n",
    "                print(\"j=%.0f, nbPositiveHit=%.0f\"%(j,nbPositiveHit))\n",
    "            # to speed up, change only wheen needed\n",
    "            for minNbPositiveHit in list_minNbPositiveHit:\n",
    "                if nbPositiveHit<minNbPositiveHit:\n",
    "                    # replace this one line with the one with all negative\n",
    "                    dict_minNbPositiveHit_nparray_output_all[minNbPositiveHit][j]=nparray_output_negative_one_bucket\n",
    "                    dict_minNbPositiveHit_nparray_nbPositiveHit_all[minNbPositiveHit][j]=nparray_nbPositiveHit_negative_one_bucket\n",
    "                if debug:\n",
    "                    print(\"j=%.0f, minNbPositiveHit=%.0f, bucket\"%(j,minNbPositiveHit),dict_minNbPositiveHit_nparray_output_all[minNbPositiveHit][j])\n",
    "            # done for loop over minNbPositiveHit  \n",
    "        # done for loop over buckets\n",
    "        # for this event, write the new processed output to file\n",
    "        for minNbPositiveHit in list_minNbPositiveHit:\n",
    "            minString=\"Min\"\n",
    "            if minNbPositiveHit<9:\n",
    "                minString+=\"0\"\n",
    "            minString+=str(minNbPositiveHit)\n",
    "            fileNameNNOutputAll=outputFolderName+\"/NN_2_data_Output\"+minString+\"_\"+eventNumber+\".npy\"\n",
    "            np.save(fileNameNNOutputAll,dict_minNbPositiveHit_nparray_output_all[minNbPositiveHit])\n",
    "            fileNameNNNbPositiveHitAll=outputFolderName+\"/NN_2_data_NbPositiveHit\"+minString+\"_\"+eventNumber+\".npy\"\n",
    "            np.save(fileNameNNNbPositiveHitAll,dict_minNbPositiveHit_nparray_nbPositiveHit_all[minNbPositiveHit])\n",
    "        # done for loop over minNbPositiveHit  \n",
    "    # done for loop over events\n",
    "# done function\n",
    "if debug or verbose:\n",
    "    print(\"Done do_create_output_based_on_min_nb_of_positive_hits(list_eventNumber\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doProcessIndividual:\n",
    "    do_create_output_based_on_min_nb_of_positive_hits(list_eventNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_merge_events(mergeOption):\n",
    "    # create output folder name for the merge, for the current number of events to merge\n",
    "    eventNumberMin=mergeOption[0] # included\n",
    "    eventNumberMax=mergeOption[1] # excluded\n",
    "    #\n",
    "    outputFolderNameMerge=outputFolderName+\"_ev_\"\n",
    "    #\n",
    "    if eventNumberMin<10:\n",
    "        outputFolderNameMerge+=\"00\"\n",
    "    elif eventNumberMin<100:\n",
    "        outputFolderNameMerge+=\"0\"\n",
    "    outputFolderNameMerge+=str(eventNumberMin)\n",
    "    #\n",
    "    outputFolderNameMerge+=\"_\"\n",
    "    #\n",
    "    if eventNumberMax<10:\n",
    "        outputFolderNameMerge+=\"00\"\n",
    "    elif eventNumberMax<100:\n",
    "        outputFolderNameMerge+=\"0\"\n",
    "    outputFolderNameMerge+=str(eventNumberMax)\n",
    "    #\n",
    "    if not os.path.exists(outputFolderNameMerge):\n",
    "        os.makedirs(outputFolderNameMerge)\n",
    "    \n",
    "    # for loop over events and merge them in train or test and save in a different folder\n",
    "    counter_event_Train=0\n",
    "    counter_event_Test=0\n",
    "    if debug or verbose:\n",
    "        print(\"Start loop over all events and merge them in either train or test\")\n",
    "    \n",
    "    # list of variables to merge\n",
    "    list_var=[\n",
    "        \"Input\",\n",
    "        \"Output\",\n",
    "        \"VolumeID\",\n",
    "        \"LayerID\",\n",
    "        \"Eta\",\n",
    "        \"QueryIndex\",\n",
    "        \"NbPositiveHit\",\n",
    "    ]\n",
    "    # add to list_var also the OutputMin04, OutputMin07, etc\n",
    "    for minNbPositiveHit in list_minNbPositiveHit:\n",
    "        minString=\"Min\"\n",
    "        if minNbPositiveHit<9:\n",
    "            minString+=\"0\"\n",
    "        minString+=str(minNbPositiveHit)\n",
    "        list_var.append(\"Output\"+minString)\n",
    "        list_var.append(\"NbPositiveHit\"+minString)\n",
    "    # done for loop over minNbPositiveHit\n",
    "    if debug:\n",
    "        print(\"list_var\",list_var)\n",
    "    \n",
    "    #\n",
    "    dict_nameMerged_nparray={}\n",
    "    \n",
    "    # loop over the events, but careful, not all, but only from the first nrEventsToMerge\n",
    "    for i,eventNumber in enumerate(list_eventNumber[eventNumberMin:eventNumberMax]):\n",
    "        print(\"i\",i,\"eventNumber\",eventNumber)\n",
    "        \n",
    "        # for loop over all variables\n",
    "        dict_var_nparray={}\n",
    "        for var in list_var:\n",
    "            # read the stored file\n",
    "            fileName=outputFolderName+\"/NN_2_data_\"+var+\"_\"+eventNumber+\".npy\"\n",
    "            dict_var_nparray[var]=np.load(fileName)\n",
    "        # done for loop over var\n",
    "        \n",
    "        # calculate the rest of the event index to division by 100\n",
    "        # assuming we only merge multiples of ten, for each 10, we put first 7 in Train and last 3 in Test\n",
    "        rest=i%10\n",
    "\n",
    "        if rest<7: \n",
    "            # put this event into Train, with rest=i%10=0,1,2,3,4,5,6\n",
    "            counter_event_Train+=1\n",
    "            if counter_event_Train==1:\n",
    "                # it is the first event of type Train, so we simply deep copy it to the output\n",
    "                for var in list_var:\n",
    "                    dict_nameMerged_nparray[\"Train\"+var]=copy.deepcopy(dict_var_nparray[var])\n",
    "            else:\n",
    "                # it is not the first, so we concatentane to the new output\n",
    "                for var in list_var:\n",
    "                    dict_nameMerged_nparray[\"Train\"+var]=np.concatenate((dict_nameMerged_nparray[\"Train\"+var],dict_var_nparray[var]),axis=0,out=None)\n",
    "            # done if\n",
    "        else:\n",
    "            # put this event into Test, with rest=i%10=7,8,9\n",
    "            counter_event_Test+=1\n",
    "            if counter_event_Test==1:\n",
    "                # it is the first event of type Test, so we simply deep copy it to the output\n",
    "                for var in list_var:\n",
    "                    dict_nameMerged_nparray[\"Test\"+var]=copy.deepcopy(dict_var_nparray[var])\n",
    "            else:\n",
    "                # it is not the first, so we concatentane to the new output\n",
    "                for var in list_var:\n",
    "                    dict_nameMerged_nparray[\"Test\"+var]=np.concatenate((dict_nameMerged_nparray[\"Test\"+var],dict_var_nparray[var]),axis=0,out=None)\n",
    "            # done if\n",
    "        # done if  \n",
    "    # done for loop over eventNumber\n",
    "\n",
    "    # write file to the output\n",
    "    eventNumber=\"all\"\n",
    "    for TrainOrTest in \"Train,Test\".split(\",\"):\n",
    "        for var in list_var:\n",
    "            fileName=outputFolderNameMerge+\"/NN_2_data_\"+var+\"_\"+TrainOrTest+\"_\"+eventNumber+\".npy\"\n",
    "            np.save(fileName,dict_nameMerged_nparray[TrainOrTest+var])\n",
    "            if debug:\n",
    "                p(\"nparray_\"+TrainOrTest+\"_\"+var+\"_all_events\",dict_nameMerged_nparray[TrainOrTest+var])\n",
    "        # done for loop over var\n",
    "    # done for loop over TrainOrTest\n",
    "    \n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doMerge:\n",
    "    for mergeOption in list_mergeOption:\n",
    "        print()\n",
    "        print()\n",
    "        print(\"******************************\")\n",
    "        print(\"mergeOption\",mergeOption)\n",
    "        print(\"******************************\")\n",
    "        do_merge_events(mergeOption)\n",
    "        if debug:\n",
    "            print(\"Done mergeOption\",mergeOption)\n",
    "    # done for loop over mergeOption\n",
    "# done if"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_balanceOld(TrainOrTest,nparray_input,nparray_output,nparray_nbPositiveHit,nparray_eta,nparray_layerID,nparray_volumeID,nparray_queryIndex,outputFolderNameMerge,doVerifyForHit=False,debug=False,verbose=False):\n",
    "    if debug or verbose:\n",
    "        print(\"Start get_nparray_weight for TrainOrTest=\",TrainOrTest)\n",
    " \n",
    "    if verbose:\n",
    "        p(\"nparray_input\",nparray_input)\n",
    "        p(\"nparray_output\",nparray_output)\n",
    "        p(\"nparray_nbPositiveHit\",nparray_nbPositiveHit)\n",
    "        p(\"nparray_eta\",nparray_eta)\n",
    "        p(\"nparray_layerID\",nparray_layerID)\n",
    "        p(\"nparray_volumeID\",nparray_volumeID)\n",
    "        p(\"nparray_queryIndex\",nparray_queryIndex)\n",
    "\n",
    "    # total number of buckets\n",
    "    nbBucket=nparray_nbPositiveHit.shape[0]\n",
    "    nrHitTotal=nbBucket*bucketSize\n",
    "    if debug or verbose:\n",
    "        print()\n",
    "        print()\n",
    "        print(TrainOrTest+\":\")\n",
    "        print(\"nbBucket\",nbBucket,\"bucketSize\",bucketSize,\"nrHitTotal\",nrHitTotal)\n",
    "        \n",
    "        \n",
    "    # dictionary between number of positive hits in a bucket and the number of buckets with this number of positive hits\n",
    "    dict_nbPositiveHit_counterBucket_unbalanced={}\n",
    "    dict_nbPositiveHit_counterBucket_balancedDesired={}\n",
    "    dict_nbPositiveHit_counterBucket_balancedCounter={}\n",
    "    dict_nbPositiveHit_counterBucket_balancedObtained={}\n",
    "    # create explicit the list of posssible values for nbPositiveHit (0, 1, 2, ..., 19, 20)\n",
    "    list_nbPositiveHit=[i for i in range(0,21)]\n",
    "    for nbPositiveHit in list_nbPositiveHit:\n",
    "        dict_nbPositiveHit_counterBucket_unbalanced[nbPositiveHit]=0\n",
    "        dict_nbPositiveHit_counterBucket_balancedDesired[nbPositiveHit]=0\n",
    "        dict_nbPositiveHit_counterBucket_balancedCounter[nbPositiveHit]=0\n",
    "        dict_nbPositiveHit_counterBucket_balancedObtained[nbPositiveHit]=0\n",
    "    \n",
    "    \n",
    "    # now fill the dictionary of the counters depending on nbPositiveHit\n",
    "    # loop over buckets\n",
    "    for i in range(nbBucket):\n",
    "        nbPositiveHit=nparray_nbPositiveHit[i][0] # one column only, so take that element index [0]\n",
    "        if debug:\n",
    "            print(\"nbPositiveHit\",nbPositiveHit)\n",
    "        dict_nbPositiveHit_counterBucket_unbalanced[nbPositiveHit]+=1\n",
    "\n",
    "    # now dictionary is filled and we can print the values\n",
    "    if debug or verbose:\n",
    "        print()\n",
    "        print(\"Original unbalanced. \"+TrainOrTest+\":\")\n",
    "        for nbPositiveHit in list_nbPositiveHit:\n",
    "            counterBucket=dict_nbPositiveHit_counterBucket_unbalanced[nbPositiveHit]\n",
    "            percentBucket=100*counterBucket/nbBucket\n",
    "            print(\"nbPositiveHit=%.0f\"%nbPositiveHit,\"counterBucket=%.0f\"%counterBucket,\"percentBucket=%.1f\"%percentBucket)\n",
    "            \n",
    "    # plot legend position\n",
    "    legend_position=\"upper right\"\n",
    "            \n",
    "    # make plots\n",
    "    # we want bins from 0 to 21, as bin 20 is between 20 and 21\n",
    "    bins=[i-0.5 for i in range(0,22)]\n",
    "    print(\"bins\",bins)\n",
    "    # plot not normalized\n",
    "    plt.hist(nparray_nbPositiveHit,bins=bins,histtype=\"step\",color=\"red\",label=\"Unbalanced\",)\n",
    "    plt.title(\"Histogram not normalized for \"+TrainOrTest)\n",
    "    plt.xlabel(\"Number of positive hits per bucket\")\n",
    "    plt.ylabel(\"Number of buckets\")\n",
    "    plt.legend(loc=legend_position)\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.savefig(outputFolderNameMerge+\"/plot_histo_1_NbBucket_vs_NbPositiveHit_\"+TrainOrTest+\".png\")\n",
    "    plt.savefig(outputFolderNameMerge+\"/plot_histo_1_NbBucket_vs_NbPositiveHit_\"+TrainOrTest+\".pdf\")\n",
    "    plt.close()\n",
    "    # plot normalized\n",
    "    plt.hist(nparray_nbPositiveHit,bins=bins,density=True,histtype=\"step\",color=\"red\",label=\"Unbalanced\")\n",
    "    plt.title(\"Histogram normalized for \"+TrainOrTest)\n",
    "    plt.xlabel(\"Number of positive hits per bucket\")\n",
    "    plt.ylabel(\"Number of buckets\")\n",
    "    plt.legend(loc=legend_position)\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.savefig(outputFolderNameMerge+\"/plot_histo_1_NbBucket_vs_NbPositiveHit_normalized_\"+TrainOrTest+\".png\")\n",
    "    plt.savefig(outputFolderNameMerge+\"/plot_histo_1_NbBucket_vs_NbPositiveHit_normalized_\"+TrainOrTest+\".pdf\")\n",
    "    plt.close()\n",
    "    \n",
    "    # we want to balance out around 10 by removing not needed buckets, to balance 9 with 11, 8 with 12, etc\n",
    "    \n",
    "    # first calculate the desired number of buckets in each category, so that it is balanced around 10\n",
    "    # original version, trully symmetric, with a peak in the middle\n",
    "    #for i in range(bucketSize//2+1):\n",
    "    #    #print(\"i\",i)\n",
    "    #    val_left=dict_nbPositiveHit_counterBucket_unbalanced[i]\n",
    "    #    #print(\"val_left\",val_left)\n",
    "    #    val_right=dict_nbPositiveHit_counterBucket_unbalanced[bucketSize-i]\n",
    "    #    #print(\"val_right\",val_right)\n",
    "    #    if val_left<=val_right:\n",
    "    #        val_min=val_left\n",
    "    #    else:\n",
    "    #        val_min=val_right\n",
    "    #    dict_nbPositiveHit_counterBucket_balancedDesired[i]=val_min\n",
    "    #    dict_nbPositiveHit_counterBucket_balancedDesired[bucketSize-i]=val_min\n",
    "    # done for loop over i \n",
    "    \n",
    "    # first calculate the desired number of buckets in each category, so that it is balanced around 10\n",
    "    # the idea is we want to cut from the peak to be equal with the sides\n",
    "    # we can not cut all the way to 20, as there would be too little\n",
    "    # we find a compromise and use the same value from 6 to 14, and that value is the value of 14\n",
    "    # as there are more on the left than on the right\n",
    "    # also difference with respect to the first version\n",
    "    # 0 and 20, remain as they are, as 0 is not possible, it is unfair to put 20 to count of 0 just to be symmetric\n",
    "    # do the same for 1 and 19, as very few are at 1\n",
    "    # so for 0-20 and 1-19 to nothing\n",
    "    # then to as above for 2-18, 3-17, 4-16, 5-15\n",
    "    # and then for 6-14, 7-13, 8-12, 9-11 and 10 use the count from 14\n",
    "    for i in range(bucketSize//2+1):\n",
    "        #print(\"i\",i)\n",
    "        val_left=dict_nbPositiveHit_counterBucket_unbalanced[i]\n",
    "        #print(\"val_left\",val_left)\n",
    "        val_right=dict_nbPositiveHit_counterBucket_unbalanced[bucketSize-i]\n",
    "        #print(\"val_right\",val_right)\n",
    "        if i==0 or i==1:\n",
    "            # do not change the values, so set them as they are\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesired[i]=val_left\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesired[bucketSize-i]=val_right\n",
    "        elif i==2 or i==3 or i==4 or i==5:\n",
    "            # do as before\n",
    "            if val_left<=val_right:\n",
    "                val_min=val_left\n",
    "            else:\n",
    "                val_min=val_right\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesired[i]=val_min\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesired[bucketSize-i]=val_min\n",
    "        elif i==6 or i==7 or i==8 or i==9 or i==10:\n",
    "            # set the smallest value of them\n",
    "            # which we know from the shape of the distribution it is the value of i=20-6=14\n",
    "            val_common=dict_nbPositiveHit_counterBucket_unbalanced[14]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesired[i]=val_common\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesired[bucketSize-i]=val_common\n",
    "        else:\n",
    "            print(\"Warning! You should not get here!\")\n",
    "        # done if\n",
    "    # done for loop over i \n",
    "\n",
    "    # now dictionary is filled and we can print the values\n",
    "    if debug or verbose:\n",
    "        print()\n",
    "        print(\"BalancedDesired. \"+TrainOrTest+\":\")\n",
    "        for nbPositiveHit in list_nbPositiveHit:\n",
    "            counterBucket=dict_nbPositiveHit_counterBucket_balancedDesired[nbPositiveHit]\n",
    "            percentBucket=100*counterBucket/nbBucket\n",
    "            print(\"nbPositiveHit=%.0f\"%nbPositiveHit,\"counterBucket=%.0f\"%counterBucket,\"percentBucket=%.1f\"%percentBucket)\n",
    "    \n",
    "\n",
    "    # we want a nparray_inputC that will have a subset of nparray_input, same for nparray_output\n",
    "    list_inputBalanced=[]\n",
    "    list_outputBalanced=[]\n",
    "    list_nbPositiveHitBalanced=[]\n",
    "    list_etaBalanced=[]\n",
    "    list_layerIDBalanced=[]\n",
    "    list_volumeIDBalanced=[]\n",
    "    list_queryIndexBalanced=[]\n",
    "    # loop over buckets\n",
    "    \n",
    "    print()\n",
    "    for i in range(nbBucket):\n",
    "        # first decide if I need to keep this bucket or not\n",
    "        # first find the nbPositiveHit of this bucket\n",
    "        nbPositiveHit=nparray_nbPositiveHit[i][0]\n",
    "        # find how many buckets I am allowed to keep in this category\n",
    "        nrBucketDesired=dict_nbPositiveHit_counterBucket_balancedDesired[nbPositiveHit]\n",
    "        #print(\"j\",j,\"nbPositiveHit\",nbPositiveHit,\"nrBucketDesired\",nrBucketDesired)\n",
    "        # find if this bucket is allowed, meaning if the counter of this bucketg is smaller than allowed\n",
    "        # so I need to find the current counter\n",
    "        dict_nbPositiveHit_counterBucket_balancedCounter[nbPositiveHit]+=1\n",
    "        # now counter is increased and I can make the check, so first bucket has counter 1 (not start from 0)\n",
    "        # if current current counter smaller or equal with the allowed, you write the current bucket\n",
    "        # if not, you skip it\n",
    "        #print(\"nbPositiveHit\",nbPositiveHit,\"nrBucketDesired\",nrBucketDesired,\"counter\",dict_nbPositiveHit_counterBucket_balancedObtained[nbPositiveHit])\n",
    "        if dict_nbPositiveHit_counterBucket_balancedCounter[nbPositiveHit]<=nrBucketDesired:\n",
    "            dict_nbPositiveHit_counterBucket_balancedObtained[nbPositiveHit]+=1\n",
    "            list_inputBalanced.append(nparray_input[i])\n",
    "            list_outputBalanced.append(nparray_output[i])\n",
    "            list_nbPositiveHitBalanced.append(nparray_nbPositiveHit[i])\n",
    "            list_etaBalanced.append(nparray_eta[i])\n",
    "            list_layerIDBalanced.append(nparray_layerID[i])\n",
    "            list_volumeIDBalanced.append(nparray_volumeID[i])\n",
    "            list_queryIndexBalanced.append(nparray_queryIndex[i])           \n",
    "        else:\n",
    "            # do not add it, so that the datasets remained balanced\n",
    "            #print(\"Not add nbPositiveHit\",nbPositiveHit,\"nrBucketDesired\",nrBucketDesired,\"counter\",dict_nbPositiveHit_counterBucket_balancedObtained[nbPositiveHit])\n",
    "            pass\n",
    "        # done if\n",
    "    # done loop over buckets\n",
    "    \n",
    "    # now dictionary is filled and we can print the values\n",
    "    if debug or verbose:\n",
    "        print()\n",
    "        print(\"BalancedCounter after. \"+TrainOrTest+\":\")\n",
    "        for nbPositiveHit in list_nbPositiveHit:\n",
    "            counterBucket=dict_nbPositiveHit_counterBucket_balancedObtained[nbPositiveHit]\n",
    "            percentBucket=100*counterBucket/nbBucket\n",
    "            print(\"nbPositiveHit=%.0f\"%nbPositiveHit,\"counterBucket=%.0f\"%counterBucket,\"percentBucket=%.1f\"%percentBucket)\n",
    "\n",
    "    \n",
    "    # now dictionary is filled and we can print the values\n",
    "    if debug or verbose:\n",
    "        print()\n",
    "        print(\"BalancedObtained after. \"+TrainOrTest+\":\")\n",
    "        for nbPositiveHit in list_nbPositiveHit:\n",
    "            counterBucket=dict_nbPositiveHit_counterBucket_balancedObtained[nbPositiveHit]\n",
    "            percentBucket=100*counterBucket/nbBucket\n",
    "            print(\"nbPositiveHit=%.0f\"%nbPositiveHit,\"counterBucket=%.0f\"%counterBucket,\"percentBucket=%.1f\"%percentBucket)\n",
    "\n",
    "    # convert list to numpy arrays\n",
    "    nparray_inputBalanced=np.array(list_inputBalanced)\n",
    "    nparray_outputBalanced=np.array(list_outputBalanced)\n",
    "    nparray_nbPositiveHitBalanced=np.array(list_nbPositiveHitBalanced)\n",
    "    nparray_etaBalanced=np.array(list_etaBalanced)\n",
    "    nparray_layerIDBalanced=np.array(list_layerIDBalanced)\n",
    "    nparray_volumeIDBalanced=np.array(list_volumeIDBalanced)\n",
    "    nparray_queryIndexBalanced=np.array(list_queryIndexBalanced)\n",
    "    #\n",
    "    p(\"nparray_inputBalanced\",nparray_inputBalanced)\n",
    "    outputFileNameBalanced=outputFolderNameMerge+\"/NN_2_data_InputBalanced_\"+TrainOrTest+\"_all.npy\"\n",
    "    np.save(outputFileNameBalanced,nparray_inputBalanced)\n",
    "    #\n",
    "    p(\"nparray_outputBalanced\",nparray_outputBalanced)\n",
    "    outputFileNameBalanced=outputFolderNameMerge+\"/NN_2_data_OutputBalanced_\"+TrainOrTest+\"_all.npy\"\n",
    "    np.save(outputFileNameBalanced,nparray_outputBalanced)\n",
    "    #\n",
    "    p(\"nparray_nbPositiveHitBalanced\",nparray_nbPositiveHitBalanced)\n",
    "    outputFileNameBalanced=outputFolderNameMerge+\"/NN_2_data_NbPositiveHitBalanced_\"+TrainOrTest+\"_all.npy\"\n",
    "    np.save(outputFileNameBalanced,nparray_nbPositiveHitBalanced)\n",
    "    \n",
    "    p(\"nparray_etaBalanced\",nparray_etaBalanced)\n",
    "    outputFileNameBalanced=outputFolderNameMerge+\"/NN_2_data_EtaBalanced_\"+TrainOrTest+\"_all.npy\"\n",
    "    np.save(outputFileNameBalanced,nparray_etaBalanced)\n",
    "    \n",
    "    p(\"nparray_layerIDBalanced\",nparray_layerIDBalanced)\n",
    "    outputFileNameBalanced=outputFolderNameMerge+\"/NN_2_data_LayerIDBalanced_\"+TrainOrTest+\"_all.npy\"\n",
    "    np.save(outputFileNameBalanced,nparray_layerIDBalanced)\n",
    "    \n",
    "    p(\"nparray_volumeIDBalanced\",nparray_volumeIDBalanced)\n",
    "    outputFileNameBalanced=outputFolderNameMerge+\"/NN_2_data_VolumeIDBalanced_\"+TrainOrTest+\"_all.npy\"\n",
    "    np.save(outputFileNameBalanced,nparray_volumeIDBalanced)\n",
    "    \n",
    "    p(\"nparray_queryIndexBalanced\",nparray_queryIndexBalanced)\n",
    "    outputFileNameBalanced=outputFolderNameMerge+\"/NN_2_data_QueryIndexBalanced_\"+TrainOrTest+\"_all.npy\"\n",
    "    np.save(outputFileNameBalanced,nparray_queryIndexBalanced)\n",
    "    \n",
    "    # make plots\n",
    "    # we want bins from 0 to 21, as bin 20 is between 20 and 21\n",
    "    bins=[i-0.5 for i in range(0,22)]\n",
    "    print(\"bins\",bins)\n",
    "    # plot not normalized\n",
    "    plt.hist(nparray_nbPositiveHit,bins=bins,histtype=\"step\",color=\"red\",label=\"Unbalanced\")\n",
    "    plt.hist(nparray_nbPositiveHitBalanced,bins=bins,histtype=\"step\",color=\"blue\",label=\"Balanced\")\n",
    "    plt.title(\"Histogram not normalized for \"+TrainOrTest)\n",
    "    plt.xlabel(\"Number of positive hits per bucket\")\n",
    "    plt.ylabel(\"Number of buckets\")\n",
    "    plt.legend(loc=legend_position)\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.savefig(outputFolderNameMerge+\"/plot_histo_2_NbBucket_vs_NbPositiveHit_\"+TrainOrTest+\".png\")\n",
    "    plt.savefig(outputFolderNameMerge+\"/plot_histo_2_NbBucket_vs_NbPositiveHit_\"+TrainOrTest+\".pdf\")\n",
    "    plt.close()\n",
    "    # plot normalized\n",
    "    plt.hist(nparray_nbPositiveHit,bins=bins,density=True,histtype=\"step\",color=\"red\",label=\"Unbalanced\")\n",
    "    plt.hist(nparray_nbPositiveHitBalanced,bins=bins,density=True,histtype=\"step\",color=\"blue\",label=\"Balanced\")\n",
    "    plt.title(\"Histogram normalized for \"+TrainOrTest)\n",
    "    plt.xlabel(\"Number of positive hits per bucket\")\n",
    "    plt.ylabel(\"Number of buckets\")\n",
    "    plt.legend(loc=legend_position)\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.savefig(outputFolderNameMerge+\"/plot_histo_2_NbBucket_vs_NbPositiveHit_normalized_\"+TrainOrTest+\".png\")\n",
    "    plt.savefig(outputFolderNameMerge+\"/plot_histo_2_NbBucket_vs_NbPositiveHit_normalized_\"+TrainOrTest+\".pdf\")\n",
    "    plt.close()\n",
    "\n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_balance_mergeOptionOld(mergeOption):\n",
    "    if debug or verbose:\n",
    "        print(\"do_balance_mergeOption(), with mergeOption=\",mergeOption)\n",
    "        \n",
    "        \n",
    "    # create output folder name for the merge, for the current number of events to merge\n",
    "    eventNumberMin=mergeOption[0] # included\n",
    "    eventNumberMax=mergeOption[1] # excluded\n",
    "    #\n",
    "    outputFolderNameMerge=outputFolderName+\"_ev_\"\n",
    "    #\n",
    "    if eventNumberMin<10:\n",
    "        outputFolderNameMerge+=\"00\"\n",
    "    elif eventNumberMin<100:\n",
    "        outputFolderNameMerge+=\"0\"\n",
    "    outputFolderNameMerge+=str(eventNumberMin)\n",
    "    #\n",
    "    outputFolderNameMerge+=\"_\"\n",
    "    #\n",
    "    if eventNumberMax<10:\n",
    "        outputFolderNameMerge+=\"00\"\n",
    "    elif eventNumberMax<100:\n",
    "        outputFolderNameMerge+=\"0\"\n",
    "    outputFolderNameMerge+=str(eventNumberMax)\n",
    "    #\n",
    "    if not os.path.exists(outputFolderNameMerge):\n",
    "        os.makedirs(outputFolderNameMerge)\n",
    "        \n",
    "    eventNumber=\"all\"\n",
    "    \n",
    "    # Input\n",
    "    fileNameNNInputTrainAll=outputFolderNameMerge+\"/NN_2_data_Input_Train_\"+eventNumber+\".npy\"\n",
    "    fileNameNNInputTestAll=outputFolderNameMerge+\"/NN_2_data_Input_Test_\"+eventNumber+\".npy\"\n",
    "    nparray_Input_Train_all_Events=np.load(fileNameNNInputTrainAll)\n",
    "    nparray_Input_Test_all_Events=np.load(fileNameNNInputTestAll)\n",
    "    if debug:\n",
    "        print()\n",
    "        p(\"nparray_Input_Train_all_Events\",nparray_Input_Train_all_Events)\n",
    "        p(\"nparray_Input_Test_all_Events\",nparray_Input_Test_all_Events) \n",
    "    \n",
    "    # Output\n",
    "    fileNameNNOutputTrainAll=outputFolderNameMerge+\"/NN_2_data_Output_Train_\"+eventNumber+\".npy\"\n",
    "    fileNameNNOutputTestAll=outputFolderNameMerge+\"/NN_2_data_Output_Test_\"+eventNumber+\".npy\"\n",
    "    nparray_Output_Train_all_Events=np.load(fileNameNNOutputTrainAll)\n",
    "    nparray_Output_Test_all_Events=np.load(fileNameNNOutputTestAll)\n",
    "    if debug:\n",
    "        print()\n",
    "        p(\"nparray_Output_Train_all_Events\",nparray_Output_Train_all_Events)\n",
    "        p(\"nparray_Output_Test_all_Events\",nparray_Output_Test_all_Events)  \n",
    "    \n",
    "    # NbPositiveHit\n",
    "    fileNameNNNbPositiveHitTrainAll=outputFolderNameMerge+\"/NN_2_data_NbPositiveHit_Train_\"+eventNumber+\".npy\"\n",
    "    fileNameNNNbPositiveHitTestAll=outputFolderNameMerge+\"/NN_2_data_NbPositiveHit_Test_\"+eventNumber+\".npy\"\n",
    "    nparray_NbPositiveHit_Train_all_Events=np.load(fileNameNNNbPositiveHitTrainAll)\n",
    "    nparray_NbPositiveHit_Test_all_Events=np.load(fileNameNNNbPositiveHitTestAll)\n",
    "    if debug:\n",
    "        print()\n",
    "        p(\"nparray_NbPositiveHit_Train_all_Events\",nparray_NbPositiveHit_Train_all_Events)\n",
    "        p(\"nparray_NbPositiveHit_Test_all_Events\",nparray_NbPositiveHit_Test_all_Events)\n",
    "        \n",
    "    # Eta\n",
    "    fileNameNNEtaTrainAll=outputFolderNameMerge+\"/NN_2_data_Eta_Train_\"+eventNumber+\".npy\"\n",
    "    fileNameNNEtaTestAll=outputFolderNameMerge+\"/NN_2_data_Eta_Test_\"+eventNumber+\".npy\"\n",
    "    nparray_Eta_Train_all_Events=np.load(fileNameNNEtaTrainAll)\n",
    "    nparray_Eta_Test_all_Events=np.load(fileNameNNEtaTestAll)\n",
    "    if debug:\n",
    "        print()\n",
    "        p(\"nparray_Eta_Train_all_Events\",nparray_Eta_Train_all_Events)\n",
    "        p(\"nparray_Eta_Test_all_Events\",nparray_Eta_Test_all_Events)\n",
    "        \n",
    "    # LayerID\n",
    "    fileNameNNLayerIDTrainAll=outputFolderNameMerge+\"/NN_2_data_LayerID_Train_\"+eventNumber+\".npy\"\n",
    "    fileNameNNLayerIDTestAll=outputFolderNameMerge+\"/NN_2_data_LayerID_Test_\"+eventNumber+\".npy\"\n",
    "    nparray_LayerID_Train_all_Events=np.load(fileNameNNLayerIDTrainAll)\n",
    "    nparray_LayerID_Test_all_Events=np.load(fileNameNNLayerIDTestAll)\n",
    "    if debug:\n",
    "        print()\n",
    "        p(\"nparray_LayerID_Train_all_Events\",nparray_LayerID_Train_all_Events)\n",
    "        p(\"nparray_LayerID_Test_all_Events\",nparray_LayerID_Test_all_Events)\n",
    "        \n",
    "    # VolumeID\n",
    "    fileNameNNVolumeIDTrainAll=outputFolderNameMerge+\"/NN_2_data_VolumeID_Train_\"+eventNumber+\".npy\"\n",
    "    fileNameNNVolumeIDTestAll=outputFolderNameMerge+\"/NN_2_data_VolumeID_Test_\"+eventNumber+\".npy\"\n",
    "    nparray_VolumeID_Train_all_Events=np.load(fileNameNNVolumeIDTrainAll)\n",
    "    nparray_VolumeID_Test_all_Events=np.load(fileNameNNVolumeIDTestAll)\n",
    "    if debug:\n",
    "        print()\n",
    "        p(\"nparray_VolumeID_Train_all_Events\",nparray_VolumeID_Train_all_Events)\n",
    "        p(\"nparray_VolumeID_Test_all_Events\",nparray_VolumeID_Test_all_Events)\n",
    "        \n",
    "    # QueryIndex\n",
    "    fileNameNNQueryIndexTrainAll=outputFolderNameMerge+\"/NN_2_data_QueryIndex_Train_\"+eventNumber+\".npy\"\n",
    "    fileNameNNQueryIndexTestAll=outputFolderNameMerge+\"/NN_2_data_QueryIndex_Test_\"+eventNumber+\".npy\"\n",
    "    nparray_QueryIndex_Train_all_Events=np.load(fileNameNNQueryIndexTrainAll)\n",
    "    nparray_QueryIndex_Test_all_Events=np.load(fileNameNNQueryIndexTestAll)\n",
    "    if debug:\n",
    "        print()\n",
    "        p(\"nparray_QueryIndex_Train_all_Events\",nparray_QueryIndex_Train_all_Events)\n",
    "        p(\"nparray_QueryIndex_Test_all_Events\",nparray_QueryIndex_Test_all_Events)\n",
    "    \n",
    "    # for Train and Test\n",
    "    if True:\n",
    "        do_balanceOld(\"Train\",nparray_Input_Train_all_Events,nparray_Output_Train_all_Events,nparray_NbPositiveHit_Train_all_Events,nparray_Eta_Train_all_Events,nparray_LayerID_Train_all_Events,nparray_VolumeID_Train_all_Events,nparray_QueryIndex_Train_all_Events,outputFolderNameMerge,doVerifyForHit=True,debug=debug,verbose=True)\n",
    "        do_balanceOld(\"Test\",nparray_Input_Test_all_Events,nparray_Output_Test_all_Events,nparray_NbPositiveHit_Test_all_Events,nparray_Eta_Test_all_Events,nparray_LayerID_Test_all_Events,nparray_VolumeID_Test_all_Events,nparray_QueryIndex_Test_all_Events,outputFolderNameMerge,doVerifyForHit=True,debug=debug,verbose=True)\n",
    "    # weights ready\n",
    "    \n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doBalanceOld:\n",
    "    for mergeOption in list_mergeOption:\n",
    "        print()\n",
    "        print()\n",
    "        print(\"******************************\")\n",
    "        print(\"mergeOption\",mergeOption)\n",
    "        print(\"******************************\")\n",
    "        do_balance_mergeOptionOld(mergeOption)\n",
    "        if debug:\n",
    "            print(\"Done mergeOption\",mergeOption)\n",
    "    # done for loop over mergeOption\n",
    "# done if"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_process_balancedOld(mergeOption):\n",
    "    \n",
    "    # create output folder name for the merge, for the current number of events to merge\n",
    "    eventNumberMin=mergeOption[0] # included\n",
    "    eventNumberMax=mergeOption[1] # excluded\n",
    "    #\n",
    "    outputFolderNameMerge=outputFolderName+\"_ev_\"\n",
    "    #\n",
    "    if eventNumberMin<10:\n",
    "        outputFolderNameMerge+=\"00\"\n",
    "    elif eventNumberMin<100:\n",
    "        outputFolderNameMerge+=\"0\"\n",
    "    outputFolderNameMerge+=str(eventNumberMin)\n",
    "    #\n",
    "    outputFolderNameMerge+=\"_\"\n",
    "    #\n",
    "    if eventNumberMax<10:\n",
    "        outputFolderNameMerge+=\"00\"\n",
    "    elif eventNumberMax<100:\n",
    "        outputFolderNameMerge+=\"0\"\n",
    "    outputFolderNameMerge+=str(eventNumberMax)\n",
    "    #\n",
    "    if not os.path.exists(outputFolderNameMerge):\n",
    "        os.makedirs(outputFolderNameMerge)\n",
    "    \n",
    "    \n",
    "    # create the numpy array with all negative with which we replace those that need to\n",
    "    nparray_output_negative_one_bucket=np.array([-1 for i in range(bucketSize)]).astype(np.int8)\n",
    "    \n",
    "    # we change outputC for both Train and Test\n",
    "    list_TrainOrTest=[\"Train\",\"Test\"]\n",
    "    \n",
    "    eventNumber=\"all\"\n",
    "    \n",
    "    # for loop over TrainOrTest\n",
    "    for TrainOrTest in list_TrainOrTest:\n",
    "        if debug or verbose:\n",
    "            print(\"TrainOrTest\",TrainOrTest)\n",
    "        # open the files needed for this event: Output and NbPositiveHit\n",
    "        fileNameNNOutputAll=outputFolderNameMerge+\"/NN_2_data_OutputBalanced_\"+TrainOrTest+\"_\"+eventNumber+\".npy\"\n",
    "        nparray_output_all=np.load(fileNameNNOutputAll)\n",
    "        fileNameNNNbPositiveHitAll=outputFolderNameMerge+\"/NN_2_data_NbPositiveHitBalanced_\"+TrainOrTest+\"_\"+eventNumber+\".npy\"\n",
    "        nparray_nbPositiveHit_all=np.load(fileNameNNNbPositiveHitAll)\n",
    "        # \n",
    "        dict_minNbPositiveHit_nparray_output_all={}\n",
    "        # for loop over each val of min number of positive fits\n",
    "        for minNbPositiveHit in list_minNbPositiveHit:\n",
    "            if debug:\n",
    "                print(\"i\",i,\"eventNumber\",eventNumber,\"minNbPositiveHit\",minNbPositiveHit)\n",
    "            # copy the current output in a completely new version with deep copy\n",
    "            # as we will want to replace only the values that need changed for less than needed positive hits\n",
    "            dict_minNbPositiveHit_nparray_output_all[minNbPositiveHit]=copy.deepcopy(nparray_output_all)\n",
    "        # done for loop over minNbPositiveHit\n",
    "        #\n",
    "        # loop over each row (bucket) in the output\n",
    "        for i in range(len(nparray_output_all)):\n",
    "            nbPositiveHit=nparray_nbPositiveHit_all[i][0]\n",
    "            if debug:\n",
    "                print(\"i=%.0f, nbPositiveHit=%.0f\"%(i,nbPositiveHit))\n",
    "            # to speed up, change only wheen needed\n",
    "            for minNbPositiveHit in list_minNbPositiveHit:\n",
    "                if nbPositiveHit<minNbPositiveHit:\n",
    "                    # replace this one line with the one with all negative\n",
    "                    dict_minNbPositiveHit_nparray_output_all[minNbPositiveHit][i]=nparray_output_negative_one_bucket\n",
    "                if debug:\n",
    "                    print(\"i=%.0f, minNbPositiveHit=%.0f, bucket\"%(i,minNbPositiveHit),dict_minNbPositiveHit_nparray_output_all[minNbPositiveHit][j])\n",
    "            # done for loop over minNbPositiveHit  \n",
    "        # done for loop over buckets\n",
    "        # write the new processed output to file\n",
    "        for minNbPositiveHit in list_minNbPositiveHit:\n",
    "            minString=\"Min\"\n",
    "            if minNbPositiveHit<9:\n",
    "                minString+=\"0\"\n",
    "            minString+=str(minNbPositiveHit)\n",
    "            fileNameNNOutputAll=outputFolderNameMerge+\"/NN_2_data_OutputBalanced\"+minString+\"_\"+TrainOrTest+\"_\"+eventNumber+\".npy\"\n",
    "            np.save(fileNameNNOutputAll,dict_minNbPositiveHit_nparray_output_all[minNbPositiveHit])\n",
    "        # done for loop over minNbPositiveHit  \n",
    "    # done for loop over events\n",
    "# done function\n",
    "if debug or verbose:\n",
    "    print(\"Done do_create_output_based_on_min_nb_of_positive_hits(list_eventNumber\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if doProcessBalancedOld:\n",
    "    for mergeOption in list_mergeOption:\n",
    "        print()\n",
    "        print()\n",
    "        print(\"******************************\")\n",
    "        print(\"mergeOption\",mergeOption)\n",
    "        print(\"******************************\")\n",
    "        do_process_balancedOld(mergeOption)\n",
    "        if debug:\n",
    "            print(\"Done mergeOption\",mergeOption)\n",
    "    # done for loop over mergeOption\n",
    "# done if"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dict_nbPositiveHit_counterBucket(text, TrainOrTest, nbBucket, nbHitTotal, list_nbPositiveHit, dict_nbPositiveHit_counterBucket):\n",
    "    # now dictionary is filled and we can print the values\n",
    "    if debug or verbose:\n",
    "        print()\n",
    "        print(text+\" \"+TrainOrTest+\":\")\n",
    "        sum_weighted_nbPositiveHit=0.0\n",
    "        sum_weight=0.0\n",
    "        for nbPositiveHit in list_nbPositiveHit:\n",
    "            counterBucket=dict_nbPositiveHit_counterBucket[nbPositiveHit]\n",
    "            sum_weighted_nbPositiveHit+=counterBucket*nbPositiveHit # Sum_i (w_i * x_i)\n",
    "            sum_weight+=counterBucket # Sum_i (w_i)\n",
    "        # done for loop\n",
    "        # now that sum_weight (meaning nbBucket) is calculated for this particular case\n",
    "        # which can be after some buckets were cut away for rebalancing\n",
    "        # we can calculate and show the percentage\n",
    "        for nbPositiveHit in list_nbPositiveHit:\n",
    "            counterBucket=dict_nbPositiveHit_counterBucket[nbPositiveHit]\n",
    "            percentBucket=100*counterBucket/sum_weight\n",
    "            print(\"nbPositiveHit=%.0f\"%nbPositiveHit,\"counterBucket=%.0f\"%counterBucket,\"percentBucket=%.1f\"%percentBucket)\n",
    "        # done for loop\n",
    "        weighted_average=sum_weighted_nbPositiveHit/sum_weight\n",
    "        fraction_nbPositiveHit=weighted_average/bucketSize # (weighted_average/20)\n",
    "        percent_nbPositiveHit=fraction_nbPositiveHit*100.0\n",
    "        percent_nbNegativeHit=100.0-percent_nbPositiveHit\n",
    "        print()\n",
    "        print(\"nbBucket=%.0f, sum_weight=%.0f, ratio=%.2f\"%(nbBucket,sum_weight,sum_weight/nbBucket))\n",
    "        print(\"nbHitTotal=%.0f, sum_weighted_nbPositiveHit=%.0f, ratio=%.2f\"%(nbHitTotal,sum_weighted_nbPositiveHit,sum_weighted_nbPositiveHit/nbHitTotal))\n",
    "        print(\"weighted_average=%.2f, fraction_nbPositiveHit=%.2f\"%(weighted_average,fraction_nbPositiveHit))\n",
    "        print(\"percent_nbPositiveHit=%.1f, percent_nbNegativeHit=%.1f\"%(percent_nbPositiveHit,percent_nbNegativeHit))\n",
    "    # done if\n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_balance(TrainOrTest,nparray_input,nparray_output,nparray_nbPositiveHit,nparray_eta,nparray_layerID,nparray_volumeID,nparray_queryIndex,outputFolderNameMergeBalanced,xValue,doVerifyForHit=False,debug=False,verbose=False):\n",
    "    if debug or verbose:\n",
    "        print(\"Start do_balance() for TrainOrTest=\",TrainOrTest)\n",
    " \n",
    "    if debug:\n",
    "        p(\"nparray_input\",nparray_input)\n",
    "        p(\"nparray_output\",nparray_output)\n",
    "        p(\"nparray_nbPositiveHit\",nparray_nbPositiveHit)\n",
    "        p(\"nparray_eta\",nparray_eta)\n",
    "        p(\"nparray_layerID\",nparray_layerID)\n",
    "        p(\"nparray_volumeID\",nparray_volumeID)\n",
    "        p(\"nparray_queryIndex\",nparray_queryIndex)\n",
    "\n",
    "    # total number of buckets\n",
    "    nbBucket=nparray_nbPositiveHit.shape[0]\n",
    "    nbHitTotal=nbBucket*bucketSize\n",
    "    if debug or verbose:\n",
    "        print()\n",
    "        print()\n",
    "        print(TrainOrTest+\":\")\n",
    "        print(\"nbBucket\",nbBucket,\"bucketSize\",bucketSize,\"nbHitTotal\",nbHitTotal)\n",
    "        \n",
    "    # dictionary between number of positive hits in a bucket and the number of buckets with this number of positive hits\n",
    "    dict_nbPositiveHit_counterBucket_unbalanced={}\n",
    "    dict_nbPositiveHit_counterBucket_balancedDesired={}\n",
    "    dict_nbPositiveHit_counterBucket_balancedCounter={}\n",
    "    dict_nbPositiveHit_counterBucket_balancedObtained={}\n",
    "    # create explicit the list of posssible values for nbPositiveHit (0, 1, 2, ..., 19, 20)\n",
    "    list_nbPositiveHit=[i for i in range(0,21)]\n",
    "    for nbPositiveHit in list_nbPositiveHit:\n",
    "        dict_nbPositiveHit_counterBucket_unbalanced[nbPositiveHit]=0\n",
    "        dict_nbPositiveHit_counterBucket_balancedDesired[nbPositiveHit]=0\n",
    "        dict_nbPositiveHit_counterBucket_balancedCounter[nbPositiveHit]=0\n",
    "        dict_nbPositiveHit_counterBucket_balancedObtained[nbPositiveHit]=0\n",
    "    \n",
    "    # now fill the dictionary of the counters depending on nbPositiveHit\n",
    "    # loop over buckets\n",
    "    for i in range(nbBucket):\n",
    "        nbPositiveHit=nparray_nbPositiveHit[i][0] # one column only, so take that element index [0]\n",
    "        if debug:\n",
    "            print(\"nbPositiveHit\",nbPositiveHit)\n",
    "        dict_nbPositiveHit_counterBucket_unbalanced[nbPositiveHit]+=1\n",
    "\n",
    "    print_dict_nbPositiveHit_counterBucket(\"Original unbalanced.\",\n",
    "                                           TrainOrTest, nbBucket, nbHitTotal,\n",
    "                                           list_nbPositiveHit, \n",
    "                                           dict_nbPositiveHit_counterBucket_unbalanced)\n",
    "        \n",
    "    # plot legend position\n",
    "    legend_position=\"upper right\"\n",
    "            \n",
    "    # make plots\n",
    "    # we want bins from 0 to 21, as bin 20 is between 20 and 21\n",
    "    bins=[i-0.5 for i in range(0,22)]\n",
    "    print()\n",
    "    print(\"bins\",bins)\n",
    "    # plot not normalized\n",
    "    plt.hist(nparray_nbPositiveHit,bins=bins,histtype=\"step\",color=\"red\",label=\"Unbalanced\",)\n",
    "    plt.title(\"Histogram not normalized for \"+TrainOrTest)\n",
    "    plt.xlabel(\"Number of positive hits per bucket\")\n",
    "    plt.ylabel(\"Number of buckets\")\n",
    "    plt.legend(loc=legend_position)\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.savefig(outputFolderNameMergeBalanced+\"/plot_histo_1_NbBucket_vs_NbPositiveHit_\"+TrainOrTest+\".png\")\n",
    "    plt.savefig(outputFolderNameMergeBalanced+\"/plot_histo_1_NbBucket_vs_NbPositiveHit_\"+TrainOrTest+\".pdf\")\n",
    "    plt.close()\n",
    "    # plot normalized\n",
    "    plt.hist(nparray_nbPositiveHit,bins=bins,density=True,histtype=\"step\",color=\"red\",label=\"Unbalanced\")\n",
    "    plt.title(\"Histogram normalized for \"+TrainOrTest)\n",
    "    plt.xlabel(\"Number of positive hits per bucket\")\n",
    "    plt.ylabel(\"Number of buckets\")\n",
    "    plt.legend(loc=legend_position)\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.savefig(outputFolderNameMergeBalanced+\"/plot_histo_1_NbBucket_vs_NbPositiveHit_normalized_\"+TrainOrTest+\".png\")\n",
    "    plt.savefig(outputFolderNameMergeBalanced+\"/plot_histo_1_NbBucket_vs_NbPositiveHit_normalized_\"+TrainOrTest+\".pdf\")\n",
    "    plt.close()\n",
    "    \n",
    "    # we want to balance out around 10 by removing not needed buckets, to balance 9 with 11, 8 with 12, etc\n",
    "    \n",
    "    if False:\n",
    "        # first calculate the desired number of buckets in each category, so that it is balanced around 10\n",
    "        # original version, trully symmetric, with a peak in the middle\n",
    "        for i in range(bucketSize//2+1):\n",
    "            #print(\"i\",i)\n",
    "            val_left=dict_nbPositiveHit_counterBucket_unbalanced[i]\n",
    "            #print(\"val_left\",val_left)\n",
    "            val_right=dict_nbPositiveHit_counterBucket_unbalanced[bucketSize-i]\n",
    "            #print(\"val_right\",val_right)\n",
    "            if val_left<=val_right:\n",
    "                val_min=val_left\n",
    "            else:\n",
    "                val_min=val_right\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesired[i]=val_min\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesired[bucketSize-i]=val_min\n",
    "        # done for loop over i \n",
    "    # done if\n",
    "    \n",
    "    if False:\n",
    "        # first calculate the desired number of buckets in each category, so that it is balanced around 10\n",
    "        # the idea is we want to cut from the peak to be equal with the sides\n",
    "        # we can not cut all the way to 20, as there would be too little\n",
    "        # we find a compromise and use the same value from 6 to 14, and that value is the value of 14\n",
    "        # as there are more on the left than on the right\n",
    "        # also difference with respect to the first version\n",
    "        # 0 and 20, remain as they are, as 0 is not possible, it is unfair to put 20 to count of 0 just to be symmetric\n",
    "        # do the same for 1 and 19, as very few are at 1\n",
    "        # so for 0-20 and 1-19 to nothing\n",
    "        # then to as above for 2-18, 3-17, 4-16, 5-15\n",
    "        # and then for 6-14, 7-13, 8-12, 9-11 and 10 use the count from 14\n",
    "        for i in range(bucketSize//2+1):\n",
    "            #print(\"i\",i)\n",
    "            val_left=dict_nbPositiveHit_counterBucket_unbalanced[i]\n",
    "            #print(\"val_left\",val_left)\n",
    "            val_right=dict_nbPositiveHit_counterBucket_unbalanced[bucketSize-i]\n",
    "            #print(\"val_right\",val_right)\n",
    "            if i==0 or i==1:\n",
    "                # do not change the values, so set them as they are\n",
    "                dict_nbPositiveHit_counterBucket_balancedDesired[i]=val_left\n",
    "                dict_nbPositiveHit_counterBucket_balancedDesired[bucketSize-i]=val_right\n",
    "            elif i==2 or i==3 or i==4 or i==5:\n",
    "                # do as before\n",
    "                if val_left<=val_right:\n",
    "                    val_min=val_left\n",
    "                else:\n",
    "                    val_min=val_right\n",
    "                dict_nbPositiveHit_counterBucket_balancedDesired[i]=val_min\n",
    "                dict_nbPositiveHit_counterBucket_balancedDesired[bucketSize-i]=val_min\n",
    "            elif i==6 or i==7 or i==8 or i==9 or i==10:\n",
    "                # set the smallest value of them\n",
    "                # which we know from the shape of the distribution it is the value of i=20-6=14\n",
    "                val_common=dict_nbPositiveHit_counterBucket_unbalanced[14]\n",
    "                dict_nbPositiveHit_counterBucket_balancedDesired[i]=val_common\n",
    "                dict_nbPositiveHit_counterBucket_balancedDesired[bucketSize-i]=val_common\n",
    "            else:\n",
    "                print(\"Warning! You should not get here!\")\n",
    "            # done if\n",
    "        # done for loop over i\n",
    "    # done if\n",
    "    \n",
    "    if False:\n",
    "        # first calculate the desired number of buckets in each category, so that it is balanced around 10\n",
    "        # the idea is we want to cut from the peak to be equal with the sides\n",
    "        # we can not cut all the way to 20, as there would be too little\n",
    "        # we find a compromise and use the same value from 6 to 14, and that value is the value of 14\n",
    "        # as there are more on the left than on the right\n",
    "        # also difference with respect to the first version\n",
    "        # 0 and 20, remain as they are, as 0 is not possible, it is unfair to put 20 to count of 0 just to be symmetric\n",
    "        # do the same for 1 and 19, as very few are at 1\n",
    "        # so for 0-20 and 1-19 to nothing\n",
    "        # then to as above for 2-18, 3-17, 4-16, 5-15\n",
    "        # and then for 6-14, 7-13, 8-12, 9-11 and 10 use the count from 14\n",
    "        for i in range(bucketSize//2+1):\n",
    "            #print(\"i\",i)\n",
    "            val_left=dict_nbPositiveHit_counterBucket_unbalanced[i]\n",
    "            #print(\"val_left\",val_left)\n",
    "            val_right=dict_nbPositiveHit_counterBucket_unbalanced[bucketSize-i]\n",
    "            #print(\"val_right\",val_right)\n",
    "            if i==0 or i==1:\n",
    "                # do not change the values, so set them as they are\n",
    "                dict_nbPositiveHit_counterBucket_balancedDesired[i]=val_left\n",
    "                dict_nbPositiveHit_counterBucket_balancedDesired[bucketSize-i]=val_right\n",
    "            elif i==2 or i==3 or i==4 or i==5:\n",
    "                # do as before\n",
    "                if val_left<=val_right:\n",
    "                    val_min=val_left\n",
    "                else:\n",
    "                    val_min=val_right\n",
    "                dict_nbPositiveHit_counterBucket_balancedDesired[i]=val_min\n",
    "                dict_nbPositiveHit_counterBucket_balancedDesired[bucketSize-i]=val_min\n",
    "            elif i==6 or i==7 or i==8 or i==9 or i==10:\n",
    "                # set the smallest value of them\n",
    "                # which we know from the shape of the distribution it is the value of i=20-6=14\n",
    "                val_common=dict_nbPositiveHit_counterBucket_unbalanced[14]\n",
    "                dict_nbPositiveHit_counterBucket_balancedDesired[i]=val_common\n",
    "                dict_nbPositiveHit_counterBucket_balancedDesired[bucketSize-i]=val_common\n",
    "            else:\n",
    "                print(\"Warning! You should not get here!\")\n",
    "            # done if\n",
    "        # done for loop over i\n",
    "    # done if\n",
    "    \n",
    "    # unbalanced, same as the original\n",
    "    if False:\n",
    "        dict_nbPositiveHit_counterBucket_balancedDesired=dict_nbPositiveHit_counterBucket_unbalanced\n",
    "        \n",
    "    if False:\n",
    "        # calculate how many buckets to keep for nbPositiveHit=0 so that the average becomes 10\n",
    "        # w0 = (Sum_1_20(w_i*x_i)/10)-Sum_1_20(w_i)\n",
    "        # w_i = counterBucket for a given nbPositiveHit\n",
    "        # x_i = nbPositiveHit\n",
    "        \n",
    "        sum_weighted_nbPositiveHit=0.0\n",
    "        sum_weight=0.0\n",
    "        for nbPositiveHit in list_nbPositiveHit:\n",
    "            if nbPositiveHit==0:\n",
    "                continue\n",
    "            counterBucket=dict_nbPositiveHit_counterBucket_unbalanced[nbPositiveHit]\n",
    "            sum_weighted_nbPositiveHit+=counterBucket*nbPositiveHit # Sum_i (w_i * x_i)\n",
    "            sum_weight+=counterBucket # Sum_i (w_i)\n",
    "        # done for loop\n",
    "        counterBucket_for_nbPositiveHit_0_as_float=(sum_weighted_nbPositiveHit/10)-sum_weight\n",
    "        counterBucket_for_nbPositiveHit_0_as_int=int(counterBucket_for_nbPositiveHit_0_as_float)\n",
    "        print(\"counterBucket_for_nbPositiveHit_0_as_float=%.3f\"%counterBucket_for_nbPositiveHit_0_as_float)\n",
    "        print(\"counterBucket_for_nbPositiveHit_0_as_int=%.3f\"%counterBucket_for_nbPositiveHit_0_as_int)\n",
    "        \n",
    "        #\n",
    "        dict_nbPositiveHit_counterBucket_balancedDesired=dict_nbPositiveHit_counterBucket_unbalanced\n",
    "        dict_nbPositiveHit_counterBucket_balancedDesired[0]=counterBucket_for_nbPositiveHit_0_as_int\n",
    "        \n",
    "    if True:\n",
    "        # first flatten the peak by setting nbBucket the same from 10 to x, where x varies\n",
    "        # then do the same as above, by calculating an even lower nbBucket for nbPositiveHit=0\n",
    "        dict_nbPositiveHit_counterBucket_balancedDesiredTemp=dict_nbPositiveHit_counterBucket_unbalanced\n",
    "        if xValue==13:   \n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[10]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[11]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[12]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[13]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "        elif xValue==14:   \n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[10]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[11]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[12]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[13]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[14]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "        elif xValue==15:   \n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[10]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[11]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[12]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[13]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[14]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[15]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "        elif xValue==16:   \n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[10]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[11]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[12]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[13]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[14]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[15]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[16]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "        elif xValue==17:   \n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[10]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[11]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[12]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[13]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[14]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[15]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[16]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[17]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "        elif xValue==18:   \n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[10]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[11]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[12]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[13]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[14]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[15]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[16]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[17]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[18]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "        elif xValue==19:   \n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[10]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[11]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[12]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[13]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[14]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[15]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[16]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[17]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[18]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[19]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "        elif xValue==20:   \n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[10]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[11]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[12]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[13]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[14]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[15]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[16]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[17]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[18]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[19]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesiredTemp[20]=dict_nbPositiveHit_counterBucket_unbalanced[xValue]\n",
    "        else:\n",
    "            print(\"xValue\",xValue,\"not known. Will ABORT!!!\")\n",
    "            assert(False)\n",
    "        # done if\n",
    "        \n",
    "        print_dict_nbPositiveHit_counterBucket(\"BalancedDesiredTemp\",\n",
    "                                               TrainOrTest, nbBucket, nbHitTotal,\n",
    "                                               list_nbPositiveHit,\n",
    "                                               dict_nbPositiveHit_counterBucket_balancedDesiredTemp)\n",
    "        \n",
    "        # calculate how many buckets to keep for nbPositiveHit=0 so that the average becomes 10\n",
    "        # w0 = (Sum_1_20(w_i*x_i)/10)-Sum_1_20(w_i)\n",
    "        # w_i = counterBucket for a given nbPositiveHit\n",
    "        # x_i = nbPositiveHit\n",
    "        \n",
    "        sum_weighted_nbPositiveHit=0.0\n",
    "        sum_weight=0.0\n",
    "        for nbPositiveHit in list_nbPositiveHit:\n",
    "            if nbPositiveHit==0:\n",
    "                continue\n",
    "            counterBucket=dict_nbPositiveHit_counterBucket_balancedDesiredTemp[nbPositiveHit]\n",
    "            sum_weighted_nbPositiveHit+=counterBucket*nbPositiveHit # Sum_i (w_i * x_i)\n",
    "            sum_weight+=counterBucket # Sum_i (w_i)\n",
    "        # done for loop\n",
    "        counterBucket_for_nbPositiveHit_0_as_float=(sum_weighted_nbPositiveHit/10)-sum_weight\n",
    "        counterBucket_for_nbPositiveHit_0_as_int=int(counterBucket_for_nbPositiveHit_0_as_float)\n",
    "        print(\"counterBucket_for_nbPositiveHit_0_as_float=%.3f\"%counterBucket_for_nbPositiveHit_0_as_float)\n",
    "        print(\"counterBucket_for_nbPositiveHit_0_as_int=%.3f\"%counterBucket_for_nbPositiveHit_0_as_int)\n",
    "        \n",
    "        #\n",
    "        dict_nbPositiveHit_counterBucket_balancedDesired=dict_nbPositiveHit_counterBucket_balancedDesiredTemp\n",
    "        dict_nbPositiveHit_counterBucket_balancedDesired[0]=counterBucket_for_nbPositiveHit_0_as_int\n",
    "        \n",
    "    print_dict_nbPositiveHit_counterBucket(\"BalancedDesired\",\n",
    "                                           TrainOrTest, nbBucket, nbHitTotal,\n",
    "                                           list_nbPositiveHit,\n",
    "                                           dict_nbPositiveHit_counterBucket_balancedDesired)\n",
    "        \n",
    "    # we want a nparray_inputC that will have a subset of nparray_input, same for nparray_output\n",
    "    list_inputBalanced=[]\n",
    "    list_outputBalanced=[]\n",
    "    list_nbPositiveHitBalanced=[]\n",
    "    list_etaBalanced=[]\n",
    "    list_layerIDBalanced=[]\n",
    "    list_volumeIDBalanced=[]\n",
    "    list_queryIndexBalanced=[]\n",
    "    # loop over buckets\n",
    "    \n",
    "    print()\n",
    "    for i in range(nbBucket):\n",
    "        # first decide if I need to keep this bucket or not\n",
    "        # first find the nbPositiveHit of this bucket\n",
    "        nbPositiveHit=nparray_nbPositiveHit[i][0]\n",
    "        # find how many buckets I am allowed to keep in this category\n",
    "        nrBucketDesired=dict_nbPositiveHit_counterBucket_balancedDesired[nbPositiveHit]\n",
    "        #print(\"j\",j,\"nbPositiveHit\",nbPositiveHit,\"nrBucketDesired\",nrBucketDesired)\n",
    "        # find if this bucket is allowed, meaning if the counter of this bucketg is smaller than allowed\n",
    "        # so I need to find the current counter\n",
    "        dict_nbPositiveHit_counterBucket_balancedCounter[nbPositiveHit]+=1\n",
    "        # now counter is increased and I can make the check, so first bucket has counter 1 (not start from 0)\n",
    "        # if current current counter smaller or equal with the allowed, you write the current bucket\n",
    "        # if not, you skip it\n",
    "        #print(\"nbPositiveHit\",nbPositiveHit,\"nrBucketDesired\",nrBucketDesired,\"counter\",dict_nbPositiveHit_counterBucket_balancedObtained[nbPositiveHit])\n",
    "        if dict_nbPositiveHit_counterBucket_balancedCounter[nbPositiveHit]<=nrBucketDesired:\n",
    "            dict_nbPositiveHit_counterBucket_balancedObtained[nbPositiveHit]+=1\n",
    "            list_inputBalanced.append(nparray_input[i])\n",
    "            list_outputBalanced.append(nparray_output[i])\n",
    "            list_nbPositiveHitBalanced.append(nparray_nbPositiveHit[i])\n",
    "            list_etaBalanced.append(nparray_eta[i])\n",
    "            list_layerIDBalanced.append(nparray_layerID[i])\n",
    "            list_volumeIDBalanced.append(nparray_volumeID[i])\n",
    "            list_queryIndexBalanced.append(nparray_queryIndex[i])           \n",
    "        else:\n",
    "            # do not add it, so that the datasets remained balanced\n",
    "            #print(\"Not add nbPositiveHit\",nbPositiveHit,\"nrBucketDesired\",nrBucketDesired,\"counter\",dict_nbPositiveHit_counterBucket_balancedObtained[nbPositiveHit])\n",
    "            pass\n",
    "        # done if\n",
    "    # done loop over buckets\n",
    "    \n",
    "    print_dict_nbPositiveHit_counterBucket(\"BalancedObtained after.\",\n",
    "                                           TrainOrTest, nbBucket, nbHitTotal,\n",
    "                                           list_nbPositiveHit,\n",
    "                                           dict_nbPositiveHit_counterBucket_balancedObtained)\n",
    "            \n",
    "    # convert list to numpy arrays\n",
    "    print()\n",
    "    nparray_inputBalanced=np.array(list_inputBalanced)\n",
    "    nparray_outputBalanced=np.array(list_outputBalanced)\n",
    "    nparray_nbPositiveHitBalanced=np.array(list_nbPositiveHitBalanced)\n",
    "    nparray_etaBalanced=np.array(list_etaBalanced)\n",
    "    nparray_layerIDBalanced=np.array(list_layerIDBalanced)\n",
    "    nparray_volumeIDBalanced=np.array(list_volumeIDBalanced)\n",
    "    nparray_queryIndexBalanced=np.array(list_queryIndexBalanced)\n",
    "    #\n",
    "    if debug:\n",
    "        p(\"nparray_inputBalanced\",nparray_inputBalanced)\n",
    "    outputFileNameBalanced=outputFolderNameMergeBalanced+\"/NN_2_data_InputBalanced_\"+TrainOrTest+\"_all.npy\"\n",
    "    np.save(outputFileNameBalanced,nparray_inputBalanced)\n",
    "    #\n",
    "    if debug:\n",
    "        p(\"nparray_outputBalanced\",nparray_outputBalanced)\n",
    "    outputFileNameBalanced=outputFolderNameMergeBalanced+\"/NN_2_data_OutputBalanced_\"+TrainOrTest+\"_all.npy\"\n",
    "    np.save(outputFileNameBalanced,nparray_outputBalanced)\n",
    "    #\n",
    "    if debug:\n",
    "        p(\"nparray_nbPositiveHitBalanced\",nparray_nbPositiveHitBalanced)\n",
    "    outputFileNameBalanced=outputFolderNameMergeBalanced+\"/NN_2_data_NbPositiveHitBalanced_\"+TrainOrTest+\"_all.npy\"\n",
    "    np.save(outputFileNameBalanced,nparray_nbPositiveHitBalanced)\n",
    "    #\n",
    "    if debug:\n",
    "        p(\"nparray_etaBalanced\",nparray_etaBalanced)\n",
    "    outputFileNameBalanced=outputFolderNameMergeBalanced+\"/NN_2_data_EtaBalanced_\"+TrainOrTest+\"_all.npy\"\n",
    "    np.save(outputFileNameBalanced,nparray_etaBalanced)\n",
    "    #\n",
    "    if debug:\n",
    "        p(\"nparray_layerIDBalanced\",nparray_layerIDBalanced)\n",
    "    outputFileNameBalanced=outputFolderNameMergeBalanced+\"/NN_2_data_LayerIDBalanced_\"+TrainOrTest+\"_all.npy\"\n",
    "    np.save(outputFileNameBalanced,nparray_layerIDBalanced)\n",
    "    #\n",
    "    if debug:\n",
    "        p(\"nparray_volumeIDBalanced\",nparray_volumeIDBalanced)\n",
    "    outputFileNameBalanced=outputFolderNameMergeBalanced+\"/NN_2_data_VolumeIDBalanced_\"+TrainOrTest+\"_all.npy\"\n",
    "    np.save(outputFileNameBalanced,nparray_volumeIDBalanced)\n",
    "    #\n",
    "    if debug:\n",
    "        p(\"nparray_queryIndexBalanced\",nparray_queryIndexBalanced)\n",
    "    outputFileNameBalanced=outputFolderNameMergeBalanced+\"/NN_2_data_QueryIndexBalanced_\"+TrainOrTest+\"_all.npy\"\n",
    "    np.save(outputFileNameBalanced,nparray_queryIndexBalanced)\n",
    "    \n",
    "    # make plots\n",
    "    # we want bins from 0 to 21, as bin 20 is between 20 and 21\n",
    "    bins=[i-0.5 for i in range(0,22)]\n",
    "    print()\n",
    "    print(\"bins\",bins)\n",
    "    # plot not normalized\n",
    "    plt.hist(nparray_nbPositiveHit,bins=bins,histtype=\"step\",color=\"red\",label=\"Unbalanced\")\n",
    "    plt.hist(nparray_nbPositiveHitBalanced,bins=bins,histtype=\"step\",color=\"blue\",label=\"Balanced\")\n",
    "    plt.title(\"Histogram not normalized for \"+TrainOrTest)\n",
    "    plt.xlabel(\"Number of positive hits per bucket\")\n",
    "    plt.ylabel(\"Number of buckets\")\n",
    "    plt.legend(loc=legend_position)\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.savefig(outputFolderNameMergeBalanced+\"/plot_histo_2_NbBucket_vs_NbPositiveHit_\"+TrainOrTest+\".png\")\n",
    "    plt.savefig(outputFolderNameMergeBalanced+\"/plot_histo_2_NbBucket_vs_NbPositiveHit_\"+TrainOrTest+\".pdf\")\n",
    "    plt.close()\n",
    "    # plot normalized\n",
    "    plt.hist(nparray_nbPositiveHit,bins=bins,density=True,histtype=\"step\",color=\"red\",label=\"Unbalanced\")\n",
    "    plt.hist(nparray_nbPositiveHitBalanced,bins=bins,density=True,histtype=\"step\",color=\"blue\",label=\"Balanced\")\n",
    "    plt.title(\"Histogram normalized for \"+TrainOrTest)\n",
    "    plt.xlabel(\"Number of positive hits per bucket\")\n",
    "    plt.ylabel(\"Number of buckets\")\n",
    "    plt.legend(loc=legend_position)\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.savefig(outputFolderNameMergeBalanced+\"/plot_histo_2_NbBucket_vs_NbPositiveHit_normalized_\"+TrainOrTest+\".png\")\n",
    "    plt.savefig(outputFolderNameMergeBalanced+\"/plot_histo_2_NbBucket_vs_NbPositiveHit_normalized_\"+TrainOrTest+\".pdf\")\n",
    "    plt.close()\n",
    "\n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_balance_mergeOption(mergeOption, list_minNbPositiveHit):\n",
    "    if debug or verbose:\n",
    "        print(\"do_balance_mergeOption(), with mergeOption=\",mergeOption)\n",
    "        \n",
    "    # create output folder name for the merge, for the current number of events to merge\n",
    "    eventNumberMin=mergeOption[0] # included\n",
    "    eventNumberMax=mergeOption[1] # excluded\n",
    "    #\n",
    "    outputFolderNameMerge=outputFolderName+\"_ev_\"\n",
    "    #\n",
    "    if eventNumberMin<10:\n",
    "        outputFolderNameMerge+=\"00\"\n",
    "    elif eventNumberMin<100:\n",
    "        outputFolderNameMerge+=\"0\"\n",
    "    outputFolderNameMerge+=str(eventNumberMin)\n",
    "    #\n",
    "    outputFolderNameMerge+=\"_\"\n",
    "    #\n",
    "    if eventNumberMax<10:\n",
    "        outputFolderNameMerge+=\"00\"\n",
    "    elif eventNumberMax<100:\n",
    "        outputFolderNameMerge+=\"0\"\n",
    "    outputFolderNameMerge+=str(eventNumberMax)\n",
    "    #\n",
    "    if not os.path.exists(outputFolderNameMerge):\n",
    "        os.makedirs(outputFolderNameMerge)\n",
    "        \n",
    "    eventNumber=\"all\"\n",
    "    \n",
    "    # Input\n",
    "    fileNameNNInputTrainAll=outputFolderNameMerge+\"/NN_2_data_Input_Train_\"+eventNumber+\".npy\"\n",
    "    fileNameNNInputTestAll=outputFolderNameMerge+\"/NN_2_data_Input_Test_\"+eventNumber+\".npy\"\n",
    "    nparray_Input_Train_all_Events=np.load(fileNameNNInputTrainAll)\n",
    "    nparray_Input_Test_all_Events=np.load(fileNameNNInputTestAll)\n",
    "    if debug:\n",
    "        print()\n",
    "        p(\"nparray_Input_Train_all_Events\",nparray_Input_Train_all_Events)\n",
    "        p(\"nparray_Input_Test_all_Events\",nparray_Input_Test_all_Events)  \n",
    "        \n",
    "    # Eta\n",
    "    fileNameNNEtaTrainAll=outputFolderNameMerge+\"/NN_2_data_Eta_Train_\"+eventNumber+\".npy\"\n",
    "    fileNameNNEtaTestAll=outputFolderNameMerge+\"/NN_2_data_Eta_Test_\"+eventNumber+\".npy\"\n",
    "    nparray_Eta_Train_all_Events=np.load(fileNameNNEtaTrainAll)\n",
    "    nparray_Eta_Test_all_Events=np.load(fileNameNNEtaTestAll)\n",
    "    if debug:\n",
    "        print()\n",
    "        p(\"nparray_Eta_Train_all_Events\",nparray_Eta_Train_all_Events)\n",
    "        p(\"nparray_Eta_Test_all_Events\",nparray_Eta_Test_all_Events)\n",
    "        \n",
    "    # LayerID\n",
    "    fileNameNNLayerIDTrainAll=outputFolderNameMerge+\"/NN_2_data_LayerID_Train_\"+eventNumber+\".npy\"\n",
    "    fileNameNNLayerIDTestAll=outputFolderNameMerge+\"/NN_2_data_LayerID_Test_\"+eventNumber+\".npy\"\n",
    "    nparray_LayerID_Train_all_Events=np.load(fileNameNNLayerIDTrainAll)\n",
    "    nparray_LayerID_Test_all_Events=np.load(fileNameNNLayerIDTestAll)\n",
    "    if debug:\n",
    "        print()\n",
    "        p(\"nparray_LayerID_Train_all_Events\",nparray_LayerID_Train_all_Events)\n",
    "        p(\"nparray_LayerID_Test_all_Events\",nparray_LayerID_Test_all_Events)\n",
    "        \n",
    "    # VolumeID\n",
    "    fileNameNNVolumeIDTrainAll=outputFolderNameMerge+\"/NN_2_data_VolumeID_Train_\"+eventNumber+\".npy\"\n",
    "    fileNameNNVolumeIDTestAll=outputFolderNameMerge+\"/NN_2_data_VolumeID_Test_\"+eventNumber+\".npy\"\n",
    "    nparray_VolumeID_Train_all_Events=np.load(fileNameNNVolumeIDTrainAll)\n",
    "    nparray_VolumeID_Test_all_Events=np.load(fileNameNNVolumeIDTestAll)\n",
    "    if debug:\n",
    "        print()\n",
    "        p(\"nparray_VolumeID_Train_all_Events\",nparray_VolumeID_Train_all_Events)\n",
    "        p(\"nparray_VolumeID_Test_all_Events\",nparray_VolumeID_Test_all_Events)\n",
    "        \n",
    "    # QueryIndex\n",
    "    fileNameNNQueryIndexTrainAll=outputFolderNameMerge+\"/NN_2_data_QueryIndex_Train_\"+eventNumber+\".npy\"\n",
    "    fileNameNNQueryIndexTestAll=outputFolderNameMerge+\"/NN_2_data_QueryIndex_Test_\"+eventNumber+\".npy\"\n",
    "    nparray_QueryIndex_Train_all_Events=np.load(fileNameNNQueryIndexTrainAll)\n",
    "    nparray_QueryIndex_Test_all_Events=np.load(fileNameNNQueryIndexTestAll)\n",
    "    if debug:\n",
    "        print()\n",
    "        p(\"nparray_QueryIndex_Train_all_Events\",nparray_QueryIndex_Train_all_Events)\n",
    "        p(\"nparray_QueryIndex_Test_all_Events\",nparray_QueryIndex_Test_all_Events)\n",
    "        \n",
    "    # for loop over minNbPositiveHit\n",
    "    for minNbPositiveHit in list_minNbPositiveHit:\n",
    "        print()\n",
    "        print()\n",
    "        print(\"****************************************************************\")\n",
    "        print(\"mergeOption\",mergeOption,\"minNbPositiveHit\",minNbPositiveHit)\n",
    "        print(\"*****************************************************************\")\n",
    "        \n",
    "        minString=\"Min\"\n",
    "        if minNbPositiveHit<9:\n",
    "            minString+=\"0\"\n",
    "        minString+=str(minNbPositiveHit)\n",
    "        \n",
    "        minString2=\"min_\"\n",
    "        if minNbPositiveHit<9:\n",
    "            minString2+=\"0\"\n",
    "        minString2+=str(minNbPositiveHit)\n",
    "        \n",
    "        # output folder and how much of the peak to flatten\n",
    "        xValue=20\n",
    "        outputFolderNameMergeBalanced=outputFolderNameMerge+\"_\"+minString2+\"_balanced\"+str(xValue)\n",
    "        if debug or verbose:\n",
    "            print(\"outputFolderNameMergeBalanced\",outputFolderNameMergeBalanced)\n",
    "        #\n",
    "        if not os.path.exists(outputFolderNameMergeBalanced):\n",
    "            os.makedirs(outputFolderNameMergeBalanced)\n",
    "    \n",
    "        # Output\n",
    "        fileNameNNOutputTrainAll=outputFolderNameMerge+\"/NN_2_data_Output\"+minString+\"_Train_\"+eventNumber+\".npy\"\n",
    "        fileNameNNOutputTestAll=outputFolderNameMerge+\"/NN_2_data_Output\"+minString+\"_Test_\"+eventNumber+\".npy\"\n",
    "        nparray_Output_Train_all_Events=np.load(fileNameNNOutputTrainAll)\n",
    "        nparray_Output_Test_all_Events=np.load(fileNameNNOutputTestAll)\n",
    "        if debug:\n",
    "            print()\n",
    "            p(\"nparray_Output_Train_all_Events\",nparray_Output_Train_all_Events)\n",
    "            p(\"nparray_Output_Test_all_Events\",nparray_Output_Test_all_Events)\n",
    "            \n",
    "        # NbPositiveHit\n",
    "        fileNameNNNbPositiveHitTrainAll=outputFolderNameMerge+\"/NN_2_data_NbPositiveHit\"+minString+\"_Train_\"+eventNumber+\".npy\"\n",
    "        fileNameNNNbPositiveHitTestAll=outputFolderNameMerge+\"/NN_2_data_NbPositiveHit\"+minString+\"_Test_\"+eventNumber+\".npy\"\n",
    "        nparray_NbPositiveHit_Train_all_Events=np.load(fileNameNNNbPositiveHitTrainAll)\n",
    "        nparray_NbPositiveHit_Test_all_Events=np.load(fileNameNNNbPositiveHitTestAll)\n",
    "        if debug:\n",
    "            print()\n",
    "            p(\"nparray_NbPositiveHit_Train_all_Events\",nparray_NbPositiveHit_Train_all_Events)\n",
    "            p(\"nparray_NbPositiveHit_Test_all_Events\",nparray_NbPositiveHit_Test_all_Events)\n",
    "    \n",
    "        # for Train and Test\n",
    "        if True:\n",
    "            do_balance(\"Train\",nparray_Input_Train_all_Events,nparray_Output_Train_all_Events,nparray_NbPositiveHit_Train_all_Events,nparray_Eta_Train_all_Events,nparray_LayerID_Train_all_Events,nparray_VolumeID_Train_all_Events,nparray_QueryIndex_Train_all_Events,outputFolderNameMergeBalanced,xValue,doVerifyForHit=True,debug=debug,verbose=True)\n",
    "            do_balance(\"Test\",nparray_Input_Test_all_Events,nparray_Output_Test_all_Events,nparray_NbPositiveHit_Test_all_Events,nparray_Eta_Test_all_Events,nparray_LayerID_Test_all_Events,nparray_VolumeID_Test_all_Events,nparray_QueryIndex_Test_all_Events,outputFolderNameMergeBalanced,xValue,doVerifyForHit=True,debug=debug,verbose=True)\n",
    "        # weights ready\n",
    "    \n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doBalance:\n",
    "    for mergeOption in list_mergeOption:\n",
    "        print()\n",
    "        print()\n",
    "        print(\"******************************\")\n",
    "        print(\"mergeOption\",mergeOption)\n",
    "        print(\"******************************\")\n",
    "        do_balance_mergeOption(mergeOption, list_minNbPositiveHit)\n",
    "        if debug:\n",
    "            print(\"Done mergeOption\",mergeOption) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done all!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
