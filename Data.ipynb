{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug=False\n",
    "verbose=True\n",
    "doBucketPlots=False\n",
    "doIndividual=False\n",
    "doProcessIndividual=False\n",
    "doMerge=False\n",
    "doBalance=True\n",
    "doProcessMerged=True # for OutputBalanced do Min04, Min07, etc\n",
    "#\n",
    "list_minNbPositiveHit=[\n",
    "    0,\n",
    "    4,\n",
    "    7,\n",
    "    10,\n",
    "]\n",
    "list_mergeOption=[\n",
    "    #[0,10],\n",
    "    #[10,20],\n",
    "    #[20,30],\n",
    "    #[30,40],\n",
    "    #[40,50],\n",
    "    #[50,60],\n",
    "    #[60,70],\n",
    "    #[70,80],\n",
    "    #[80,90],\n",
    "    #[90,100],\n",
    "    #[0,20],\n",
    "    [0,30],\n",
    "    #[0,40],\n",
    "    #[0,50],\n",
    "    #[0,60],\n",
    "    #[0,70],\n",
    "    #[0,80],\n",
    "    #[0,90],\n",
    "    #[0,100],\n",
    "]\n",
    "bucketSize=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderStem=\"/Volumes/Luiza_SSD\"\n",
    "\n",
    "#\n",
    "inputFolderName=folderStem+\"/ATLAS/TrackML/data/ttbar_mu200-generic\"\n",
    "#outputFolderName=folderStem+\"/ATLAS/TrackML/output_general\"\n",
    "#outputFolderName=folderStem+\"/ATLAS/TrackML/output_again\"\n",
    "outputFolderName=folderStem+\"/ATLAS/TrackML/output_new\"\n",
    "# if output folder does not exist, create it\n",
    "if not os.path.exists(outputFolderName):\n",
    "    os.makedirs(outputFolderName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eventNumber_from_fileName(fileName):\n",
    "    eventNumber=fileName.replace(\"event\",\"\").replace(\"-hits.csv\",\"\")\n",
    "    return eventNumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All events available in my folder. list_eventNumber ['000000000', '000000001', '000000002', '000000003', '000000004', '000000005', '000000006', '000000007', '000000008', '000000009', '000000010', '000000011', '000000012', '000000013', '000000014', '000000015', '000000016', '000000017', '000000018', '000000019', '000000020', '000000021', '000000022', '000000023', '000000024', '000000025', '000000026', '000000027', '000000028', '000000029', '000000030', '000000031', '000000032', '000000033', '000000034', '000000035', '000000036', '000000037', '000000038', '000000039', '000000040', '000000041', '000000042', '000000043', '000000044', '000000045', '000000046', '000000047', '000000048', '000000049', '000000050', '000000051', '000000052', '000000053', '000000054', '000000055', '000000056', '000000057', '000000058', '000000059', '000000060', '000000061', '000000062', '000000063', '000000064', '000000065', '000000066', '000000067', '000000068', '000000069', '000000070', '000000071', '000000072', '000000073', '000000074', '000000075', '000000076', '000000077', '000000078', '000000079', '000000080', '000000081', '000000082', '000000083', '000000084', '000000085', '000000086', '000000087', '000000088', '000000089', '000000090', '000000091', '000000092', '000000093', '000000094', '000000095', '000000096', '000000097', '000000098', '000000099']\n"
     ]
    }
   ],
   "source": [
    "# calculate event numbers from my folder\n",
    "# lista goala in care sa pun numerele evenimentelor\n",
    "list_eventNumber=[]\n",
    "# The sorted() function returns a sorted list of the specified iterable object \n",
    "# Strings are sorted alphabetically, and numbers are sorted numerically\n",
    "# os operating system \n",
    "# hits ca sa nu am de doura ori evenimentul in lista\n",
    "for fileName in sorted(os.listdir(inputFolderName)):\n",
    "    if fileName.endswith(\"-hits.csv\"):\n",
    "        #print(fileName)\n",
    "        eventNumber=get_eventNumber_from_fileName(fileName)\n",
    "        #print(eventNumber)\n",
    "        list_eventNumber.append(eventNumber)\n",
    "# done for loop\n",
    "#list_eventNumber=list_eventNumber[0:20] # keep only first 10 events\n",
    "#list_eventNumber=[\"000000007\"] # keep only one event for test\n",
    "print(\"All events available in my folder. list_eventNumber\", list_eventNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p(name,nparray):\n",
    "    print(\"Start\",name)\n",
    "    print(nparray)\n",
    "    print(\"End\",name,\"shape\",nparray.shape,\"dtype\",nparray.dtype,\"type\",type(nparray))\n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.59663469141339"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_eta(px,py,pz,m):\n",
    "    p=np.sqrt(px*px+py*py+pz*pz)\n",
    "    theta=np.arccos(pz/p)\n",
    "    eta=-np.log(np.tan(theta/2))\n",
    "    return eta\n",
    "# done function\n",
    "\n",
    "# example\n",
    "eta=calculate_eta(3,4,100000,0)\n",
    "eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this runs only once per event\n",
    "def buildAnnoyIndex(nparray_position,metric=\"angular\",ntrees=10,debug=False): \n",
    "    \n",
    "    numberDimension=nparray_position.shape[1] # 3 (x,y,z)\n",
    "    if debug:\n",
    "        print(\"numberDimension\",numberDimension,\"metric\",metric)\n",
    "    index=AnnoyIndex(numberDimension,metric)\n",
    "    if debug:\n",
    "        print(\"type(index)\",type(index))\n",
    "        print(\"enumerate data\")\n",
    "    # add each hit to the index\n",
    "    for i,position in enumerate(nparray_position):\n",
    "        if debug:\n",
    "            print(\"i\",i,\"position\",position)\n",
    "        index.add_item(i,position)\n",
    "    # done for loop over hits\n",
    "    # build the index with 10 trees\n",
    "    index.build(ntrees) # 10 trees\n",
    "    return index\n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_output_from_df_hits(df_hits,index):\n",
    "    list_nparray_input=[]\n",
    "    list_nparray_output=[]\n",
    "    list_nparray_eta=[]\n",
    "    list_nparray_volumeID=[]\n",
    "    list_nparray_layerID=[]\n",
    "    list_nparray_queryIndex=[]\n",
    "    list_nparray_nbPositiveHit=[]\n",
    "    \n",
    "    nparray_volume_id=df_hits[\"volume_id\"].values\n",
    "    nparray_layer_id=df_hits[\"layer_id\"].values \n",
    "\n",
    "    counterBucket=0\n",
    "    \n",
    "    #my_title=\"FirstLayer\"\n",
    "    my_title=\"All\"\n",
    "    for i in range(len(df_hits)):\n",
    "        if (\n",
    "            True # for each hit create a bucket\n",
    "            #(nparray_volume_id[i]==8 and nparray_layer_id[i]==2) \n",
    "            #or (nparray_volume_id[i]==7 and nparray_layer_id[i]==2)\n",
    "            #or (nparray_volume_id[i]==9 and nparray_layer_id[i]==14)\n",
    "            #or (nparray_volume_id[i]==16 and nparray_layer_id[i]==2)\n",
    "            #or (nparray_volume_id[i]==12 and nparray_layer_id[i]==2)\n",
    "            #or (nparray_volume_id[i]==18 and nparray_layer_id[i]==12)\n",
    "            #or (nparray_volume_id[i]==14 and nparray_layer_id[i]==12)\n",
    "            #or (nparray_volume_id[i]==17 and nparray_layer_id[i]==4)\n",
    "            )==False:\n",
    "            continue\n",
    "\n",
    "        counterBucket+=1\n",
    "        \n",
    "        #if (counterBucket<1)==False:\n",
    "        #    continue\n",
    "        \n",
    "        #if (counterBucket%10000==1)==False:\n",
    "        #    continue\n",
    "        \n",
    "        if debug or counterBucket%20000==1:\n",
    "            print(\"i\",i,\"counterBucket\",counterBucket)\n",
    "            \n",
    "        # using annoy to find the 20 nearest neighboring hits by angle to this hit\n",
    "        list_index=index.get_nns_by_item(i,bucketSize)\n",
    "        if debug:\n",
    "            print(\"list_index\",list_index)\n",
    "            \n",
    "        # create bucket\n",
    "        df_bucket=df_hits.iloc[list_index]\n",
    "        df_query=df_hits.iloc[i]\n",
    "        \n",
    "        if debug:\n",
    "            print(\"df_bucket\",df_bucket)\n",
    "            print(\"df_query\",df_query)\n",
    "            print(\"df_query_hit_id\",df_query[\"hit_id\"])\n",
    "        \n",
    "        # find the position (index) in the bucket of the query point and store it for later user\n",
    "        query_hitID=df_query[\"hit_id\"].astype(np.uint64)\n",
    "        if debug:\n",
    "            print(\"query_hitID\",query_hitID) \n",
    "        # for loop over hits in bucket and remember at which position (index) inside the bucket we have the query point\n",
    "        nparray_hitID=df_bucket[\"hit_id\"].values.astype(np.uint64)\n",
    "        for j in range(bucketSize):\n",
    "            current_hitID=nparray_hitID[j]\n",
    "            if debug:\n",
    "                print(\"current_hitID\",current_hitID)\n",
    "            if current_hitID==query_hitID:\n",
    "                queryIndex=j\n",
    "                break\n",
    "        # done for loop over hits in bucket\n",
    "        if debug:\n",
    "            print(\"queryIndex\",queryIndex)\n",
    "        # create numpy array for queryIndex\n",
    "        list_queryIndex=[queryIndex]\n",
    "        nparray_queryIndex=np.array(list_queryIndex).astype(np.uint8)\n",
    "        \n",
    "        # create NN input for this one bucket\n",
    "        # convert from default in pandas from np.float64 to np.float32, which is enough for the 4-5 digits it has\n",
    "        nparray_input=df_bucket[[\"x\",\"y\",\"z\"]].values.astype(np.float32).flatten()\n",
    "        if debug:\n",
    "            p(\"nparray_input\",nparray_input)\n",
    "            \n",
    "        # as for input, for volumeID\n",
    "        nparray_volumeID=df_bucket[\"volume_id\"].values.astype(np.uint8)\n",
    "        \n",
    "        # as for input, for layerID\n",
    "        nparray_layerID=df_bucket[\"layer_id\"].values.astype(np.uint8)\n",
    "            \n",
    "        # create NN output for this one bucket\n",
    "        # identify particle with the largest number of hits in this bucket\n",
    "        nparray_particleID=df_bucket[\"particle_id\"].values\n",
    "\n",
    "        if debug:\n",
    "            p(\"nparray_particleID\",nparray_particleID)\n",
    "        dict_particleID_counterParticleID={}\n",
    "        for particleID in nparray_particleID:\n",
    "            if particleID not in dict_particleID_counterParticleID:\n",
    "                dict_particleID_counterParticleID[particleID]=1\n",
    "            else:\n",
    "                dict_particleID_counterParticleID[particleID]+=1\n",
    "        if debug:\n",
    "            print(\"dict_particleID_counterParticleID\",dict_particleID_counterParticleID)\n",
    "          \n",
    "        # find the maximum value of the counters\n",
    "        particleIDWithMaxHits=0\n",
    "        counterParticleIDWithMaxHits=0\n",
    "        for particleID in dict_particleID_counterParticleID:\n",
    "            counterParticleID=dict_particleID_counterParticleID[particleID]\n",
    "            if counterParticleID>counterParticleIDWithMaxHits:\n",
    "                counterParticleIDWithMaxHits=counterParticleID\n",
    "                particleIDWithMaxHits=particleID\n",
    "        if debug:\n",
    "            print(\"counterBucket\",counterBucket,\"particleIDWithMaxHits\",particleIDWithMaxHits,\"counterParticleIDWithMaxHits\",counterParticleIDWithMaxHits)           \n",
    "        \n",
    "        # store the number of positive hits in this bucket, as it will be used a lot later\n",
    "        nparray_nbPositiveHit=np.array([counterParticleIDWithMaxHits]).astype(np.uint8)\n",
    "        \n",
    "        # calculate eta\n",
    "        nparray_tp=df_bucket[[\"tpx\",\"tpy\",\"tpz\"]].values.astype(np.float32)\n",
    "        if debug:\n",
    "            p(\"nparray_tp\",nparray_tp)\n",
    "        \n",
    "        # calculate eta for each hit and store it\n",
    "        if debug:\n",
    "            print(\"Start eta\")\n",
    "            print(\"particleIDWithMaxHits\",particleIDWithMaxHits)\n",
    "            \n",
    "        # for loop over hits in a bucket\n",
    "        # create list of eta for each hit in the bucket\n",
    "        list_eta=[]\n",
    "        for j,particleID in enumerate(nparray_particleID):\n",
    "            #print(\"j\",j,\"particleID\",particleID)\n",
    "            list_tp=nparray_tp[j]\n",
    "            # print(\"list_tp\",list_tp)\n",
    "            tpx=list_tp[0]\n",
    "            tpy=list_tp[1]\n",
    "            tpz=list_tp[2]\n",
    "            # let's assume mass of particle is zero (good approximationf for pions, kaons forming tracks)\n",
    "            m=0.0\n",
    "            #print(\"j\",j,\"tpx=%.4f, tpy=%.4f, tpz=%.4f\"%(tpx,tpy,tpz))\n",
    "            # \n",
    "            eta=calculate_eta(tpx,tpy,tpz,m)\n",
    "            if debug:\n",
    "                print(\"j\",j,\"particleID\",particleID,\"tpx=%.10f, tpy=%.10f, tpz=%.10f, eta=%.10f\"%(tpx,tpy,tpz,eta))\n",
    "            list_eta.append(eta)\n",
    "        # done for loop over hits in bucket\n",
    "        # by printing out I confirmed that all hits belonging to the particle have exactly the same tpx,tpy,tpz\n",
    "        # list_eta has bucketSize elements, so now make it a numpy array and convert it to type float 32\n",
    "        nparray_eta=np.array(list_eta).astype(np.float32)\n",
    "        if debug:\n",
    "            p(\"nparray_eta\",nparray_eta)\n",
    "        \n",
    "        # create nparray_output\n",
    "        list_output=[]\n",
    "        \n",
    "        # loop over every hit in the bucket\n",
    "        for particleID in nparray_particleID:      \n",
    "            if particleID==particleIDWithMaxHits:\n",
    "                list_output.append(1)\n",
    "            else:\n",
    "                list_output.append(-1)\n",
    "            # done if\n",
    "        # done for loop for each hit in the bucket\n",
    "        if debug:        \n",
    "            print(\"list_output\",list_output)\n",
    "        # create nparray for output from list and convert from the default int64 to int8\n",
    "        nparray_output=np.array(list_output).astype(np.int8)\n",
    "        if debug:\n",
    "            print(\"nparray_output\",nparray_output)\n",
    "            \n",
    "        if doBucketPlots and counterBucket%1000==0:\n",
    "            print(\"i\",i,\"counterBucket\",counterBucket)\n",
    "            \n",
    "            outputFileNameStem=outputFolderName+\"/plot_vis_e_\"+eventNumber+\"_i_\"+str(i)+\"_c_\"+str(counterBucket)+\"_\"+my_title+\"_\"\n",
    "            if True:\n",
    "                # plot x vs y\n",
    "                plt.plot(df_bucket.x,df_bucket.y,\"o\",color=\"red\")\n",
    "                plt.plot(df_query.x,df_query.y,\"D\",color=\"blue\")\n",
    "                plt.xlabel(\"x [mm]\")\n",
    "                plt.ylabel(\"y [mm]\")\n",
    "                plt.xlim(left=-1050,right=1050)\n",
    "                plt.ylim(bottom=-1050,top=1050)\n",
    "                #plt.plot(0,0,\"r+\")\n",
    "                plt.title(my_title+\". y vs x. Blue=query. eta=%.2f. i=%.0f\"%(eta,i))\n",
    "                # plt.show()\n",
    "                plotBucketFileNameStem=outputFileNameStem+\"y_vs_x\"\n",
    "                plt.savefig(plotBucketFileNameStem+\".png\")\n",
    "                #plt.savefig(plotBucketFileNameStem+\".pdf\")\n",
    "                plt.close()\n",
    "            # done if\n",
    "            if True:\n",
    "                # plot z vs r\n",
    "                df_bucket_r=np.sqrt(df_bucket.x**2+df_bucket.y**2)\n",
    "                df_query_r=np.sqrt(df_query.x**2+df_query.y**2)\n",
    "                plt.plot(df_bucket.z,df_bucket_r,\"o\",color=\"red\")\n",
    "                plt.plot(df_query.z,df_query_r,\"D\",color=\"blue\")\n",
    "                plt.xlabel(\"z [mm]\")\n",
    "                plt.ylabel(\"radius [mm]\")\n",
    "                plt.xlim(left=-3050,right=3050)\n",
    "                plt.ylim(bottom=-1050,top=1050)\n",
    "                #plt.plot(0,0,\"r+\")\n",
    "                plt.title(my_title+\". r vs z. Blue=query. eta=%.2f. i=%.0f\"%(eta,i))\n",
    "                # plt.show()\n",
    "                plotBucketFileNameStem=outputFileNameStem+\"r_vs_z\"\n",
    "                plt.savefig(plotBucketFileNameStem+\".png\")\n",
    "                #plt.savefig(plotBucketFileNameStem+\".pdf\")\n",
    "                plt.close()\n",
    "            # done if\n",
    "        # done if\n",
    "            \n",
    "        # add for this current bucket to the list for the entire event (which has several buckets)\n",
    "        list_nparray_input.append(nparray_input)\n",
    "        list_nparray_output.append(nparray_output)\n",
    "        list_nparray_eta.append(nparray_eta)\n",
    "        list_nparray_volumeID.append(nparray_volumeID)\n",
    "        list_nparray_layerID.append(nparray_layerID)\n",
    "        list_nparray_queryIndex.append(nparray_queryIndex)\n",
    "        list_nparray_npPositiveHit.append(nparray_nbPositiveHit)\n",
    "    # done for loop over hits in the event, so that for each hit we make a bucket\n",
    "    \n",
    "    # transform list of numpy array into numpy array of dimension 2 \n",
    "    # rows are buckets, columns are intput and output values\n",
    "    nparray_input_all=np.array(list_nparray_input)\n",
    "    nparray_output_all=np.array(list_nparray_output)\n",
    "    nparray_eta_all=np.array(list_nparray_eta)\n",
    "    nparray_volumeID_all=np.array(list_nparray_volumeID)\n",
    "    nparray_layerID_all=np.array(list_nparray_layerID)\n",
    "    nparray_queryIndex_all=np.array(list_nparray_queryIndex)\n",
    "    nparray_nbPositiveHit_all=np.array(list_nparray_nbPositiveHit)\n",
    "    if debug:\n",
    "        p(\"nparray_input_all\",nparray_input_all)\n",
    "        p(\"nparray_output_all\",nparray_output_all)\n",
    "        p(\"nparray_eta_all\",nparray_eta_all)\n",
    "        p(\"nparray_volumeID_all\",nparray_volumeID_all)\n",
    "        p(\"nparray_layerID_all\",nparray_layerID_all)\n",
    "        p(\"nparray_queryIndex_all\",nparray_queryIndex_all)\n",
    "        p(\"nparray_nbPositiveHit_all\",nparray_nbPositiveHit_all)\n",
    "    \n",
    "    return nparray_input_all, nparray_output_all, nparray_eta_all, nparray_volumeID_all, nparray_layerID_all, nparray_queryIndex_all, nparray_nbPositiveHit_all\n",
    "# done function      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def do_run_one_event_at_a_time(list_eventNumber):\n",
    "    # for loop over eventNumber from list_eventNumber and write individual event to files\n",
    "    for i,eventNumber in enumerate(list_eventNumber):\n",
    "        #if i>0:\n",
    "        #    continue\n",
    "    \n",
    "        print()\n",
    "        print()\n",
    "        print(\"******************************\")\n",
    "        print(\"i\",i,\"eventNumber\",eventNumber)\n",
    "        print(\"******************************\")\n",
    "    \n",
    "        inputFileName_hits_recon=inputFolderName+\"/event\"+eventNumber+\"-hits.csv\"\n",
    "        inputFileName_hits_truth=inputFolderName+\"/event\"+eventNumber+\"-truth.csv\"\n",
    "    \n",
    "        if debug or verbose:\n",
    "            print(\"Read csv files ad df and merge them\")\n",
    "        df_hits_recon=pd.read_csv(inputFileName_hits_recon)\n",
    "        df_hits_truth=pd.read_csv(inputFileName_hits_truth)\n",
    "        # hit_id appears in both, erase from one so that when we concatenate below it appears only once\n",
    "        del df_hits_truth[\"hit_id\"]\n",
    "        df_hits=pd.concat([df_hits_recon,df_hits_truth],axis=1,sort=False)\n",
    "    \n",
    "        # build annoy index\n",
    "        if debug or verbose:\n",
    "            print(\"Start do Annoy Index\")\n",
    "        nparray_position=df_hits[[\"x\",\"y\",\"z\"]].values\n",
    "        if debug:\n",
    "            p(\"nparray_position\",nparray_position)\n",
    "\n",
    "        index=buildAnnoyIndex(nparray_position,metric=\"angular\",ntrees=10,debug=False)\n",
    "    \n",
    "        # for loop peste hituri peste hiturile preselectate volume_id=8, layer_id=2\n",
    "        if debug or verbose:\n",
    "            print(\"From df_hits produce nparray_input_all,nparray_output_all\")\n",
    "        nparray_input_all,nparray_output_all,nparray_eta_all,nparray_volumeID_all,nparray_layerID_all,nparray_queryIndex_all,nparray_nbPositiveHit_all=get_input_output_from_df_hits(df_hits,index)\n",
    "    \n",
    "        if debug:\n",
    "            p(\"nparray_input_all\",nparray_input_all)\n",
    "            p(\"nparray_output_all\",nparray_output_all)\n",
    "            p(\"nparray_eta_all\",nparray_eta_all)\n",
    "            p(\"nparray_volumeID_all\",nparray_volumeID_all)\n",
    "            p(\"nparray_layerID_all\",nparray_layerID_all)\n",
    "            p(\"nparray_queryIndex_all\",nparray_queryIndex_all)\n",
    "            p(\"nparray_nbPositiveHit_all\",nparray_nbPositiveHit_all)\n",
    "    \n",
    "        # reshape only input by adding one extra dimension needed by tensorflow, in practice it puts all into one extra bracket\n",
    "        nparray_input_all=nparray_input_all.reshape(nparray_input_all.shape[0],nparray_input_all.shape[1],1)\n",
    "        if debug or verbose:\n",
    "            p(\"nparray_input_all after reshape\",nparray_input_all)\n",
    "\n",
    "        fileNameNNInputAll=outputFolderName+\"/NN_2_data_Input_\"+eventNumber+\".npy\"\n",
    "        fileNameNNOutputAll=outputFolderName+\"/NN_2_data_Output_\"+eventNumber+\".npy\"\n",
    "        fileNameNNEtaAll=outputFolderName+\"/NN_2_data_Eta_\"+eventNumber+\".npy\"\n",
    "        fileNameNNVolumeIDAll=outputFolderName+\"/NN_2_data_VolumeID_\"+eventNumber+\".npy\"\n",
    "        fileNameNNLayerIDAll=outputFolderName+\"/NN_2_data_LayerID_\"+eventNumber+\".npy\"\n",
    "        fileNameNNQueryIndexAll=outputFolderName+\"/NN_2_data_QueryIndex_\"+eventNumber+\".npy\"\n",
    "        fileNameNNNbPositiveHitAll=outputFolderName+\"/NN_2_data_NbPositiveHit_\"+eventNumber+\".npy\"\n",
    "            \n",
    "        np.save(fileNameNNInputAll,nparray_input_all)\n",
    "        np.save(fileNameNNOutputAll,nparray_output_all)\n",
    "        np.save(fileNameNNEtaAll,nparray_eta_all)\n",
    "        np.save(fileNameNNVolumeIDAll,nparray_volumeID_all)\n",
    "        np.save(fileNameNNLayerIDAll,nparray_layerID_all)\n",
    "        np.save(fileNameNNQueryIndexAll,nparray_queryIndex_all)\n",
    "        np.save(fileNameNNNbPositiveHitAll,nparray_nbPositiveHit_all)\n",
    "\n",
    "    # done for loop over events\n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doIndividual:\n",
    "    do_run_one_event_at_a_time(list_eventNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done do_create_output_based_on_min_nb_of_positive_hits(list_eventNumber\n"
     ]
    }
   ],
   "source": [
    "def do_create_output_based_on_min_nb_of_positive_hits(list_eventNumber):\n",
    "    nparray_output_negative_one_bucket=np.array([-1 for i in range(bucketSize)]).astype(np.int8)\n",
    "    # for loop over eventNumber from list_eventNumber\n",
    "    for i,eventNumber in enumerate(list_eventNumber):\n",
    "        if debug or verbose:\n",
    "            print(\"eventNumber\",eventNumber)\n",
    "        # open the files needed for this event: Output and NbPositiveHit\n",
    "        fileNameNNOutputAll=outputFolderName+\"/NN_2_data_Output_\"+eventNumber+\".npy\"\n",
    "        nparray_output_all=np.load(fileNameNNOutputAll)\n",
    "        fileNameNNNbPositiveHitAll=outputFolderName+\"/NN_2_data_NbPositiveHit_\"+eventNumber+\".npy\"\n",
    "        nparray_nbPositiveHit_all=np.load(fileNameNNNbPositiveHitAll)\n",
    "        # \n",
    "        dict_minNbPositiveHit_nparray_output_all={}\n",
    "        # for loop over each val of min number of positive fits\n",
    "        for minNbPositiveHit in list_minNbPositiveHit:\n",
    "            if debug:\n",
    "                print(\"i\",i,\"eventNumber\",eventNumber,\"minNbPositiveHit\",minNbPositiveHit)\n",
    "            # copy the current output in a completely new version with deep copy\n",
    "            # as we will want to replace only the values that need changed for less than needed positive hits\n",
    "            dict_minNbPositiveHit_nparray_output_all[minNbPositiveHit]=copy.deepcopy(nparray_output_all)\n",
    "        # done for loop over minNbPositiveHit\n",
    "        #\n",
    "        # loop over each row (bucket) in the output\n",
    "        for j in range(len(nparray_output_all)):\n",
    "            bucket=nparray_output_all[j]\n",
    "            if debug:\n",
    "                p(\"j=%.0f, bucket\"%j,bucket)\n",
    "            nbPositiveHit=nparray_nbPositiveHit_all[j]\n",
    "            if debug:\n",
    "                print(\"j=%.0f, nbPositiveHit=%.0f\"%(j,nbPositiveHit))\n",
    "            # to speed up, change only wheen needed\n",
    "            for minNbPositiveHit in list_minNbPositiveHit:\n",
    "                if nbPositiveHit<minNbPositiveHit:\n",
    "                    # replace this one line with the one with all negative\n",
    "                    dict_minNbPositiveHit_nparray_output_all[minNbPositiveHit][j]=nparray_output_negative_one_bucket\n",
    "                if debug:\n",
    "                    print(\"j=%.0f, minNbPositiveHit=%.0f, bucket\"%(j,minNbPositiveHit),dict_minNbPositiveHit_nparray_output_all[minNbPositiveHit][j])\n",
    "            # done for loop over minNbPositiveHit  \n",
    "        # done for loop over buckets\n",
    "        # for this event, write the new processed output to file\n",
    "        for minNbPositiveHit in list_minNbPositiveHit:\n",
    "            minString=\"Min\"\n",
    "            if minNbPositiveHit<9:\n",
    "                minString+=\"0\"\n",
    "            minString+=str(minNbPositiveHit)\n",
    "            fileNameNNOutputAll=outputFolderName+\"/NN_2_data_Output\"+minString+\"_\"+eventNumber+\".npy\"\n",
    "            np.save(fileNameNNOutputAll,dict_minNbPositiveHit_nparray_output_all[minNbPositiveHit])\n",
    "        # done for loop over minNbPositiveHit  \n",
    "    # done for loop over events\n",
    "# done function\n",
    "if debug or verbose:\n",
    "    print(\"Done do_create_output_based_on_min_nb_of_positive_hits(list_eventNumber\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doProcessIndividual:\n",
    "    do_create_output_based_on_min_nb_of_positive_hits(list_eventNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_merge_events(mergeOption):\n",
    "    # create output folder name for the merge, for the current number of events to merge\n",
    "    eventNumberMin=mergeOption[0] # included\n",
    "    eventNumberMax=mergeOption[1] # excluded\n",
    "    #\n",
    "    outputFolderNameMerge=outputFolderName+\"_ev_\"\n",
    "    #\n",
    "    if eventNumberMin<10:\n",
    "        outputFolderNameMerge+=\"00\"\n",
    "    elif eventNumberMin<100:\n",
    "        outputFolderNameMerge+=\"0\"\n",
    "    outputFolderNameMerge+=str(eventNumberMin)\n",
    "    #\n",
    "    outputFolderNameMerge+=\"_\"\n",
    "    #\n",
    "    if eventNumberMax<10:\n",
    "        outputFolderNameMerge+=\"00\"\n",
    "    elif eventNumberMax<100:\n",
    "        outputFolderNameMerge+=\"0\"\n",
    "    outputFolderNameMerge+=str(eventNumberMax)\n",
    "    #\n",
    "    if not os.path.exists(outputFolderNameMerge):\n",
    "        os.makedirs(outputFolderNameMerge)\n",
    "    \n",
    "    # for loop over events and merge them in train or test and save in a different folder\n",
    "    counter_event_Train=0\n",
    "    counter_event_Test=0\n",
    "    if debug or verbose:\n",
    "        print(\"Start loop over all events and merge them in either train or test\")\n",
    "    \n",
    "    # list of variables to merge\n",
    "    list_var=[\n",
    "        \"Input\",\n",
    "        \"Output\",\n",
    "        \"VolumeID\",\n",
    "        \"LayerID\",\n",
    "        \"Eta\",\n",
    "        \"QueryIndex\",\n",
    "        \"NbPositiveHit\",\n",
    "    ]\n",
    "    # add to list_var also the OutputMin04, OutputMin07, etc\n",
    "    for minNbPositiveHit in list_minNbPositiveHit:\n",
    "        minString=\"OutputMin\"\n",
    "        if minNbPositiveHit<9:\n",
    "            minString+=\"0\"\n",
    "        minString+=str(minNbPositiveHit)\n",
    "        list_var.append(minString)\n",
    "    # done for loop over minNbPositiveHit\n",
    "    if debug:\n",
    "        print(\"list_var\",list_var)\n",
    "    \n",
    "    #\n",
    "    dict_nameMerged_nparray={}\n",
    "    \n",
    "    # loop over the events, but careful, not all, but only from the first nrEventsToMerge\n",
    "    for i,eventNumber in enumerate(list_eventNumber[eventNumberMin:eventNumberMax]):\n",
    "        print(\"i\",i,\"eventNumber\",eventNumber)\n",
    "        \n",
    "        # for loop over all variables\n",
    "        dict_var_nparray={}\n",
    "        for var in list_var:\n",
    "            # read the stored file\n",
    "            fileName=outputFolderName+\"/NN_2_data_\"+var+\"_\"+eventNumber+\".npy\"\n",
    "            dict_var_nparray[var]=np.load(fileName)\n",
    "        # done for loop over var\n",
    "        \n",
    "        # calculate the rest of the event index to division by 100\n",
    "        # assuming we only merge multiples of ten, for each 10, we put first 7 in Train and last 3 in Test\n",
    "        rest=i%10\n",
    "\n",
    "        if rest<7: \n",
    "            # put this event into Train, with rest=i%10=0,1,2,3,4,5,6\n",
    "            counter_event_Train+=1\n",
    "            if counter_event_Train==1:\n",
    "                # it is the first event of type Train, so we simply deep copy it to the output\n",
    "                for var in list_var:\n",
    "                    dict_nameMerged_nparray[\"Train\"+var]=copy.deepcopy(dict_var_nparray[var])\n",
    "            else:\n",
    "                # it is not the first, so we concatentane to the new output\n",
    "                for var in list_var:\n",
    "                    dict_nameMerged_nparray[\"Train\"+var]=np.concatenate((dict_nameMerged_nparray[\"Train\"+var],dict_var_nparray[var]),axis=0,out=None)\n",
    "            # done if\n",
    "        else:\n",
    "            # put this event into Test, with rest=i%10=7,8,9\n",
    "            counter_event_Test+=1\n",
    "            if counter_event_Test==1:\n",
    "                # it is the first event of type Test, so we simply deep copy it to the output\n",
    "                for var in list_var:\n",
    "                    dict_nameMerged_nparray[\"Test\"+var]=copy.deepcopy(dict_var_nparray[var])\n",
    "            else:\n",
    "                # it is not the first, so we concatentane to the new output\n",
    "                for var in list_var:\n",
    "                    dict_nameMerged_nparray[\"Test\"+var]=np.concatenate((dict_nameMerged_nparray[\"Test\"+var],dict_var_nparray[var]),axis=0,out=None)\n",
    "            # done if\n",
    "        # done if  \n",
    "    # done for loop over eventNumber\n",
    "\n",
    "    # write file to the output\n",
    "    eventNumber=\"all\"\n",
    "    for TrainOrTest in \"Train,Test\".split(\",\"):\n",
    "        for var in list_var:\n",
    "            fileName=outputFolderNameMerge+\"/NN_2_data_\"+var+\"_\"+TrainOrTest+\"_\"+eventNumber+\".npy\"\n",
    "            np.save(fileName,dict_nameMerged_nparray[TrainOrTest+var])\n",
    "            if debug:\n",
    "                p(\"nparray_\"+TrainOrTest+\"_\"+var+\"_all_events\",dict_nameMerged_nparray[TrainOrTest+var])\n",
    "        # done for loop over var\n",
    "    # done for loop over TrainOrTest\n",
    "    \n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doMerge:\n",
    "    for mergeOption in list_mergeOption:\n",
    "        print()\n",
    "        print()\n",
    "        print(\"******************************\")\n",
    "        print(\"mergeOption\",mergeOption)\n",
    "        print(\"******************************\")\n",
    "        do_merge_events(mergeOption)\n",
    "        if debug:\n",
    "            print(\"Done mergeOption\",mergeOption)\n",
    "    # done for loop over mergeOption\n",
    "# done if"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_balance(TrainOrTest,nparray_input,nparray_output,nparray_nbPositiveHit,nparray_eta,nparray_layerID,nparray_volumeID,nparray_queryIndex,outputFolderNameMerge,doVerifyForHit=False,debug=False,verbose=False):\n",
    "    if debug or verbose:\n",
    "        print(\"Start get_nparray_weight for TrainOrTest=\",TrainOrTest)\n",
    " \n",
    "    if verbose:\n",
    "        p(\"nparray_input\",nparray_input)\n",
    "        p(\"nparray_output\",nparray_output)\n",
    "        p(\"nparray_nbPositiveHit\",nparray_nbPositiveHit)\n",
    "        p(\"nparray_eta\",nparray_eta)\n",
    "        p(\"nparray_layerID\",nparray_layerID)\n",
    "        p(\"nparray_volumeID\",nparray_volumeID)\n",
    "        p(\"nparray_queryIndex\",nparray_queryIndex)\n",
    "\n",
    "    # total number of buckets\n",
    "    nbBucket=nparray_nbPositiveHit.shape[0]\n",
    "    nrHitTotal=nbBucket*bucketSize\n",
    "    if debug or verbose:\n",
    "        print()\n",
    "        print()\n",
    "        print(TrainOrTest+\":\")\n",
    "        print(\"nbBucket\",nbBucket,\"bucketSize\",bucketSize,\"nrHitTotal\",nrHitTotal)\n",
    "        \n",
    "        \n",
    "    # dictionary between number of positive hits in a bucket and the number of buckets with this number of positive hits\n",
    "    dict_nbPositiveHit_counterBucket_unbalanced={}\n",
    "    dict_nbPositiveHit_counterBucket_balancedDesired={}\n",
    "    dict_nbPositiveHit_counterBucket_balancedCounter={}\n",
    "    dict_nbPositiveHit_counterBucket_balancedObtained={}\n",
    "    # create explicit the list of posssible values for nbPositiveHit (0, 1, 2, ..., 19, 20)\n",
    "    list_nbPositiveHit=[i for i in range(0,21)]\n",
    "    for nbPositiveHit in list_nbPositiveHit:\n",
    "        dict_nbPositiveHit_counterBucket_unbalanced[nbPositiveHit]=0\n",
    "        dict_nbPositiveHit_counterBucket_balancedDesired[nbPositiveHit]=0\n",
    "        dict_nbPositiveHit_counterBucket_balancedCounter[nbPositiveHit]=0\n",
    "        dict_nbPositiveHit_counterBucket_balancedObtained[nbPositiveHit]=0\n",
    "    \n",
    "    # now fill the dictionary of the counters depending on nbPositiveHit\n",
    "    # loop over buckets\n",
    "    for i in range(nbBucket):\n",
    "        nbPositiveHit=nparray_nbPositiveHit[i][0] # one column only, so take that element index [0]\n",
    "        if debug:\n",
    "            print(\"nbPositiveHit\",nbPositiveHit)\n",
    "        dict_nbPositiveHit_counterBucket_unbalanced[nbPositiveHit]+=1\n",
    "\n",
    "    # now dictionary is filled and we can print the values\n",
    "    if debug or verbose:\n",
    "        print()\n",
    "        print(\"Original unbalanced. \"+TrainOrTest+\":\")\n",
    "        for nbPositiveHit in list_nbPositiveHit:\n",
    "            counterBucket=dict_nbPositiveHit_counterBucket_unbalanced[nbPositiveHit]\n",
    "            percentBucket=100*counterBucket/nbBucket\n",
    "            print(\"nbPositiveHit=%.0f\"%nbPositiveHit,\"counterBucket=%.0f\"%counterBucket,\"percentBucket=%.1f\"%percentBucket)\n",
    "            \n",
    "    # plot legend position\n",
    "    legend_position=\"upper right\"\n",
    "            \n",
    "    # make plots\n",
    "    # we want bins from 0 to 21, as bin 20 is between 20 and 21\n",
    "    bins=[i-0.5 for i in range(0,22)]\n",
    "    print(\"bins\",bins)\n",
    "    # plot not normalized\n",
    "    plt.hist(nparray_nbPositiveHit,bins=bins,histtype=\"step\",color=\"red\",label=\"Unbalanced\",)\n",
    "    plt.title(\"Histogram not normalized for \"+TrainOrTest)\n",
    "    plt.xlabel(\"Number of positive hits per bucket\")\n",
    "    plt.ylabel(\"Number of buckets\")\n",
    "    plt.legend(loc=legend_position)\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.savefig(outputFolderNameMerge+\"/plot_histo_1_NbBucket_vs_NbPositiveHit_\"+TrainOrTest+\".png\")\n",
    "    plt.savefig(outputFolderNameMerge+\"/plot_histo_1_NbBucket_vs_NbPositiveHit_\"+TrainOrTest+\".pdf\")\n",
    "    plt.close()\n",
    "    # plot normalized\n",
    "    plt.hist(nparray_nbPositiveHit,bins=bins,density=True,histtype=\"step\",color=\"red\",label=\"Unbalanced\")\n",
    "    plt.title(\"Histogram normalized for \"+TrainOrTest)\n",
    "    plt.xlabel(\"Number of positive hits per bucket\")\n",
    "    plt.ylabel(\"Number of buckets\")\n",
    "    plt.legend(loc=legend_position)\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.savefig(outputFolderNameMerge+\"/plot_histo_1_NbBucket_vs_NbPositiveHit_normalized_\"+TrainOrTest+\".png\")\n",
    "    plt.savefig(outputFolderNameMerge+\"/plot_histo_1_NbBucket_vs_NbPositiveHit_normalized_\"+TrainOrTest+\".pdf\")\n",
    "    plt.close()\n",
    "    \n",
    "    # we want to balance out around 10 by removing not needed buckets, to balance 9 with 11, 8 with 12, etc\n",
    "    \n",
    "    # first calculate the desired number of buckets in each category, so that it is balanced around 10\n",
    "    for i in range(bucketSize//2+1):\n",
    "        #print(\"i\",i)\n",
    "        val_left=dict_nbPositiveHit_counterBucket_unbalanced[i]\n",
    "        #print(\"val_left\",val_left)\n",
    "        val_right=dict_nbPositiveHit_counterBucket_unbalanced[bucketSize-i]\n",
    "        #print(\"val_right\",val_right)\n",
    "        if val_left<=val_right:\n",
    "            val_min=val_left\n",
    "        else:\n",
    "            val_min=val_right\n",
    "        dict_nbPositiveHit_counterBucket_balancedDesired[i]=val_min\n",
    "        dict_nbPositiveHit_counterBucket_balancedDesired[bucketSize-i]=val_min\n",
    "    # done for loop over i \n",
    "\n",
    "    # now dictionary is filled and we can print the values\n",
    "    if debug or verbose:\n",
    "        print()\n",
    "        print(\"BalancedDesired. \"+TrainOrTest+\":\")\n",
    "        for nbPositiveHit in list_nbPositiveHit:\n",
    "            counterBucket=dict_nbPositiveHit_counterBucket_balancedDesired[nbPositiveHit]\n",
    "            percentBucket=100*counterBucket/nbBucket\n",
    "            print(\"nbPositiveHit=%.0f\"%nbPositiveHit,\"counterBucket=%.0f\"%counterBucket,\"percentBucket=%.1f\"%percentBucket)\n",
    "    \n",
    "\n",
    "    # we want a nparray_inputC that will have a subset of nparray_input, same for nparray_output\n",
    "    list_inputBalanced=[]\n",
    "    list_outputBalanced=[]\n",
    "    list_nbPositiveHitBalanced=[]\n",
    "    list_etaBalanced=[]\n",
    "    list_layerIDBalanced=[]\n",
    "    list_volumeIDBalanced=[]\n",
    "    list_queryIndexBalanced=[]\n",
    "    # loop over buckets\n",
    "    \n",
    "    print()\n",
    "    for i in range(nbBucket):\n",
    "        # first decide if I need to keep this bucket or not\n",
    "        # first find the nbPositiveHit of this bucket\n",
    "        nbPositiveHit=nparray_nbPositiveHit[i][0]\n",
    "        # find how many buckets I am allowed to keep in this category\n",
    "        nrBucketDesired=dict_nbPositiveHit_counterBucket_balancedDesired[nbPositiveHit]\n",
    "        #print(\"j\",j,\"nbPositiveHit\",nbPositiveHit,\"nrBucketDesired\",nrBucketDesired)\n",
    "        # find if this bucket is allowed, meaning if the counter of this bucketg is smaller than allowed\n",
    "        # so I need to find the current counter\n",
    "        dict_nbPositiveHit_counterBucket_balancedCounter[nbPositiveHit]+=1\n",
    "        # now counter is increased and I can make the check, so first bucket has counter 1 (not start from 0)\n",
    "        # if current current counter smaller or equal with the allowed, you write the current bucket\n",
    "        # if not, you skip it\n",
    "        #print(\"nbPositiveHit\",nbPositiveHit,\"nrBucketDesired\",nrBucketDesired,\"counter\",dict_nbPositiveHit_counterBucket_balancedObtained[nbPositiveHit])\n",
    "        if dict_nbPositiveHit_counterBucket_balancedCounter[nbPositiveHit]<=nrBucketDesired:\n",
    "            dict_nbPositiveHit_counterBucket_balancedObtained[nbPositiveHit]+=1\n",
    "            list_inputBalanced.append(nparray_input[i])\n",
    "            list_outputBalanced.append(nparray_output[i])\n",
    "            list_nbPositiveHitBalanced.append(nparray_nbPositiveHit[i])\n",
    "            list_etaBalanced.append(nparray_eta[i])\n",
    "            list_layerIDBalanced.append(nparray_layerID[i])\n",
    "            list_volumeIDBalanced.append(nparray_volumeID[i])\n",
    "            list_queryIndexBalanced.append(nparray_queryIndex[i])           \n",
    "        else:\n",
    "            # do not add it, so that the datasets remained balanced\n",
    "            #print(\"Not add nbPositiveHit\",nbPositiveHit,\"nrBucketDesired\",nrBucketDesired,\"counter\",dict_nbPositiveHit_counterBucket_balancedObtained[nbPositiveHit])\n",
    "            pass\n",
    "        # done if\n",
    "    # done loop over buckets\n",
    "    \n",
    "    # now dictionary is filled and we can print the values\n",
    "    if debug:\n",
    "        print()\n",
    "        print(\"BalancedCounter after. \"+TrainOrTest+\":\")\n",
    "        for nbPositiveHit in list_nbPositiveHit:\n",
    "            counterBucket=dict_nbPositiveHit_counterBucket_balancedObtained[nbPositiveHit]\n",
    "            percentBucket=100*counterBucket/nbBucket\n",
    "            print(\"nbPositiveHit=%.0f\"%nbPositiveHit,\"counterBucket=%.0f\"%counterBucket,\"percentBucket=%.1f\"%percentBucket)\n",
    "\n",
    "    \n",
    "    # now dictionary is filled and we can print the values\n",
    "    if debug or verbose:\n",
    "        print()\n",
    "        print(\"BalancedObtained after. \"+TrainOrTest+\":\")\n",
    "        for nbPositiveHit in list_nbPositiveHit:\n",
    "            counterBucket=dict_nbPositiveHit_counterBucket_balancedObtained[nbPositiveHit]\n",
    "            percentBucket=100*counterBucket/nbBucket\n",
    "            print(\"nbPositiveHit=%.0f\"%nbPositiveHit,\"counterBucket=%.0f\"%counterBucket,\"percentBucket=%.1f\"%percentBucket)\n",
    "\n",
    "    # convert list to numpy arrays\n",
    "    nparray_inputBalanced=np.array(list_inputBalanced)\n",
    "    nparray_outputBalanced=np.array(list_outputBalanced)\n",
    "    nparray_nbPositiveHitBalanced=np.array(list_nbPositiveHitBalanced)\n",
    "    nparray_etaBalanced=np.array(list_etaBalanced)\n",
    "    nparray_layerIDBalanced=np.array(list_layerIDBalanced)\n",
    "    nparray_volumeIDBalanced=np.array(list_volumeIDBalanced)\n",
    "    nparray_queryIndexBalanced=np.array(list_queryIndexBalanced)\n",
    "    #\n",
    "    p(\"nparray_inputBalanced\",nparray_inputBalanced)\n",
    "    outputFileNameBalanced=outputFolderNameMerge+\"/NN_2_data_InputBalanced_\"+TrainOrTest+\"_all.npy\"\n",
    "    np.save(outputFileNameBalanced,nparray_inputBalanced)\n",
    "    #\n",
    "    p(\"nparray_outputBalanced\",nparray_outputBalanced)\n",
    "    outputFileNameBalanced=outputFolderNameMerge+\"/NN_2_data_OutputBalanced_\"+TrainOrTest+\"_all.npy\"\n",
    "    np.save(outputFileNameBalanced,nparray_outputBalanced)\n",
    "    #\n",
    "    p(\"nparray_nbPositiveHitBalanced\",nparray_nbPositiveHitBalanced)\n",
    "    outputFileNameBalanced=outputFolderNameMerge+\"/NN_2_data_NbPositiveHitBalanced_\"+TrainOrTest+\"_all.npy\"\n",
    "    np.save(outputFileNameBalanced,nparray_nbPositiveHitBalanced)\n",
    "    \n",
    "    p(\"nparray_etaBalanced\",nparray_etaBalanced)\n",
    "    outputFileNameBalanced=outputFolderNameMerge+\"/NN_2_data_EtaBalanced_\"+TrainOrTest+\"_all.npy\"\n",
    "    np.save(outputFileNameBalanced,nparray_etaBalanced)\n",
    "    \n",
    "    p(\"nparray_layerIDBalanced\",nparray_layerIDBalanced)\n",
    "    outputFileNameBalanced=outputFolderNameMerge+\"/NN_2_data_LayerIDBalanced_\"+TrainOrTest+\"_all.npy\"\n",
    "    np.save(outputFileNameBalanced,nparray_layerIDBalanced)\n",
    "    \n",
    "    p(\"nparray_volumeIDBalanced\",nparray_volumeIDBalanced)\n",
    "    outputFileNameBalanced=outputFolderNameMerge+\"/NN_2_data_VolumeIDBalanced_\"+TrainOrTest+\"_all.npy\"\n",
    "    np.save(outputFileNameBalanced,nparray_volumeIDBalanced)\n",
    "    \n",
    "    p(\"nparray_queryIndexBalanced\",nparray_queryIndexBalanced)\n",
    "    outputFileNameBalanced=outputFolderNameMerge+\"/NN_2_data_QueryIndexBalanced_\"+TrainOrTest+\"_all.npy\"\n",
    "    np.save(outputFileNameBalanced,nparray_queryIndexBalanced)\n",
    "    \n",
    "    # make plots\n",
    "    # we want bins from 0 to 21, as bin 20 is between 20 and 21\n",
    "    bins=[i-0.5 for i in range(0,22)]\n",
    "    print(\"bins\",bins)\n",
    "    # plot not normalized\n",
    "    plt.hist(nparray_nbPositiveHit,bins=bins,histtype=\"step\",color=\"red\",label=\"Unbalanced\")\n",
    "    plt.hist(nparray_nbPositiveHitBalanced,bins=bins,histtype=\"step\",color=\"blue\",label=\"Balanced\")\n",
    "    plt.title(\"Histogram not normalized for \"+TrainOrTest)\n",
    "    plt.xlabel(\"Number of positive hits per bucket\")\n",
    "    plt.ylabel(\"Number of buckets\")\n",
    "    plt.legend(loc=legend_position)\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.savefig(outputFolderNameMerge+\"/plot_histo_2_NbBucket_vs_NbPositiveHit_\"+TrainOrTest+\".png\")\n",
    "    plt.savefig(outputFolderNameMerge+\"/plot_histo_2_NbBucket_vs_NbPositiveHit_\"+TrainOrTest+\".pdf\")\n",
    "    plt.close()\n",
    "    # plot normalized\n",
    "    plt.hist(nparray_nbPositiveHit,bins=bins,density=True,histtype=\"step\",color=\"red\",label=\"Unbalanced\")\n",
    "    plt.hist(nparray_nbPositiveHitBalanced,bins=bins,density=True,histtype=\"step\",color=\"blue\",label=\"Balanced\")\n",
    "    plt.title(\"Histogram normalized for \"+TrainOrTest)\n",
    "    plt.xlabel(\"Number of positive hits per bucket\")\n",
    "    plt.ylabel(\"Number of buckets\")\n",
    "    plt.legend(loc=legend_position)\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.savefig(outputFolderNameMerge+\"/plot_histo_2_NbBucket_vs_NbPositiveHit_normalized_\"+TrainOrTest+\".png\")\n",
    "    plt.savefig(outputFolderNameMerge+\"/plot_histo_2_NbBucket_vs_NbPositiveHit_normalized_\"+TrainOrTest+\".pdf\")\n",
    "    plt.close()\n",
    "\n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_balance_mergeOption(mergeOption):\n",
    "    if debug or verbose:\n",
    "        print(\"do_balance_mergeOption(), with mergeOption=\",mergeOption)\n",
    "        \n",
    "        \n",
    "    # create output folder name for the merge, for the current number of events to merge\n",
    "    eventNumberMin=mergeOption[0] # included\n",
    "    eventNumberMax=mergeOption[1] # excluded\n",
    "    #\n",
    "    outputFolderNameMerge=outputFolderName+\"_ev_\"\n",
    "    #\n",
    "    if eventNumberMin<10:\n",
    "        outputFolderNameMerge+=\"00\"\n",
    "    elif eventNumberMin<100:\n",
    "        outputFolderNameMerge+=\"0\"\n",
    "    outputFolderNameMerge+=str(eventNumberMin)\n",
    "    #\n",
    "    outputFolderNameMerge+=\"_\"\n",
    "    #\n",
    "    if eventNumberMax<10:\n",
    "        outputFolderNameMerge+=\"00\"\n",
    "    elif eventNumberMax<100:\n",
    "        outputFolderNameMerge+=\"0\"\n",
    "    outputFolderNameMerge+=str(eventNumberMax)\n",
    "    #\n",
    "    if not os.path.exists(outputFolderNameMerge):\n",
    "        os.makedirs(outputFolderNameMerge)\n",
    "        \n",
    "    eventNumber=\"all\"\n",
    "    \n",
    "    # Input\n",
    "    fileNameNNInputTrainAll=outputFolderNameMerge+\"/NN_2_data_Input_Train_\"+eventNumber+\".npy\"\n",
    "    fileNameNNInputTestAll=outputFolderNameMerge+\"/NN_2_data_Input_Test_\"+eventNumber+\".npy\"\n",
    "    nparray_Input_Train_all_Events=np.load(fileNameNNInputTrainAll)\n",
    "    nparray_Input_Test_all_Events=np.load(fileNameNNInputTestAll)\n",
    "    if debug:\n",
    "        print()\n",
    "        p(\"nparray_Input_Train_all_Events\",nparray_Input_Train_all_Events)\n",
    "        p(\"nparray_Input_Test_all_Events\",nparray_Input_Test_all_Events) \n",
    "    \n",
    "    # Output\n",
    "    fileNameNNOutputTrainAll=outputFolderNameMerge+\"/NN_2_data_Output_Train_\"+eventNumber+\".npy\"\n",
    "    fileNameNNOutputTestAll=outputFolderNameMerge+\"/NN_2_data_Output_Test_\"+eventNumber+\".npy\"\n",
    "    nparray_Output_Train_all_Events=np.load(fileNameNNOutputTrainAll)\n",
    "    nparray_Output_Test_all_Events=np.load(fileNameNNOutputTestAll)\n",
    "    if debug:\n",
    "        print()\n",
    "        p(\"nparray_Output_Train_all_Events\",nparray_Output_Train_all_Events)\n",
    "        p(\"nparray_Output_Test_all_Events\",nparray_Output_Test_all_Events)  \n",
    "    \n",
    "    # NbPositiveHit\n",
    "    fileNameNNNbPositiveHitTrainAll=outputFolderNameMerge+\"/NN_2_data_NbPositiveHit_Train_\"+eventNumber+\".npy\"\n",
    "    fileNameNNNbPositiveHitTestAll=outputFolderNameMerge+\"/NN_2_data_NbPositiveHit_Test_\"+eventNumber+\".npy\"\n",
    "    nparray_NbPositiveHit_Train_all_Events=np.load(fileNameNNNbPositiveHitTrainAll)\n",
    "    nparray_NbPositiveHit_Test_all_Events=np.load(fileNameNNNbPositiveHitTestAll)\n",
    "    if debug:\n",
    "        print()\n",
    "        p(\"nparray_NbPositiveHit_Train_all_Events\",nparray_NbPositiveHit_Train_all_Events)\n",
    "        p(\"nparray_NbPositiveHit_Test_all_Events\",nparray_NbPositiveHit_Test_all_Events)\n",
    "        \n",
    "    # Eta\n",
    "    fileNameNNEtaTrainAll=outputFolderNameMerge+\"/NN_2_data_Eta_Train_\"+eventNumber+\".npy\"\n",
    "    fileNameNNEtaTestAll=outputFolderNameMerge+\"/NN_2_data_Eta_Test_\"+eventNumber+\".npy\"\n",
    "    nparray_Eta_Train_all_Events=np.load(fileNameNNEtaTrainAll)\n",
    "    nparray_Eta_Test_all_Events=np.load(fileNameNNEtaTestAll)\n",
    "    if debug:\n",
    "        print()\n",
    "        p(\"nparray_Eta_Train_all_Events\",nparray_Eta_Train_all_Events)\n",
    "        p(\"nparray_Eta_Test_all_Events\",nparray_Eta_Test_all_Events)\n",
    "        \n",
    "    # LayerID\n",
    "    fileNameNNLayerIDTrainAll=outputFolderNameMerge+\"/NN_2_data_LayerID_Train_\"+eventNumber+\".npy\"\n",
    "    fileNameNNLayerIDTestAll=outputFolderNameMerge+\"/NN_2_data_LayerID_Test_\"+eventNumber+\".npy\"\n",
    "    nparray_LayerID_Train_all_Events=np.load(fileNameNNLayerIDTrainAll)\n",
    "    nparray_LayerID_Test_all_Events=np.load(fileNameNNLayerIDTestAll)\n",
    "    if debug:\n",
    "        print()\n",
    "        p(\"nparray_LayerID_Train_all_Events\",nparray_LayerID_Train_all_Events)\n",
    "        p(\"nparray_LayerID_Test_all_Events\",nparray_LayerID_Test_all_Events)\n",
    "        \n",
    "    # VolumeID\n",
    "    fileNameNNVolumeIDTrainAll=outputFolderNameMerge+\"/NN_2_data_VolumeID_Train_\"+eventNumber+\".npy\"\n",
    "    fileNameNNVolumeIDTestAll=outputFolderNameMerge+\"/NN_2_data_VolumeID_Test_\"+eventNumber+\".npy\"\n",
    "    nparray_VolumeID_Train_all_Events=np.load(fileNameNNVolumeIDTrainAll)\n",
    "    nparray_VolumeID_Test_all_Events=np.load(fileNameNNVolumeIDTestAll)\n",
    "    if debug:\n",
    "        print()\n",
    "        p(\"nparray_VolumeID_Train_all_Events\",nparray_VolumeID_Train_all_Events)\n",
    "        p(\"nparray_VolumeID_Test_all_Events\",nparray_VolumeID_Test_all_Events)\n",
    "        \n",
    "    # QueryIndex\n",
    "    fileNameNNQueryIndexTrainAll=outputFolderNameMerge+\"/NN_2_data_QueryIndex_Train_\"+eventNumber+\".npy\"\n",
    "    fileNameNNQueryIndexTestAll=outputFolderNameMerge+\"/NN_2_data_QueryIndex_Test_\"+eventNumber+\".npy\"\n",
    "    nparray_QueryIndex_Train_all_Events=np.load(fileNameNNQueryIndexTrainAll)\n",
    "    nparray_QueryIndex_Test_all_Events=np.load(fileNameNNQueryIndexTestAll)\n",
    "    if debug:\n",
    "        print()\n",
    "        p(\"nparray_QueryIndex_Train_all_Events\",nparray_QueryIndex_Train_all_Events)\n",
    "        p(\"nparray_QueryIndex_Test_all_Events\",nparray_QueryIndex_Test_all_Events)\n",
    "    \n",
    "    # for Train and Test\n",
    "    if True:\n",
    "        do_balance(\"Train\",nparray_Input_Train_all_Events,nparray_Output_Train_all_Events,nparray_NbPositiveHit_Train_all_Events,nparray_Eta_Train_all_Events,nparray_LayerID_Train_all_Events,nparray_VolumeID_Train_all_Events,nparray_QueryIndex_Train_all_Events,outputFolderNameMerge,doVerifyForHit=True,debug=debug,verbose=True)\n",
    "        do_balance(\"Test\",nparray_Input_Test_all_Events,nparray_Output_Test_all_Events,nparray_NbPositiveHit_Test_all_Events,nparray_Eta_Test_all_Events,nparray_LayerID_Test_all_Events,nparray_VolumeID_Test_all_Events,nparray_QueryIndex_Test_all_Events,outputFolderNameMerge,doVerifyForHit=True,debug=debug,verbose=True)\n",
    "    # weights ready\n",
    "    \n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "******************************\n",
      "mergeOption [0, 30]\n",
      "******************************\n",
      "do_balance_mergeOption(), with mergeOption= [0, 30]\n",
      "Start get_nparray_weight for TrainOrTest= Train\n",
      "Start nparray_input\n",
      "[[[ 3.68880e+01]\n",
      "  [ 1.38507e+00]\n",
      "  [-1.29750e+03]\n",
      "  ...\n",
      "  [ 3.74781e+01]\n",
      "  [-1.87127e+00]\n",
      "  [-1.29750e+03]]\n",
      "\n",
      " [[ 4.72534e+01]\n",
      "  [ 3.84431e+00]\n",
      "  [-1.50250e+03]\n",
      "  ...\n",
      "  [ 3.99709e+01]\n",
      "  [ 5.32750e+00]\n",
      "  [-1.30250e+03]]\n",
      "\n",
      " [[ 7.07738e+01]\n",
      "  [ 8.25329e+00]\n",
      "  [-1.30250e+03]\n",
      "  ...\n",
      "  [ 4.35503e+01]\n",
      "  [ 7.16400e+00]\n",
      "  [-8.22500e+02]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-9.78331e+02]\n",
      "  [ 1.20597e+02]\n",
      "  [ 2.95550e+03]\n",
      "  ...\n",
      "  [-4.93845e+02]\n",
      "  [ 7.48192e+01]\n",
      "  [ 1.50450e+03]]\n",
      "\n",
      " [[-8.33462e+02]\n",
      "  [ 1.02791e+02]\n",
      "  [ 2.54450e+03]\n",
      "  ...\n",
      "  [-4.93845e+02]\n",
      "  [ 7.48192e+01]\n",
      "  [ 1.50450e+03]]\n",
      "\n",
      " [[-9.57845e+02]\n",
      "  [ 1.09019e+02]\n",
      "  [ 2.95250e+03]\n",
      "  ...\n",
      "  [-2.58630e+02]\n",
      "  [ 2.93903e+01]\n",
      "  [ 8.29133e+02]]]\n",
      "End nparray_input shape (2290382, 60, 1) dtype float32 type <class 'numpy.ndarray'>\n",
      "Start nparray_output\n",
      "[[ 1  1  1 ... -1 -1 -1]\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " ...\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " [-1 -1 -1 ...  1  1 -1]]\n",
      "End nparray_output shape (2290382, 20) dtype int8 type <class 'numpy.ndarray'>\n",
      "Start nparray_nbPositiveHit\n",
      "[[ 6]\n",
      " [ 8]\n",
      " [ 9]\n",
      " ...\n",
      " [12]\n",
      " [10]\n",
      " [10]]\n",
      "End nparray_nbPositiveHit shape (2290382, 1) dtype uint8 type <class 'numpy.ndarray'>\n",
      "Start nparray_eta\n",
      "[[-4.2717023 -4.2717023 -4.2717023 ... -4.2464147 -4.2272186 -4.2272186]\n",
      " [-4.244235  -4.244235  -4.244235  ... -4.1417737 -4.1417737 -4.1417737]\n",
      " [-3.5683124 -3.5683124 -3.5683124 ... -3.5681052 -3.6279647 -3.6279647]\n",
      " ...\n",
      " [ 1.8253161  1.8253161  1.8253161 ...  1.8180457  1.8493754  1.8493754]\n",
      " [ 1.8253161  1.8253161  1.8253161 ...  1.8493754  1.5347885  1.8493754]\n",
      " [ 1.8180457  1.8180457  1.8180457 ...  1.8253161  1.8253161  1.8180457]]\n",
      "End nparray_eta shape (2290382, 20) dtype float32 type <class 'numpy.ndarray'>\n",
      "Start nparray_layerID\n",
      "[[ 4  2  6 ...  6  6  4]\n",
      " [ 2  4  4 ...  6  4  4]\n",
      " [ 4  2  6 ...  2 10 10]\n",
      " ...\n",
      " [12  2  2 ...  6  2  4]\n",
      " [10 12  4 ... 10  6  4]\n",
      " [12  8 10 ...  6  6  2]]\n",
      "End nparray_layerID shape (2290382, 20) dtype uint8 type <class 'numpy.ndarray'>\n",
      "Start nparray_volumeID\n",
      "[[ 7  7  7 ...  7  7  7]\n",
      " [ 7  7  7 ...  7  7  7]\n",
      " [ 7  7  7 ...  7  7  7]\n",
      " ...\n",
      " [18 14 13 ... 14 14 14]\n",
      " [18 18 14 ... 18  8 14]\n",
      " [18 14 18 ...  8  8 13]]\n",
      "End nparray_volumeID shape (2290382, 20) dtype uint8 type <class 'numpy.ndarray'>\n",
      "Start nparray_queryIndex\n",
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "End nparray_queryIndex shape (2290382, 1) dtype uint8 type <class 'numpy.ndarray'>\n",
      "\n",
      "\n",
      "Train:\n",
      "nbBucket 2290382 bucketSize 20 nrHitTotal 45807640\n",
      "\n",
      "Original unbalanced. Train:\n",
      "nbPositiveHit=0 counterBucket=0 percentBucket=0.0\n",
      "nbPositiveHit=1 counterBucket=0 percentBucket=0.0\n",
      "nbPositiveHit=2 counterBucket=3973 percentBucket=0.2\n",
      "nbPositiveHit=3 counterBucket=33349 percentBucket=1.5\n",
      "nbPositiveHit=4 counterBucket=115266 percentBucket=5.0\n",
      "nbPositiveHit=5 counterBucket=186516 percentBucket=8.1\n",
      "nbPositiveHit=6 counterBucket=270380 percentBucket=11.8\n",
      "nbPositiveHit=7 counterBucket=294847 percentBucket=12.9\n",
      "nbPositiveHit=8 counterBucket=298765 percentBucket=13.0\n",
      "nbPositiveHit=9 counterBucket=260813 percentBucket=11.4\n",
      "nbPositiveHit=10 counterBucket=262604 percentBucket=11.5\n",
      "nbPositiveHit=11 counterBucket=194065 percentBucket=8.5\n",
      "nbPositiveHit=12 counterBucket=157420 percentBucket=6.9\n",
      "nbPositiveHit=13 counterBucket=99938 percentBucket=4.4\n",
      "nbPositiveHit=14 counterBucket=66595 percentBucket=2.9\n",
      "nbPositiveHit=15 counterBucket=29752 percentBucket=1.3\n",
      "nbPositiveHit=16 counterBucket=10828 percentBucket=0.5\n",
      "nbPositiveHit=17 counterBucket=3184 percentBucket=0.1\n",
      "nbPositiveHit=18 counterBucket=1560 percentBucket=0.1\n",
      "nbPositiveHit=19 counterBucket=379 percentBucket=0.0\n",
      "nbPositiveHit=20 counterBucket=148 percentBucket=0.0\n",
      "bins [-0.5, 0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5, 10.5, 11.5, 12.5, 13.5, 14.5, 15.5, 16.5, 17.5, 18.5, 19.5, 20.5]\n",
      "\n",
      "BalancedDesired. Train:\n",
      "nbPositiveHit=0 counterBucket=0 percentBucket=0.0\n",
      "nbPositiveHit=1 counterBucket=0 percentBucket=0.0\n",
      "nbPositiveHit=2 counterBucket=1560 percentBucket=0.1\n",
      "nbPositiveHit=3 counterBucket=3184 percentBucket=0.1\n",
      "nbPositiveHit=4 counterBucket=10828 percentBucket=0.5\n",
      "nbPositiveHit=5 counterBucket=29752 percentBucket=1.3\n",
      "nbPositiveHit=6 counterBucket=66595 percentBucket=2.9\n",
      "nbPositiveHit=7 counterBucket=99938 percentBucket=4.4\n",
      "nbPositiveHit=8 counterBucket=157420 percentBucket=6.9\n",
      "nbPositiveHit=9 counterBucket=194065 percentBucket=8.5\n",
      "nbPositiveHit=10 counterBucket=262604 percentBucket=11.5\n",
      "nbPositiveHit=11 counterBucket=194065 percentBucket=8.5\n",
      "nbPositiveHit=12 counterBucket=157420 percentBucket=6.9\n",
      "nbPositiveHit=13 counterBucket=99938 percentBucket=4.4\n",
      "nbPositiveHit=14 counterBucket=66595 percentBucket=2.9\n",
      "nbPositiveHit=15 counterBucket=29752 percentBucket=1.3\n",
      "nbPositiveHit=16 counterBucket=10828 percentBucket=0.5\n",
      "nbPositiveHit=17 counterBucket=3184 percentBucket=0.1\n",
      "nbPositiveHit=18 counterBucket=1560 percentBucket=0.1\n",
      "nbPositiveHit=19 counterBucket=0 percentBucket=0.0\n",
      "nbPositiveHit=20 counterBucket=0 percentBucket=0.0\n",
      "\n",
      "\n",
      "BalancedObtained after. Train:\n",
      "nbPositiveHit=0 counterBucket=0 percentBucket=0.0\n",
      "nbPositiveHit=1 counterBucket=0 percentBucket=0.0\n",
      "nbPositiveHit=2 counterBucket=1560 percentBucket=0.1\n",
      "nbPositiveHit=3 counterBucket=3184 percentBucket=0.1\n",
      "nbPositiveHit=4 counterBucket=10828 percentBucket=0.5\n",
      "nbPositiveHit=5 counterBucket=29752 percentBucket=1.3\n",
      "nbPositiveHit=6 counterBucket=66595 percentBucket=2.9\n",
      "nbPositiveHit=7 counterBucket=99938 percentBucket=4.4\n",
      "nbPositiveHit=8 counterBucket=157420 percentBucket=6.9\n",
      "nbPositiveHit=9 counterBucket=194065 percentBucket=8.5\n",
      "nbPositiveHit=10 counterBucket=262604 percentBucket=11.5\n",
      "nbPositiveHit=11 counterBucket=194065 percentBucket=8.5\n",
      "nbPositiveHit=12 counterBucket=157420 percentBucket=6.9\n",
      "nbPositiveHit=13 counterBucket=99938 percentBucket=4.4\n",
      "nbPositiveHit=14 counterBucket=66595 percentBucket=2.9\n",
      "nbPositiveHit=15 counterBucket=29752 percentBucket=1.3\n",
      "nbPositiveHit=16 counterBucket=10828 percentBucket=0.5\n",
      "nbPositiveHit=17 counterBucket=3184 percentBucket=0.1\n",
      "nbPositiveHit=18 counterBucket=1560 percentBucket=0.1\n",
      "nbPositiveHit=19 counterBucket=0 percentBucket=0.0\n",
      "nbPositiveHit=20 counterBucket=0 percentBucket=0.0\n",
      "Start nparray_inputBalanced\n",
      "[[[ 3.68880e+01]\n",
      "  [ 1.38507e+00]\n",
      "  [-1.29750e+03]\n",
      "  ...\n",
      "  [ 3.74781e+01]\n",
      "  [-1.87127e+00]\n",
      "  [-1.29750e+03]]\n",
      "\n",
      " [[ 4.72534e+01]\n",
      "  [ 3.84431e+00]\n",
      "  [-1.50250e+03]\n",
      "  ...\n",
      "  [ 3.99709e+01]\n",
      "  [ 5.32750e+00]\n",
      "  [-1.30250e+03]]\n",
      "\n",
      " [[ 7.07738e+01]\n",
      "  [ 8.25329e+00]\n",
      "  [-1.30250e+03]\n",
      "  ...\n",
      "  [ 4.35503e+01]\n",
      "  [ 7.16400e+00]\n",
      "  [-8.22500e+02]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-9.78331e+02]\n",
      "  [ 1.20597e+02]\n",
      "  [ 2.95550e+03]\n",
      "  ...\n",
      "  [-4.93845e+02]\n",
      "  [ 7.48192e+01]\n",
      "  [ 1.50450e+03]]\n",
      "\n",
      " [[-8.33462e+02]\n",
      "  [ 1.02791e+02]\n",
      "  [ 2.54450e+03]\n",
      "  ...\n",
      "  [-4.93845e+02]\n",
      "  [ 7.48192e+01]\n",
      "  [ 1.50450e+03]]\n",
      "\n",
      " [[-9.57845e+02]\n",
      "  [ 1.09019e+02]\n",
      "  [ 2.95250e+03]\n",
      "  ...\n",
      "  [-2.58630e+02]\n",
      "  [ 2.93903e+01]\n",
      "  [ 8.29133e+02]]]\n",
      "End nparray_inputBalanced shape (1389288, 60, 1) dtype float32 type <class 'numpy.ndarray'>\n",
      "Start nparray_outputBalanced\n",
      "[[ 1  1  1 ... -1 -1 -1]\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " ...\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " [-1 -1 -1 ...  1  1 -1]]\n",
      "End nparray_outputBalanced shape (1389288, 20) dtype int8 type <class 'numpy.ndarray'>\n",
      "Start nparray_nbPositiveHitBalanced\n",
      "[[ 6]\n",
      " [ 8]\n",
      " [ 9]\n",
      " ...\n",
      " [12]\n",
      " [10]\n",
      " [10]]\n",
      "End nparray_nbPositiveHitBalanced shape (1389288, 1) dtype uint8 type <class 'numpy.ndarray'>\n",
      "Start nparray_etaBalanced\n",
      "[[-4.2717023 -4.2717023 -4.2717023 ... -4.2464147 -4.2272186 -4.2272186]\n",
      " [-4.244235  -4.244235  -4.244235  ... -4.1417737 -4.1417737 -4.1417737]\n",
      " [-3.5683124 -3.5683124 -3.5683124 ... -3.5681052 -3.6279647 -3.6279647]\n",
      " ...\n",
      " [ 1.8253161  1.8253161  1.8253161 ...  1.8180457  1.8493754  1.8493754]\n",
      " [ 1.8253161  1.8253161  1.8253161 ...  1.8493754  1.5347885  1.8493754]\n",
      " [ 1.8180457  1.8180457  1.8180457 ...  1.8253161  1.8253161  1.8180457]]\n",
      "End nparray_etaBalanced shape (1389288, 20) dtype float32 type <class 'numpy.ndarray'>\n",
      "Start nparray_layerIDBalanced\n",
      "[[ 4  2  6 ...  6  6  4]\n",
      " [ 2  4  4 ...  6  4  4]\n",
      " [ 4  2  6 ...  2 10 10]\n",
      " ...\n",
      " [12  2  2 ...  6  2  4]\n",
      " [10 12  4 ... 10  6  4]\n",
      " [12  8 10 ...  6  6  2]]\n",
      "End nparray_layerIDBalanced shape (1389288, 20) dtype uint8 type <class 'numpy.ndarray'>\n",
      "Start nparray_volumeIDBalanced\n",
      "[[ 7  7  7 ...  7  7  7]\n",
      " [ 7  7  7 ...  7  7  7]\n",
      " [ 7  7  7 ...  7  7  7]\n",
      " ...\n",
      " [18 14 13 ... 14 14 14]\n",
      " [18 18 14 ... 18  8 14]\n",
      " [18 14 18 ...  8  8 13]]\n",
      "End nparray_volumeIDBalanced shape (1389288, 20) dtype uint8 type <class 'numpy.ndarray'>\n",
      "Start nparray_queryIndexBalanced\n",
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "End nparray_queryIndexBalanced shape (1389288, 1) dtype uint8 type <class 'numpy.ndarray'>\n",
      "bins [-0.5, 0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5, 10.5, 11.5, 12.5, 13.5, 14.5, 15.5, 16.5, 17.5, 18.5, 19.5, 20.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start get_nparray_weight for TrainOrTest= Test\n",
      "Start nparray_input\n",
      "[[[ 6.10612e+01]\n",
      "  [ 5.48271e+00]\n",
      "  [-1.30250e+03]\n",
      "  ...\n",
      "  [ 3.11451e+01]\n",
      "  [ 3.69277e+00]\n",
      "  [-6.97500e+02]]\n",
      "\n",
      " [[ 5.66778e+01]\n",
      "  [ 6.69252e+00]\n",
      "  [-1.30250e+03]\n",
      "  ...\n",
      "  [ 5.79303e+01]\n",
      "  [ 4.73475e+00]\n",
      "  [-1.30250e+03]]\n",
      "\n",
      " [[ 3.15741e+01]\n",
      "  [ 3.40823e+00]\n",
      "  [-1.49750e+03]\n",
      "  ...\n",
      "  [ 3.07265e+01]\n",
      "  [-2.34300e+00]\n",
      "  [-1.49750e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-8.80046e+02]\n",
      "  [ 2.05591e+01]\n",
      "  [ 2.95250e+03]\n",
      "  ...\n",
      "  [-2.59735e+02]\n",
      "  [-7.12118e+00]\n",
      "  [ 8.48600e+02]]\n",
      "\n",
      " [[-9.59013e+02]\n",
      "  [ 9.04554e+01]\n",
      "  [ 2.95250e+03]\n",
      "  ...\n",
      "  [-1.14631e+02]\n",
      "  [ 1.40400e+01]\n",
      "  [ 3.76046e+02]]\n",
      "\n",
      " [[-9.88007e+02]\n",
      "  [ 1.25375e+02]\n",
      "  [ 2.95550e+03]\n",
      "  ...\n",
      "  [-2.57832e+02]\n",
      "  [ 3.28188e+01]\n",
      "  [ 8.18604e+02]]]\n",
      "End nparray_input shape (947913, 60, 1) dtype float32 type <class 'numpy.ndarray'>\n",
      "Start nparray_output\n",
      "[[ 1  1  1 ... -1 -1 -1]\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " [-1 -1 -1 ...  1 -1 -1]\n",
      " ...\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " [-1 -1 -1 ... -1 -1 -1]\n",
      " [ 1  1  1 ... -1 -1  1]]\n",
      "End nparray_output shape (947913, 20) dtype int8 type <class 'numpy.ndarray'>\n",
      "Start nparray_nbPositiveHit\n",
      "[[9]\n",
      " [9]\n",
      " [6]\n",
      " ...\n",
      " [6]\n",
      " [8]\n",
      " [8]]\n",
      "End nparray_nbPositiveHit shape (947913, 1) dtype uint8 type <class 'numpy.ndarray'>\n",
      "Start nparray_eta\n",
      "[[-3.7804482 -3.7804482 -3.7804482 ... -3.7871683 -3.7871683 -3.851502 ]\n",
      " [-3.851502  -3.851502  -3.851502  ... -3.932433  -3.7871683 -3.7871683]\n",
      " [-4.597932  -4.597932  -4.5397544 ... -4.503565  -4.51713   -4.5574536]\n",
      " ...\n",
      " [ 1.8658503  1.8658503  1.8658503 ...  1.8579998  2.0295532  1.8579998]\n",
      " [ 1.8843968  1.8843968  1.8843968 ...  1.8843968  1.7174811  1.7174811]\n",
      " [ 1.7822868  1.7822868  1.7822868 ...  1.7682571  1.7206506  1.7822868]]\n",
      "End nparray_eta shape (947913, 20) dtype float32 type <class 'numpy.ndarray'>\n",
      "Start nparray_layerID\n",
      "[[ 4  2  6 ...  2  4 12]\n",
      " [ 4  6  8 ... 10  6  4]\n",
      " [ 2  2  2 ...  4  2  2]\n",
      " ...\n",
      " [12 12 10 ...  6  2  2]\n",
      " [12 10  6 ...  2  6  6]\n",
      " [12 12 10 ... 10  2  2]]\n",
      "End nparray_layerID shape (947913, 20) dtype uint8 type <class 'numpy.ndarray'>\n",
      "Start nparray_volumeID\n",
      "[[ 7  7  7 ...  7  7  7]\n",
      " [ 7  7  7 ...  7  7  7]\n",
      " [ 7  7  7 ...  7  7  7]\n",
      " ...\n",
      " [18 18 18 ...  8 13 13]\n",
      " [18 18 14 ... 14  8  8]\n",
      " [18 18 18 ... 18 13 13]]\n",
      "End nparray_volumeID shape (947913, 20) dtype uint8 type <class 'numpy.ndarray'>\n",
      "Start nparray_queryIndex\n",
      "[[1]\n",
      " [3]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "End nparray_queryIndex shape (947913, 1) dtype uint8 type <class 'numpy.ndarray'>\n",
      "\n",
      "\n",
      "Test:\n",
      "nbBucket 947913 bucketSize 20 nrHitTotal 18958260\n",
      "\n",
      "Original unbalanced. Test:\n",
      "nbPositiveHit=0 counterBucket=0 percentBucket=0.0\n",
      "nbPositiveHit=1 counterBucket=0 percentBucket=0.0\n",
      "nbPositiveHit=2 counterBucket=1383 percentBucket=0.1\n",
      "nbPositiveHit=3 counterBucket=12201 percentBucket=1.3\n",
      "nbPositiveHit=4 counterBucket=43997 percentBucket=4.6\n",
      "nbPositiveHit=5 counterBucket=73092 percentBucket=7.7\n",
      "nbPositiveHit=6 counterBucket=107498 percentBucket=11.3\n",
      "nbPositiveHit=7 counterBucket=117346 percentBucket=12.4\n",
      "nbPositiveHit=8 counterBucket=122522 percentBucket=12.9\n",
      "nbPositiveHit=9 counterBucket=108282 percentBucket=11.4\n",
      "nbPositiveHit=10 counterBucket=112926 percentBucket=11.9\n",
      "nbPositiveHit=11 counterBucket=85146 percentBucket=9.0\n",
      "nbPositiveHit=12 counterBucket=69333 percentBucket=7.3\n",
      "nbPositiveHit=13 counterBucket=43918 percentBucket=4.6\n",
      "nbPositiveHit=14 counterBucket=29392 percentBucket=3.1\n",
      "nbPositiveHit=15 counterBucket=13665 percentBucket=1.4\n",
      "nbPositiveHit=16 counterBucket=4570 percentBucket=0.5\n",
      "nbPositiveHit=17 counterBucket=1578 percentBucket=0.2\n",
      "nbPositiveHit=18 counterBucket=722 percentBucket=0.1\n",
      "nbPositiveHit=19 counterBucket=220 percentBucket=0.0\n",
      "nbPositiveHit=20 counterBucket=122 percentBucket=0.0\n",
      "bins [-0.5, 0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5, 10.5, 11.5, 12.5, 13.5, 14.5, 15.5, 16.5, 17.5, 18.5, 19.5, 20.5]\n",
      "\n",
      "BalancedDesired. Test:\n",
      "nbPositiveHit=0 counterBucket=0 percentBucket=0.0\n",
      "nbPositiveHit=1 counterBucket=0 percentBucket=0.0\n",
      "nbPositiveHit=2 counterBucket=722 percentBucket=0.1\n",
      "nbPositiveHit=3 counterBucket=1578 percentBucket=0.2\n",
      "nbPositiveHit=4 counterBucket=4570 percentBucket=0.5\n",
      "nbPositiveHit=5 counterBucket=13665 percentBucket=1.4\n",
      "nbPositiveHit=6 counterBucket=29392 percentBucket=3.1\n",
      "nbPositiveHit=7 counterBucket=43918 percentBucket=4.6\n",
      "nbPositiveHit=8 counterBucket=69333 percentBucket=7.3\n",
      "nbPositiveHit=9 counterBucket=85146 percentBucket=9.0\n",
      "nbPositiveHit=10 counterBucket=112926 percentBucket=11.9\n",
      "nbPositiveHit=11 counterBucket=85146 percentBucket=9.0\n",
      "nbPositiveHit=12 counterBucket=69333 percentBucket=7.3\n",
      "nbPositiveHit=13 counterBucket=43918 percentBucket=4.6\n",
      "nbPositiveHit=14 counterBucket=29392 percentBucket=3.1\n",
      "nbPositiveHit=15 counterBucket=13665 percentBucket=1.4\n",
      "nbPositiveHit=16 counterBucket=4570 percentBucket=0.5\n",
      "nbPositiveHit=17 counterBucket=1578 percentBucket=0.2\n",
      "nbPositiveHit=18 counterBucket=722 percentBucket=0.1\n",
      "nbPositiveHit=19 counterBucket=0 percentBucket=0.0\n",
      "nbPositiveHit=20 counterBucket=0 percentBucket=0.0\n",
      "\n",
      "\n",
      "BalancedObtained after. Test:\n",
      "nbPositiveHit=0 counterBucket=0 percentBucket=0.0\n",
      "nbPositiveHit=1 counterBucket=0 percentBucket=0.0\n",
      "nbPositiveHit=2 counterBucket=722 percentBucket=0.1\n",
      "nbPositiveHit=3 counterBucket=1578 percentBucket=0.2\n",
      "nbPositiveHit=4 counterBucket=4570 percentBucket=0.5\n",
      "nbPositiveHit=5 counterBucket=13665 percentBucket=1.4\n",
      "nbPositiveHit=6 counterBucket=29392 percentBucket=3.1\n",
      "nbPositiveHit=7 counterBucket=43918 percentBucket=4.6\n",
      "nbPositiveHit=8 counterBucket=69333 percentBucket=7.3\n",
      "nbPositiveHit=9 counterBucket=85146 percentBucket=9.0\n",
      "nbPositiveHit=10 counterBucket=112926 percentBucket=11.9\n",
      "nbPositiveHit=11 counterBucket=85146 percentBucket=9.0\n",
      "nbPositiveHit=12 counterBucket=69333 percentBucket=7.3\n",
      "nbPositiveHit=13 counterBucket=43918 percentBucket=4.6\n",
      "nbPositiveHit=14 counterBucket=29392 percentBucket=3.1\n",
      "nbPositiveHit=15 counterBucket=13665 percentBucket=1.4\n",
      "nbPositiveHit=16 counterBucket=4570 percentBucket=0.5\n",
      "nbPositiveHit=17 counterBucket=1578 percentBucket=0.2\n",
      "nbPositiveHit=18 counterBucket=722 percentBucket=0.1\n",
      "nbPositiveHit=19 counterBucket=0 percentBucket=0.0\n",
      "nbPositiveHit=20 counterBucket=0 percentBucket=0.0\n",
      "Start nparray_inputBalanced\n",
      "[[[ 6.10612e+01]\n",
      "  [ 5.48271e+00]\n",
      "  [-1.30250e+03]\n",
      "  ...\n",
      "  [ 3.11451e+01]\n",
      "  [ 3.69277e+00]\n",
      "  [-6.97500e+02]]\n",
      "\n",
      " [[ 5.66778e+01]\n",
      "  [ 6.69252e+00]\n",
      "  [-1.30250e+03]\n",
      "  ...\n",
      "  [ 5.79303e+01]\n",
      "  [ 4.73475e+00]\n",
      "  [-1.30250e+03]]\n",
      "\n",
      " [[ 3.15741e+01]\n",
      "  [ 3.40823e+00]\n",
      "  [-1.49750e+03]\n",
      "  ...\n",
      "  [ 3.07265e+01]\n",
      "  [-2.34300e+00]\n",
      "  [-1.49750e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-7.19491e+02]\n",
      "  [ 5.07839e+02]\n",
      "  [ 2.95250e+03]\n",
      "  ...\n",
      "  [-2.70072e+01]\n",
      "  [ 1.75180e+01]\n",
      "  [ 1.12759e+02]]\n",
      "\n",
      " [[-8.31373e+02]\n",
      "  [ 4.45656e+02]\n",
      "  [ 2.95550e+03]\n",
      "  ...\n",
      "  [-6.36897e+01]\n",
      "  [ 3.41380e+01]\n",
      "  [ 2.14496e+02]]\n",
      "\n",
      " [[-7.32986e+02]\n",
      "  [ 2.54835e+02]\n",
      "  [ 2.54450e+03]\n",
      "  ...\n",
      "  [-3.12371e+01]\n",
      "  [ 1.32627e+01]\n",
      "  [ 1.10693e+02]]]\n",
      "End nparray_inputBalanced shape (609574, 60, 1) dtype float32 type <class 'numpy.ndarray'>\n",
      "Start nparray_outputBalanced\n",
      "[[ 1  1  1 ... -1 -1 -1]\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " [-1 -1 -1 ...  1 -1 -1]\n",
      " ...\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " [ 1  1  1 ... -1 -1  1]\n",
      " [ 1  1  1 ... -1 -1 -1]]\n",
      "End nparray_outputBalanced shape (609574, 20) dtype int8 type <class 'numpy.ndarray'>\n",
      "Start nparray_nbPositiveHitBalanced\n",
      "[[ 9]\n",
      " [ 9]\n",
      " [ 6]\n",
      " ...\n",
      " [10]\n",
      " [11]\n",
      " [11]]\n",
      "End nparray_nbPositiveHitBalanced shape (609574, 1) dtype uint8 type <class 'numpy.ndarray'>\n",
      "Start nparray_etaBalanced\n",
      "[[-3.7804482 -3.7804482 -3.7804482 ... -3.7871683 -3.7871683 -3.851502 ]\n",
      " [-3.851502  -3.851502  -3.851502  ... -3.932433  -3.7871683 -3.7871683]\n",
      " [-4.597932  -4.597932  -4.5397544 ... -4.503565  -4.51713   -4.5574536]\n",
      " ...\n",
      " [ 1.9307309  1.9307309  1.9307309 ...  1.8838754  1.8838754  2.0236633]\n",
      " [ 1.8652344  1.8652344  1.8652344 ...  1.8565259  1.8565259  1.8652344]\n",
      " [ 1.9052281  1.9052281  1.9052281 ...  1.8355883  1.8355883  1.5009832]]\n",
      "End nparray_etaBalanced shape (609574, 20) dtype float32 type <class 'numpy.ndarray'>\n",
      "Start nparray_layerIDBalanced\n",
      "[[ 4  2  6 ...  2  4 12]\n",
      " [ 4  6  8 ... 10  6  4]\n",
      " [ 2  2  2 ...  4  2  2]\n",
      " ...\n",
      " [12 12  8 ...  6  4  2]\n",
      " [12  6  8 ...  4  2  4]\n",
      " [10 12  8 ...  2  4  2]]\n",
      "End nparray_layerIDBalanced shape (609574, 20) dtype uint8 type <class 'numpy.ndarray'>\n",
      "Start nparray_volumeIDBalanced\n",
      "[[ 7  7  7 ...  7  7  7]\n",
      " [ 7  7  7 ...  7  7  7]\n",
      " [ 7  7  7 ...  7  7  7]\n",
      " ...\n",
      " [18 18 14 ... 14 14  8]\n",
      " [18 14 14 ... 14 13  8]\n",
      " [18 18 14 ... 14 14  8]]\n",
      "End nparray_volumeIDBalanced shape (609574, 20) dtype uint8 type <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start nparray_queryIndexBalanced\n",
      "[[1]\n",
      " [3]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "End nparray_queryIndexBalanced shape (609574, 1) dtype uint8 type <class 'numpy.ndarray'>\n",
      "bins [-0.5, 0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5, 10.5, 11.5, 12.5, 13.5, 14.5, 15.5, 16.5, 17.5, 18.5, 19.5, 20.5]\n"
     ]
    }
   ],
   "source": [
    "if doBalance:\n",
    "    for mergeOption in list_mergeOption:\n",
    "        print()\n",
    "        print()\n",
    "        print(\"******************************\")\n",
    "        print(\"mergeOption\",mergeOption)\n",
    "        print(\"******************************\")\n",
    "        do_balance_mergeOption(mergeOption)\n",
    "        if debug:\n",
    "            print(\"Done mergeOption\",mergeOption)\n",
    "    # done for loop over mergeOption\n",
    "# done if"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done do_create_output_based_on_min_nb_of_positive_hits(list_eventNumber\n"
     ]
    }
   ],
   "source": [
    "def do_process_merged(mergeOption):\n",
    "    \n",
    "    # create output folder name for the merge, for the current number of events to merge\n",
    "    eventNumberMin=mergeOption[0] # included\n",
    "    eventNumberMax=mergeOption[1] # excluded\n",
    "    #\n",
    "    outputFolderNameMerge=outputFolderName+\"_ev_\"\n",
    "    #\n",
    "    if eventNumberMin<10:\n",
    "        outputFolderNameMerge+=\"00\"\n",
    "    elif eventNumberMin<100:\n",
    "        outputFolderNameMerge+=\"0\"\n",
    "    outputFolderNameMerge+=str(eventNumberMin)\n",
    "    #\n",
    "    outputFolderNameMerge+=\"_\"\n",
    "    #\n",
    "    if eventNumberMax<10:\n",
    "        outputFolderNameMerge+=\"00\"\n",
    "    elif eventNumberMax<100:\n",
    "        outputFolderNameMerge+=\"0\"\n",
    "    outputFolderNameMerge+=str(eventNumberMax)\n",
    "    #\n",
    "    if not os.path.exists(outputFolderNameMerge):\n",
    "        os.makedirs(outputFolderNameMerge)\n",
    "    \n",
    "    \n",
    "    # create the numpy array with all negative with which we replace those that need to\n",
    "    nparray_output_negative_one_bucket=np.array([-1 for i in range(bucketSize)]).astype(np.int8)\n",
    "    \n",
    "    # we change outputC for both Train and Test\n",
    "    list_TrainOrTest=[\"Train\",\"Test\"]\n",
    "    \n",
    "    eventNumber=\"all\"\n",
    "    \n",
    "    # for loop over TrainOrTest\n",
    "    for TrainOrTest in list_TrainOrTest:\n",
    "        if debug or verbose:\n",
    "            print(\"TrainOrTest\",TrainOrTest)\n",
    "        # open the files needed for this event: Output and NbPositiveHit\n",
    "        fileNameNNOutputAll=outputFolderNameMerge+\"/NN_2_data_OutputBalanced_\"+TrainOrTest+\"_\"+eventNumber+\".npy\"\n",
    "        nparray_output_all=np.load(fileNameNNOutputAll)\n",
    "        fileNameNNNbPositiveHitAll=outputFolderNameMerge+\"/NN_2_data_NbPositiveHitBalanced_\"+TrainOrTest+\"_\"+eventNumber+\".npy\"\n",
    "        nparray_nbPositiveHit_all=np.load(fileNameNNNbPositiveHitAll)\n",
    "        # \n",
    "        dict_minNbPositiveHit_nparray_output_all={}\n",
    "        # for loop over each val of min number of positive fits\n",
    "        for minNbPositiveHit in list_minNbPositiveHit:\n",
    "            if debug:\n",
    "                print(\"i\",i,\"eventNumber\",eventNumber,\"minNbPositiveHit\",minNbPositiveHit)\n",
    "            # copy the current output in a completely new version with deep copy\n",
    "            # as we will want to replace only the values that need changed for less than needed positive hits\n",
    "            dict_minNbPositiveHit_nparray_output_all[minNbPositiveHit]=copy.deepcopy(nparray_output_all)\n",
    "        # done for loop over minNbPositiveHit\n",
    "        #\n",
    "        # loop over each row (bucket) in the output\n",
    "        for i in range(len(nparray_output_all)):\n",
    "            nbPositiveHit=nparray_nbPositiveHit_all[i][0]\n",
    "            if debug:\n",
    "                print(\"i=%.0f, nbPositiveHit=%.0f\"%(i,nbPositiveHit))\n",
    "            # to speed up, change only wheen needed\n",
    "            for minNbPositiveHit in list_minNbPositiveHit:\n",
    "                if nbPositiveHit<minNbPositiveHit:\n",
    "                    # replace this one line with the one with all negative\n",
    "                    dict_minNbPositiveHit_nparray_output_all[minNbPositiveHit][i]=nparray_output_negative_one_bucket\n",
    "                if debug:\n",
    "                    print(\"i=%.0f, minNbPositiveHit=%.0f, bucket\"%(i,minNbPositiveHit),dict_minNbPositiveHit_nparray_output_all[minNbPositiveHit][j])\n",
    "            # done for loop over minNbPositiveHit  \n",
    "        # done for loop over buckets\n",
    "        # write the new processed output to file\n",
    "        for minNbPositiveHit in list_minNbPositiveHit:\n",
    "            minString=\"Min\"\n",
    "            if minNbPositiveHit<9:\n",
    "                minString+=\"0\"\n",
    "            minString+=str(minNbPositiveHit)\n",
    "            fileNameNNOutputAll=outputFolderNameMerge+\"/NN_2_data_OutputBalanced\"+minString+\"_\"+TrainOrTest+\"_\"+eventNumber+\".npy\"\n",
    "            np.save(fileNameNNOutputAll,dict_minNbPositiveHit_nparray_output_all[minNbPositiveHit])\n",
    "        # done for loop over minNbPositiveHit  \n",
    "    # done for loop over events\n",
    "# done function\n",
    "if debug or verbose:\n",
    "    print(\"Done do_create_output_based_on_min_nb_of_positive_hits(list_eventNumber\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "******************************\n",
      "mergeOption [0, 30]\n",
      "******************************\n",
      "TrainOrTest Train\n",
      "TrainOrTest Test\n"
     ]
    }
   ],
   "source": [
    "if doProcessMerged:\n",
    "    for mergeOption in list_mergeOption:\n",
    "        print()\n",
    "        print()\n",
    "        print(\"******************************\")\n",
    "        print(\"mergeOption\",mergeOption)\n",
    "        print(\"******************************\")\n",
    "        do_process_merged(mergeOption)\n",
    "        if debug:\n",
    "            print(\"Done mergeOption\",mergeOption)\n",
    "    # done for loop over mergeOption\n",
    "# done if"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done all!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
