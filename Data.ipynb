{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug=False\n",
    "verbose=True\n",
    "doBucketPlots=False\n",
    "doIndividual=False\n",
    "doProcessIndividual=False # applies minimum cut for output: 00, 04, 07, 10\n",
    "doMerge=False\n",
    "doBalance=False\n",
    "doProcessBalanced=True # for OutputBalanced do Min04, Min07, etc\n",
    "#\n",
    "list_minNbPositiveHit=[\n",
    "    0,\n",
    "    4,\n",
    "    7,\n",
    "    10,\n",
    "]\n",
    "list_mergeOption=[\n",
    "    #[0,10],\n",
    "    #[10,20],\n",
    "    #[20,30],\n",
    "    #[30,40],\n",
    "    #[40,50],\n",
    "    #[50,60],\n",
    "    #[60,70],\n",
    "    #[70,80],\n",
    "    #[80,90],\n",
    "    #[90,100],\n",
    "    #[0,20],\n",
    "    #[0,30],\n",
    "    #[0,40],\n",
    "    #[0,50],\n",
    "    #[0,60],\n",
    "    #[0,70],\n",
    "    #[0,80],\n",
    "    #[0,90],\n",
    "    [0,100],\n",
    "]\n",
    "bucketSize=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderStem=\"/Volumes/Luiza_SSD\"\n",
    "\n",
    "#\n",
    "inputFolderName=folderStem+\"/ATLAS/TrackML/data/ttbar_mu200-generic\"\n",
    "#outputFolderName=folderStem+\"/ATLAS/TrackML/output_general\"\n",
    "#outputFolderName=folderStem+\"/ATLAS/TrackML/output_again\"\n",
    "outputFolderName=folderStem+\"/ATLAS/TrackML/output_new\"\n",
    "# if output folder does not exist, create it\n",
    "if not os.path.exists(outputFolderName):\n",
    "    os.makedirs(outputFolderName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eventNumber_from_fileName(fileName):\n",
    "    eventNumber=fileName.replace(\"event\",\"\").replace(\"-hits.csv\",\"\")\n",
    "    return eventNumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All events available in my folder. list_eventNumber ['000000000', '000000001', '000000002', '000000003', '000000004', '000000005', '000000006', '000000007', '000000008', '000000009', '000000010', '000000011', '000000012', '000000013', '000000014', '000000015', '000000016', '000000017', '000000018', '000000019', '000000020', '000000021', '000000022', '000000023', '000000024', '000000025', '000000026', '000000027', '000000028', '000000029', '000000030', '000000031', '000000032', '000000033', '000000034', '000000035', '000000036', '000000037', '000000038', '000000039', '000000040', '000000041', '000000042', '000000043', '000000044', '000000045', '000000046', '000000047', '000000048', '000000049', '000000050', '000000051', '000000052', '000000053', '000000054', '000000055', '000000056', '000000057', '000000058', '000000059', '000000060', '000000061', '000000062', '000000063', '000000064', '000000065', '000000066', '000000067', '000000068', '000000069', '000000070', '000000071', '000000072', '000000073', '000000074', '000000075', '000000076', '000000077', '000000078', '000000079', '000000080', '000000081', '000000082', '000000083', '000000084', '000000085', '000000086', '000000087', '000000088', '000000089', '000000090', '000000091', '000000092', '000000093', '000000094', '000000095', '000000096', '000000097', '000000098', '000000099']\n"
     ]
    }
   ],
   "source": [
    "# calculate event numbers from my folder\n",
    "# lista goala in care sa pun numerele evenimentelor\n",
    "list_eventNumber=[]\n",
    "# The sorted() function returns a sorted list of the specified iterable object \n",
    "# Strings are sorted alphabetically, and numbers are sorted numerically\n",
    "# os operating system \n",
    "# hits ca sa nu am de doura ori evenimentul in lista\n",
    "for fileName in sorted(os.listdir(inputFolderName)):\n",
    "    if fileName.endswith(\"-hits.csv\"):\n",
    "        #print(fileName)\n",
    "        eventNumber=get_eventNumber_from_fileName(fileName)\n",
    "        #print(eventNumber)\n",
    "        list_eventNumber.append(eventNumber)\n",
    "# done for loop\n",
    "#list_eventNumber=list_eventNumber[0:20] # keep only first 10 events\n",
    "#list_eventNumber=[\"000000007\"] # keep only one event for test\n",
    "print(\"All events available in my folder. list_eventNumber\", list_eventNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p(name,nparray):\n",
    "    print(\"Start\",name)\n",
    "    print(nparray)\n",
    "    print(\"End\",name,\"shape\",nparray.shape,\"dtype\",nparray.dtype,\"type\",type(nparray))\n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.59663469141339"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_eta(px,py,pz,m):\n",
    "    p=np.sqrt(px*px+py*py+pz*pz)\n",
    "    theta=np.arccos(pz/p)\n",
    "    eta=-np.log(np.tan(theta/2))\n",
    "    return eta\n",
    "# done function\n",
    "\n",
    "# example\n",
    "eta=calculate_eta(3,4,100000,0)\n",
    "eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this runs only once per event\n",
    "def buildAnnoyIndex(nparray_position,metric=\"angular\",ntrees=10,debug=False): \n",
    "    \n",
    "    numberDimension=nparray_position.shape[1] # 3 (x,y,z)\n",
    "    if debug:\n",
    "        print(\"numberDimension\",numberDimension,\"metric\",metric)\n",
    "    index=AnnoyIndex(numberDimension,metric)\n",
    "    if debug:\n",
    "        print(\"type(index)\",type(index))\n",
    "        print(\"enumerate data\")\n",
    "    # add each hit to the index\n",
    "    for i,position in enumerate(nparray_position):\n",
    "        if debug:\n",
    "            print(\"i\",i,\"position\",position)\n",
    "        index.add_item(i,position)\n",
    "    # done for loop over hits\n",
    "    # build the index with 10 trees\n",
    "    index.build(ntrees) # 10 trees\n",
    "    return index\n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_output_from_df_hits(df_hits,index):\n",
    "    list_nparray_input=[]\n",
    "    list_nparray_output=[]\n",
    "    list_nparray_eta=[]\n",
    "    list_nparray_volumeID=[]\n",
    "    list_nparray_layerID=[]\n",
    "    list_nparray_queryIndex=[]\n",
    "    list_nparray_nbPositiveHit=[]\n",
    "    \n",
    "    nparray_volume_id=df_hits[\"volume_id\"].values\n",
    "    nparray_layer_id=df_hits[\"layer_id\"].values \n",
    "\n",
    "    counterBucket=0\n",
    "    \n",
    "    #my_title=\"FirstLayer\"\n",
    "    my_title=\"All\"\n",
    "    for i in range(len(df_hits)):\n",
    "        if (\n",
    "            True # for each hit create a bucket\n",
    "            #(nparray_volume_id[i]==8 and nparray_layer_id[i]==2) \n",
    "            #or (nparray_volume_id[i]==7 and nparray_layer_id[i]==2)\n",
    "            #or (nparray_volume_id[i]==9 and nparray_layer_id[i]==14)\n",
    "            #or (nparray_volume_id[i]==16 and nparray_layer_id[i]==2)\n",
    "            #or (nparray_volume_id[i]==12 and nparray_layer_id[i]==2)\n",
    "            #or (nparray_volume_id[i]==18 and nparray_layer_id[i]==12)\n",
    "            #or (nparray_volume_id[i]==14 and nparray_layer_id[i]==12)\n",
    "            #or (nparray_volume_id[i]==17 and nparray_layer_id[i]==4)\n",
    "            )==False:\n",
    "            continue\n",
    "\n",
    "        counterBucket+=1\n",
    "        \n",
    "        #if (counterBucket<1)==False:\n",
    "        #    continue\n",
    "        \n",
    "        #if (counterBucket%10000==1)==False:\n",
    "        #    continue\n",
    "        \n",
    "        if debug or counterBucket%20000==1:\n",
    "            print(\"i\",i,\"counterBucket\",counterBucket)\n",
    "            \n",
    "        # using annoy to find the 20 nearest neighboring hits by angle to this hit\n",
    "        list_index=index.get_nns_by_item(i,bucketSize)\n",
    "        if debug:\n",
    "            print(\"list_index\",list_index)\n",
    "            \n",
    "        # create bucket\n",
    "        df_bucket=df_hits.iloc[list_index]\n",
    "        df_query=df_hits.iloc[i]\n",
    "        \n",
    "        if debug:\n",
    "            print(\"df_bucket\",df_bucket)\n",
    "            print(\"df_query\",df_query)\n",
    "            print(\"df_query_hit_id\",df_query[\"hit_id\"])\n",
    "        \n",
    "        # find the position (index) in the bucket of the query point and store it for later user\n",
    "        query_hitID=df_query[\"hit_id\"].astype(np.uint64)\n",
    "        if debug:\n",
    "            print(\"query_hitID\",query_hitID) \n",
    "        # for loop over hits in bucket and remember at which position (index) inside the bucket we have the query point\n",
    "        nparray_hitID=df_bucket[\"hit_id\"].values.astype(np.uint64)\n",
    "        for j in range(bucketSize):\n",
    "            current_hitID=nparray_hitID[j]\n",
    "            if debug:\n",
    "                print(\"current_hitID\",current_hitID)\n",
    "            if current_hitID==query_hitID:\n",
    "                queryIndex=j\n",
    "                break\n",
    "        # done for loop over hits in bucket\n",
    "        if debug:\n",
    "            print(\"queryIndex\",queryIndex)\n",
    "        # create numpy array for queryIndex\n",
    "        list_queryIndex=[queryIndex]\n",
    "        nparray_queryIndex=np.array(list_queryIndex).astype(np.uint8)\n",
    "        \n",
    "        # create NN input for this one bucket\n",
    "        # convert from default in pandas from np.float64 to np.float32, which is enough for the 4-5 digits it has\n",
    "        nparray_input=df_bucket[[\"x\",\"y\",\"z\"]].values.astype(np.float32).flatten()\n",
    "        if debug:\n",
    "            p(\"nparray_input\",nparray_input)\n",
    "            \n",
    "        # as for input, for volumeID\n",
    "        nparray_volumeID=df_bucket[\"volume_id\"].values.astype(np.uint8)\n",
    "        \n",
    "        # as for input, for layerID\n",
    "        nparray_layerID=df_bucket[\"layer_id\"].values.astype(np.uint8)\n",
    "            \n",
    "        # create NN output for this one bucket\n",
    "        # identify particle with the largest number of hits in this bucket\n",
    "        nparray_particleID=df_bucket[\"particle_id\"].values\n",
    "\n",
    "        if debug:\n",
    "            p(\"nparray_particleID\",nparray_particleID)\n",
    "        dict_particleID_counterParticleID={}\n",
    "        for particleID in nparray_particleID:\n",
    "            if particleID not in dict_particleID_counterParticleID:\n",
    "                dict_particleID_counterParticleID[particleID]=1\n",
    "            else:\n",
    "                dict_particleID_counterParticleID[particleID]+=1\n",
    "        if debug:\n",
    "            print(\"dict_particleID_counterParticleID\",dict_particleID_counterParticleID)\n",
    "          \n",
    "        # find the maximum value of the counters\n",
    "        particleIDWithMaxHits=0\n",
    "        counterParticleIDWithMaxHits=0\n",
    "        for particleID in dict_particleID_counterParticleID:\n",
    "            counterParticleID=dict_particleID_counterParticleID[particleID]\n",
    "            if counterParticleID>counterParticleIDWithMaxHits:\n",
    "                counterParticleIDWithMaxHits=counterParticleID\n",
    "                particleIDWithMaxHits=particleID\n",
    "        if debug:\n",
    "            print(\"counterBucket\",counterBucket,\"particleIDWithMaxHits\",particleIDWithMaxHits,\"counterParticleIDWithMaxHits\",counterParticleIDWithMaxHits)           \n",
    "        \n",
    "        # store the number of positive hits in this bucket, as it will be used a lot later\n",
    "        nparray_nbPositiveHit=np.array([counterParticleIDWithMaxHits]).astype(np.uint8)\n",
    "        \n",
    "        # calculate eta\n",
    "        nparray_tp=df_bucket[[\"tpx\",\"tpy\",\"tpz\"]].values.astype(np.float32)\n",
    "        if debug:\n",
    "            p(\"nparray_tp\",nparray_tp)\n",
    "        \n",
    "        # calculate eta for each hit and store it\n",
    "        if debug:\n",
    "            print(\"Start eta\")\n",
    "            print(\"particleIDWithMaxHits\",particleIDWithMaxHits)\n",
    "            \n",
    "        # for loop over hits in a bucket\n",
    "        # create list of eta for each hit in the bucket\n",
    "        list_eta=[]\n",
    "        for j,particleID in enumerate(nparray_particleID):\n",
    "            #print(\"j\",j,\"particleID\",particleID)\n",
    "            list_tp=nparray_tp[j]\n",
    "            # print(\"list_tp\",list_tp)\n",
    "            tpx=list_tp[0]\n",
    "            tpy=list_tp[1]\n",
    "            tpz=list_tp[2]\n",
    "            # let's assume mass of particle is zero (good approximationf for pions, kaons forming tracks)\n",
    "            m=0.0\n",
    "            #print(\"j\",j,\"tpx=%.4f, tpy=%.4f, tpz=%.4f\"%(tpx,tpy,tpz))\n",
    "            # \n",
    "            eta=calculate_eta(tpx,tpy,tpz,m)\n",
    "            if debug:\n",
    "                print(\"j\",j,\"particleID\",particleID,\"tpx=%.10f, tpy=%.10f, tpz=%.10f, eta=%.10f\"%(tpx,tpy,tpz,eta))\n",
    "            list_eta.append(eta)\n",
    "        # done for loop over hits in bucket\n",
    "        # by printing out I confirmed that all hits belonging to the particle have exactly the same tpx,tpy,tpz\n",
    "        # list_eta has bucketSize elements, so now make it a numpy array and convert it to type float 32\n",
    "        nparray_eta=np.array(list_eta).astype(np.float32)\n",
    "        if debug:\n",
    "            p(\"nparray_eta\",nparray_eta)\n",
    "        \n",
    "        # create nparray_output\n",
    "        list_output=[]\n",
    "        \n",
    "        # loop over every hit in the bucket\n",
    "        for particleID in nparray_particleID:      \n",
    "            if particleID==particleIDWithMaxHits:\n",
    "                list_output.append(1)\n",
    "            else:\n",
    "                list_output.append(-1)\n",
    "            # done if\n",
    "        # done for loop for each hit in the bucket\n",
    "        if debug:        \n",
    "            print(\"list_output\",list_output)\n",
    "        # create nparray for output from list and convert from the default int64 to int8\n",
    "        nparray_output=np.array(list_output).astype(np.int8)\n",
    "        if debug:\n",
    "            print(\"nparray_output\",nparray_output)\n",
    "            \n",
    "        if doBucketPlots and counterBucket%1000==0:\n",
    "            print(\"i\",i,\"counterBucket\",counterBucket)\n",
    "            \n",
    "            outputFileNameStem=outputFolderName+\"/plot_vis_e_\"+eventNumber+\"_i_\"+str(i)+\"_c_\"+str(counterBucket)+\"_\"+my_title+\"_\"\n",
    "            if True:\n",
    "                # plot x vs y\n",
    "                plt.plot(df_bucket.x,df_bucket.y,\"o\",color=\"red\")\n",
    "                plt.plot(df_query.x,df_query.y,\"D\",color=\"blue\")\n",
    "                plt.xlabel(\"x [mm]\")\n",
    "                plt.ylabel(\"y [mm]\")\n",
    "                plt.xlim(left=-1050,right=1050)\n",
    "                plt.ylim(bottom=-1050,top=1050)\n",
    "                #plt.plot(0,0,\"r+\")\n",
    "                plt.title(my_title+\". y vs x. Blue=query. eta=%.2f. i=%.0f\"%(eta,i))\n",
    "                # plt.show()\n",
    "                plotBucketFileNameStem=outputFileNameStem+\"y_vs_x\"\n",
    "                plt.savefig(plotBucketFileNameStem+\".png\")\n",
    "                #plt.savefig(plotBucketFileNameStem+\".pdf\")\n",
    "                plt.close()\n",
    "            # done if\n",
    "            if True:\n",
    "                # plot z vs r\n",
    "                df_bucket_r=np.sqrt(df_bucket.x**2+df_bucket.y**2)\n",
    "                df_query_r=np.sqrt(df_query.x**2+df_query.y**2)\n",
    "                plt.plot(df_bucket.z,df_bucket_r,\"o\",color=\"red\")\n",
    "                plt.plot(df_query.z,df_query_r,\"D\",color=\"blue\")\n",
    "                plt.xlabel(\"z [mm]\")\n",
    "                plt.ylabel(\"radius [mm]\")\n",
    "                plt.xlim(left=-3050,right=3050)\n",
    "                plt.ylim(bottom=-1050,top=1050)\n",
    "                #plt.plot(0,0,\"r+\")\n",
    "                plt.title(my_title+\". r vs z. Blue=query. eta=%.2f. i=%.0f\"%(eta,i))\n",
    "                # plt.show()\n",
    "                plotBucketFileNameStem=outputFileNameStem+\"r_vs_z\"\n",
    "                plt.savefig(plotBucketFileNameStem+\".png\")\n",
    "                #plt.savefig(plotBucketFileNameStem+\".pdf\")\n",
    "                plt.close()\n",
    "            # done if\n",
    "        # done if\n",
    "            \n",
    "        # add for this current bucket to the list for the entire event (which has several buckets)\n",
    "        list_nparray_input.append(nparray_input)\n",
    "        list_nparray_output.append(nparray_output)\n",
    "        list_nparray_eta.append(nparray_eta)\n",
    "        list_nparray_volumeID.append(nparray_volumeID)\n",
    "        list_nparray_layerID.append(nparray_layerID)\n",
    "        list_nparray_queryIndex.append(nparray_queryIndex)\n",
    "        list_nparray_npPositiveHit.append(nparray_nbPositiveHit)\n",
    "    # done for loop over hits in the event, so that for each hit we make a bucket\n",
    "    \n",
    "    # transform list of numpy array into numpy array of dimension 2 \n",
    "    # rows are buckets, columns are intput and output values\n",
    "    nparray_input_all=np.array(list_nparray_input)\n",
    "    nparray_output_all=np.array(list_nparray_output)\n",
    "    nparray_eta_all=np.array(list_nparray_eta)\n",
    "    nparray_volumeID_all=np.array(list_nparray_volumeID)\n",
    "    nparray_layerID_all=np.array(list_nparray_layerID)\n",
    "    nparray_queryIndex_all=np.array(list_nparray_queryIndex)\n",
    "    nparray_nbPositiveHit_all=np.array(list_nparray_nbPositiveHit)\n",
    "    if debug:\n",
    "        p(\"nparray_input_all\",nparray_input_all)\n",
    "        p(\"nparray_output_all\",nparray_output_all)\n",
    "        p(\"nparray_eta_all\",nparray_eta_all)\n",
    "        p(\"nparray_volumeID_all\",nparray_volumeID_all)\n",
    "        p(\"nparray_layerID_all\",nparray_layerID_all)\n",
    "        p(\"nparray_queryIndex_all\",nparray_queryIndex_all)\n",
    "        p(\"nparray_nbPositiveHit_all\",nparray_nbPositiveHit_all)\n",
    "    \n",
    "    return nparray_input_all, nparray_output_all, nparray_eta_all, nparray_volumeID_all, nparray_layerID_all, nparray_queryIndex_all, nparray_nbPositiveHit_all\n",
    "# done function      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def do_run_one_event_at_a_time(list_eventNumber):\n",
    "    # for loop over eventNumber from list_eventNumber and write individual event to files\n",
    "    for i,eventNumber in enumerate(list_eventNumber):\n",
    "        #if i>0:\n",
    "        #    continue\n",
    "    \n",
    "        print()\n",
    "        print()\n",
    "        print(\"******************************\")\n",
    "        print(\"i\",i,\"eventNumber\",eventNumber)\n",
    "        print(\"******************************\")\n",
    "    \n",
    "        inputFileName_hits_recon=inputFolderName+\"/event\"+eventNumber+\"-hits.csv\"\n",
    "        inputFileName_hits_truth=inputFolderName+\"/event\"+eventNumber+\"-truth.csv\"\n",
    "    \n",
    "        if debug or verbose:\n",
    "            print(\"Read csv files ad df and merge them\")\n",
    "        df_hits_recon=pd.read_csv(inputFileName_hits_recon)\n",
    "        df_hits_truth=pd.read_csv(inputFileName_hits_truth)\n",
    "        # hit_id appears in both, erase from one so that when we concatenate below it appears only once\n",
    "        del df_hits_truth[\"hit_id\"]\n",
    "        df_hits=pd.concat([df_hits_recon,df_hits_truth],axis=1,sort=False)\n",
    "    \n",
    "        # build annoy index\n",
    "        if debug or verbose:\n",
    "            print(\"Start do Annoy Index\")\n",
    "        nparray_position=df_hits[[\"x\",\"y\",\"z\"]].values\n",
    "        if debug:\n",
    "            p(\"nparray_position\",nparray_position)\n",
    "\n",
    "        index=buildAnnoyIndex(nparray_position,metric=\"angular\",ntrees=10,debug=False)\n",
    "    \n",
    "        # for loop peste hituri peste hiturile preselectate volume_id=8, layer_id=2\n",
    "        if debug or verbose:\n",
    "            print(\"From df_hits produce nparray_input_all,nparray_output_all\")\n",
    "        nparray_input_all,nparray_output_all,nparray_eta_all,nparray_volumeID_all,nparray_layerID_all,nparray_queryIndex_all,nparray_nbPositiveHit_all=get_input_output_from_df_hits(df_hits,index)\n",
    "    \n",
    "        if debug:\n",
    "            p(\"nparray_input_all\",nparray_input_all)\n",
    "            p(\"nparray_output_all\",nparray_output_all)\n",
    "            p(\"nparray_eta_all\",nparray_eta_all)\n",
    "            p(\"nparray_volumeID_all\",nparray_volumeID_all)\n",
    "            p(\"nparray_layerID_all\",nparray_layerID_all)\n",
    "            p(\"nparray_queryIndex_all\",nparray_queryIndex_all)\n",
    "            p(\"nparray_nbPositiveHit_all\",nparray_nbPositiveHit_all)\n",
    "    \n",
    "        # reshape only input by adding one extra dimension needed by tensorflow, in practice it puts all into one extra bracket\n",
    "        nparray_input_all=nparray_input_all.reshape(nparray_input_all.shape[0],nparray_input_all.shape[1],1)\n",
    "        if debug or verbose:\n",
    "            p(\"nparray_input_all after reshape\",nparray_input_all)\n",
    "\n",
    "        fileNameNNInputAll=outputFolderName+\"/NN_2_data_Input_\"+eventNumber+\".npy\"\n",
    "        fileNameNNOutputAll=outputFolderName+\"/NN_2_data_Output_\"+eventNumber+\".npy\"\n",
    "        fileNameNNEtaAll=outputFolderName+\"/NN_2_data_Eta_\"+eventNumber+\".npy\"\n",
    "        fileNameNNVolumeIDAll=outputFolderName+\"/NN_2_data_VolumeID_\"+eventNumber+\".npy\"\n",
    "        fileNameNNLayerIDAll=outputFolderName+\"/NN_2_data_LayerID_\"+eventNumber+\".npy\"\n",
    "        fileNameNNQueryIndexAll=outputFolderName+\"/NN_2_data_QueryIndex_\"+eventNumber+\".npy\"\n",
    "        fileNameNNNbPositiveHitAll=outputFolderName+\"/NN_2_data_NbPositiveHit_\"+eventNumber+\".npy\"\n",
    "            \n",
    "        np.save(fileNameNNInputAll,nparray_input_all)\n",
    "        np.save(fileNameNNOutputAll,nparray_output_all)\n",
    "        np.save(fileNameNNEtaAll,nparray_eta_all)\n",
    "        np.save(fileNameNNVolumeIDAll,nparray_volumeID_all)\n",
    "        np.save(fileNameNNLayerIDAll,nparray_layerID_all)\n",
    "        np.save(fileNameNNQueryIndexAll,nparray_queryIndex_all)\n",
    "        np.save(fileNameNNNbPositiveHitAll,nparray_nbPositiveHit_all)\n",
    "\n",
    "    # done for loop over events\n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doIndividual:\n",
    "    do_run_one_event_at_a_time(list_eventNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done do_create_output_based_on_min_nb_of_positive_hits(list_eventNumber\n"
     ]
    }
   ],
   "source": [
    "def do_create_output_based_on_min_nb_of_positive_hits(list_eventNumber):\n",
    "    nparray_output_negative_one_bucket=np.array([-1 for i in range(bucketSize)]).astype(np.int8)\n",
    "    # for loop over eventNumber from list_eventNumber\n",
    "    for i,eventNumber in enumerate(list_eventNumber):\n",
    "        if debug or verbose:\n",
    "            print(\"eventNumber\",eventNumber)\n",
    "        # open the files needed for this event: Output and NbPositiveHit\n",
    "        fileNameNNOutputAll=outputFolderName+\"/NN_2_data_Output_\"+eventNumber+\".npy\"\n",
    "        nparray_output_all=np.load(fileNameNNOutputAll)\n",
    "        fileNameNNNbPositiveHitAll=outputFolderName+\"/NN_2_data_NbPositiveHit_\"+eventNumber+\".npy\"\n",
    "        nparray_nbPositiveHit_all=np.load(fileNameNNNbPositiveHitAll)\n",
    "        # \n",
    "        dict_minNbPositiveHit_nparray_output_all={}\n",
    "        # for loop over each val of min number of positive fits\n",
    "        for minNbPositiveHit in list_minNbPositiveHit:\n",
    "            if debug:\n",
    "                print(\"i\",i,\"eventNumber\",eventNumber,\"minNbPositiveHit\",minNbPositiveHit)\n",
    "            # copy the current output in a completely new version with deep copy\n",
    "            # as we will want to replace only the values that need changed for less than needed positive hits\n",
    "            dict_minNbPositiveHit_nparray_output_all[minNbPositiveHit]=copy.deepcopy(nparray_output_all)\n",
    "        # done for loop over minNbPositiveHit\n",
    "        #\n",
    "        # loop over each row (bucket) in the output\n",
    "        for j in range(len(nparray_output_all)):\n",
    "            bucket=nparray_output_all[j]\n",
    "            if debug:\n",
    "                p(\"j=%.0f, bucket\"%j,bucket)\n",
    "            nbPositiveHit=nparray_nbPositiveHit_all[j]\n",
    "            if debug:\n",
    "                print(\"j=%.0f, nbPositiveHit=%.0f\"%(j,nbPositiveHit))\n",
    "            # to speed up, change only wheen needed\n",
    "            for minNbPositiveHit in list_minNbPositiveHit:\n",
    "                if nbPositiveHit<minNbPositiveHit:\n",
    "                    # replace this one line with the one with all negative\n",
    "                    dict_minNbPositiveHit_nparray_output_all[minNbPositiveHit][j]=nparray_output_negative_one_bucket\n",
    "                if debug:\n",
    "                    print(\"j=%.0f, minNbPositiveHit=%.0f, bucket\"%(j,minNbPositiveHit),dict_minNbPositiveHit_nparray_output_all[minNbPositiveHit][j])\n",
    "            # done for loop over minNbPositiveHit  \n",
    "        # done for loop over buckets\n",
    "        # for this event, write the new processed output to file\n",
    "        for minNbPositiveHit in list_minNbPositiveHit:\n",
    "            minString=\"Min\"\n",
    "            if minNbPositiveHit<9:\n",
    "                minString+=\"0\"\n",
    "            minString+=str(minNbPositiveHit)\n",
    "            fileNameNNOutputAll=outputFolderName+\"/NN_2_data_Output\"+minString+\"_\"+eventNumber+\".npy\"\n",
    "            np.save(fileNameNNOutputAll,dict_minNbPositiveHit_nparray_output_all[minNbPositiveHit])\n",
    "        # done for loop over minNbPositiveHit  \n",
    "    # done for loop over events\n",
    "# done function\n",
    "if debug or verbose:\n",
    "    print(\"Done do_create_output_based_on_min_nb_of_positive_hits(list_eventNumber\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doProcessIndividual:\n",
    "    do_create_output_based_on_min_nb_of_positive_hits(list_eventNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_merge_events(mergeOption):\n",
    "    # create output folder name for the merge, for the current number of events to merge\n",
    "    eventNumberMin=mergeOption[0] # included\n",
    "    eventNumberMax=mergeOption[1] # excluded\n",
    "    #\n",
    "    outputFolderNameMerge=outputFolderName+\"_ev_\"\n",
    "    #\n",
    "    if eventNumberMin<10:\n",
    "        outputFolderNameMerge+=\"00\"\n",
    "    elif eventNumberMin<100:\n",
    "        outputFolderNameMerge+=\"0\"\n",
    "    outputFolderNameMerge+=str(eventNumberMin)\n",
    "    #\n",
    "    outputFolderNameMerge+=\"_\"\n",
    "    #\n",
    "    if eventNumberMax<10:\n",
    "        outputFolderNameMerge+=\"00\"\n",
    "    elif eventNumberMax<100:\n",
    "        outputFolderNameMerge+=\"0\"\n",
    "    outputFolderNameMerge+=str(eventNumberMax)\n",
    "    #\n",
    "    if not os.path.exists(outputFolderNameMerge):\n",
    "        os.makedirs(outputFolderNameMerge)\n",
    "    \n",
    "    # for loop over events and merge them in train or test and save in a different folder\n",
    "    counter_event_Train=0\n",
    "    counter_event_Test=0\n",
    "    if debug or verbose:\n",
    "        print(\"Start loop over all events and merge them in either train or test\")\n",
    "    \n",
    "    # list of variables to merge\n",
    "    list_var=[\n",
    "        \"Input\",\n",
    "        \"Output\",\n",
    "        \"VolumeID\",\n",
    "        \"LayerID\",\n",
    "        \"Eta\",\n",
    "        \"QueryIndex\",\n",
    "        \"NbPositiveHit\",\n",
    "    ]\n",
    "    # add to list_var also the OutputMin04, OutputMin07, etc\n",
    "    for minNbPositiveHit in list_minNbPositiveHit:\n",
    "        minString=\"OutputMin\"\n",
    "        if minNbPositiveHit<9:\n",
    "            minString+=\"0\"\n",
    "        minString+=str(minNbPositiveHit)\n",
    "        list_var.append(minString)\n",
    "    # done for loop over minNbPositiveHit\n",
    "    if debug:\n",
    "        print(\"list_var\",list_var)\n",
    "    \n",
    "    #\n",
    "    dict_nameMerged_nparray={}\n",
    "    \n",
    "    # loop over the events, but careful, not all, but only from the first nrEventsToMerge\n",
    "    for i,eventNumber in enumerate(list_eventNumber[eventNumberMin:eventNumberMax]):\n",
    "        print(\"i\",i,\"eventNumber\",eventNumber)\n",
    "        \n",
    "        # for loop over all variables\n",
    "        dict_var_nparray={}\n",
    "        for var in list_var:\n",
    "            # read the stored file\n",
    "            fileName=outputFolderName+\"/NN_2_data_\"+var+\"_\"+eventNumber+\".npy\"\n",
    "            dict_var_nparray[var]=np.load(fileName)\n",
    "        # done for loop over var\n",
    "        \n",
    "        # calculate the rest of the event index to division by 100\n",
    "        # assuming we only merge multiples of ten, for each 10, we put first 7 in Train and last 3 in Test\n",
    "        rest=i%10\n",
    "\n",
    "        if rest<7: \n",
    "            # put this event into Train, with rest=i%10=0,1,2,3,4,5,6\n",
    "            counter_event_Train+=1\n",
    "            if counter_event_Train==1:\n",
    "                # it is the first event of type Train, so we simply deep copy it to the output\n",
    "                for var in list_var:\n",
    "                    dict_nameMerged_nparray[\"Train\"+var]=copy.deepcopy(dict_var_nparray[var])\n",
    "            else:\n",
    "                # it is not the first, so we concatentane to the new output\n",
    "                for var in list_var:\n",
    "                    dict_nameMerged_nparray[\"Train\"+var]=np.concatenate((dict_nameMerged_nparray[\"Train\"+var],dict_var_nparray[var]),axis=0,out=None)\n",
    "            # done if\n",
    "        else:\n",
    "            # put this event into Test, with rest=i%10=7,8,9\n",
    "            counter_event_Test+=1\n",
    "            if counter_event_Test==1:\n",
    "                # it is the first event of type Test, so we simply deep copy it to the output\n",
    "                for var in list_var:\n",
    "                    dict_nameMerged_nparray[\"Test\"+var]=copy.deepcopy(dict_var_nparray[var])\n",
    "            else:\n",
    "                # it is not the first, so we concatentane to the new output\n",
    "                for var in list_var:\n",
    "                    dict_nameMerged_nparray[\"Test\"+var]=np.concatenate((dict_nameMerged_nparray[\"Test\"+var],dict_var_nparray[var]),axis=0,out=None)\n",
    "            # done if\n",
    "        # done if  \n",
    "    # done for loop over eventNumber\n",
    "\n",
    "    # write file to the output\n",
    "    eventNumber=\"all\"\n",
    "    for TrainOrTest in \"Train,Test\".split(\",\"):\n",
    "        for var in list_var:\n",
    "            fileName=outputFolderNameMerge+\"/NN_2_data_\"+var+\"_\"+TrainOrTest+\"_\"+eventNumber+\".npy\"\n",
    "            np.save(fileName,dict_nameMerged_nparray[TrainOrTest+var])\n",
    "            if debug:\n",
    "                p(\"nparray_\"+TrainOrTest+\"_\"+var+\"_all_events\",dict_nameMerged_nparray[TrainOrTest+var])\n",
    "        # done for loop over var\n",
    "    # done for loop over TrainOrTest\n",
    "    \n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "******************************\n",
      "mergeOption [0, 100]\n",
      "******************************\n",
      "Start loop over all events and merge them in either train or test\n",
      "i 0 eventNumber 000000000\n",
      "i 1 eventNumber 000000001\n",
      "i 2 eventNumber 000000002\n",
      "i 3 eventNumber 000000003\n",
      "i 4 eventNumber 000000004\n",
      "i 5 eventNumber 000000005\n",
      "i 6 eventNumber 000000006\n",
      "i 7 eventNumber 000000007\n",
      "i 8 eventNumber 000000008\n",
      "i 9 eventNumber 000000009\n",
      "i 10 eventNumber 000000010\n",
      "i 11 eventNumber 000000011\n",
      "i 12 eventNumber 000000012\n",
      "i 13 eventNumber 000000013\n",
      "i 14 eventNumber 000000014\n",
      "i 15 eventNumber 000000015\n",
      "i 16 eventNumber 000000016\n",
      "i 17 eventNumber 000000017\n",
      "i 18 eventNumber 000000018\n",
      "i 19 eventNumber 000000019\n",
      "i 20 eventNumber 000000020\n",
      "i 21 eventNumber 000000021\n",
      "i 22 eventNumber 000000022\n",
      "i 23 eventNumber 000000023\n",
      "i 24 eventNumber 000000024\n",
      "i 25 eventNumber 000000025\n",
      "i 26 eventNumber 000000026\n",
      "i 27 eventNumber 000000027\n",
      "i 28 eventNumber 000000028\n",
      "i 29 eventNumber 000000029\n",
      "i 30 eventNumber 000000030\n",
      "i 31 eventNumber 000000031\n",
      "i 32 eventNumber 000000032\n",
      "i 33 eventNumber 000000033\n",
      "i 34 eventNumber 000000034\n",
      "i 35 eventNumber 000000035\n",
      "i 36 eventNumber 000000036\n",
      "i 37 eventNumber 000000037\n",
      "i 38 eventNumber 000000038\n",
      "i 39 eventNumber 000000039\n",
      "i 40 eventNumber 000000040\n",
      "i 41 eventNumber 000000041\n",
      "i 42 eventNumber 000000042\n",
      "i 43 eventNumber 000000043\n",
      "i 44 eventNumber 000000044\n",
      "i 45 eventNumber 000000045\n",
      "i 46 eventNumber 000000046\n",
      "i 47 eventNumber 000000047\n",
      "i 48 eventNumber 000000048\n",
      "i 49 eventNumber 000000049\n",
      "i 50 eventNumber 000000050\n",
      "i 51 eventNumber 000000051\n",
      "i 52 eventNumber 000000052\n",
      "i 53 eventNumber 000000053\n",
      "i 54 eventNumber 000000054\n",
      "i 55 eventNumber 000000055\n",
      "i 56 eventNumber 000000056\n",
      "i 57 eventNumber 000000057\n",
      "i 58 eventNumber 000000058\n",
      "i 59 eventNumber 000000059\n",
      "i 60 eventNumber 000000060\n",
      "i 61 eventNumber 000000061\n",
      "i 62 eventNumber 000000062\n",
      "i 63 eventNumber 000000063\n",
      "i 64 eventNumber 000000064\n",
      "i 65 eventNumber 000000065\n",
      "i 66 eventNumber 000000066\n",
      "i 67 eventNumber 000000067\n",
      "i 68 eventNumber 000000068\n",
      "i 69 eventNumber 000000069\n",
      "i 70 eventNumber 000000070\n",
      "i 71 eventNumber 000000071\n",
      "i 72 eventNumber 000000072\n",
      "i 73 eventNumber 000000073\n",
      "i 74 eventNumber 000000074\n",
      "i 75 eventNumber 000000075\n",
      "i 76 eventNumber 000000076\n",
      "i 77 eventNumber 000000077\n",
      "i 78 eventNumber 000000078\n",
      "i 79 eventNumber 000000079\n",
      "i 80 eventNumber 000000080\n",
      "i 81 eventNumber 000000081\n",
      "i 82 eventNumber 000000082\n",
      "i 83 eventNumber 000000083\n",
      "i 84 eventNumber 000000084\n",
      "i 85 eventNumber 000000085\n",
      "i 86 eventNumber 000000086\n",
      "i 87 eventNumber 000000087\n",
      "i 88 eventNumber 000000088\n",
      "i 89 eventNumber 000000089\n",
      "i 90 eventNumber 000000090\n",
      "i 91 eventNumber 000000091\n",
      "i 92 eventNumber 000000092\n",
      "i 93 eventNumber 000000093\n",
      "i 94 eventNumber 000000094\n",
      "i 95 eventNumber 000000095\n",
      "i 96 eventNumber 000000096\n",
      "i 97 eventNumber 000000097\n",
      "i 98 eventNumber 000000098\n",
      "i 99 eventNumber 000000099\n"
     ]
    }
   ],
   "source": [
    "if doMerge:\n",
    "    for mergeOption in list_mergeOption:\n",
    "        print()\n",
    "        print()\n",
    "        print(\"******************************\")\n",
    "        print(\"mergeOption\",mergeOption)\n",
    "        print(\"******************************\")\n",
    "        do_merge_events(mergeOption)\n",
    "        if debug:\n",
    "            print(\"Done mergeOption\",mergeOption)\n",
    "    # done for loop over mergeOption\n",
    "# done if"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_balance(TrainOrTest,nparray_input,nparray_output,nparray_nbPositiveHit,nparray_eta,nparray_layerID,nparray_volumeID,nparray_queryIndex,outputFolderNameMerge,doVerifyForHit=False,debug=False,verbose=False):\n",
    "    if debug or verbose:\n",
    "        print(\"Start get_nparray_weight for TrainOrTest=\",TrainOrTest)\n",
    " \n",
    "    if verbose:\n",
    "        p(\"nparray_input\",nparray_input)\n",
    "        p(\"nparray_output\",nparray_output)\n",
    "        p(\"nparray_nbPositiveHit\",nparray_nbPositiveHit)\n",
    "        p(\"nparray_eta\",nparray_eta)\n",
    "        p(\"nparray_layerID\",nparray_layerID)\n",
    "        p(\"nparray_volumeID\",nparray_volumeID)\n",
    "        p(\"nparray_queryIndex\",nparray_queryIndex)\n",
    "\n",
    "    # total number of buckets\n",
    "    nbBucket=nparray_nbPositiveHit.shape[0]\n",
    "    nrHitTotal=nbBucket*bucketSize\n",
    "    if debug or verbose:\n",
    "        print()\n",
    "        print()\n",
    "        print(TrainOrTest+\":\")\n",
    "        print(\"nbBucket\",nbBucket,\"bucketSize\",bucketSize,\"nrHitTotal\",nrHitTotal)\n",
    "        \n",
    "        \n",
    "    # dictionary between number of positive hits in a bucket and the number of buckets with this number of positive hits\n",
    "    dict_nbPositiveHit_counterBucket_unbalanced={}\n",
    "    dict_nbPositiveHit_counterBucket_balancedDesired={}\n",
    "    dict_nbPositiveHit_counterBucket_balancedCounter={}\n",
    "    dict_nbPositiveHit_counterBucket_balancedObtained={}\n",
    "    # create explicit the list of posssible values for nbPositiveHit (0, 1, 2, ..., 19, 20)\n",
    "    list_nbPositiveHit=[i for i in range(0,21)]\n",
    "    for nbPositiveHit in list_nbPositiveHit:\n",
    "        dict_nbPositiveHit_counterBucket_unbalanced[nbPositiveHit]=0\n",
    "        dict_nbPositiveHit_counterBucket_balancedDesired[nbPositiveHit]=0\n",
    "        dict_nbPositiveHit_counterBucket_balancedCounter[nbPositiveHit]=0\n",
    "        dict_nbPositiveHit_counterBucket_balancedObtained[nbPositiveHit]=0\n",
    "    \n",
    "    \n",
    "    # now fill the dictionary of the counters depending on nbPositiveHit\n",
    "    # loop over buckets\n",
    "    for i in range(nbBucket):\n",
    "        nbPositiveHit=nparray_nbPositiveHit[i][0] # one column only, so take that element index [0]\n",
    "        if debug:\n",
    "            print(\"nbPositiveHit\",nbPositiveHit)\n",
    "        dict_nbPositiveHit_counterBucket_unbalanced[nbPositiveHit]+=1\n",
    "\n",
    "    # now dictionary is filled and we can print the values\n",
    "    if debug or verbose:\n",
    "        print()\n",
    "        print(\"Original unbalanced. \"+TrainOrTest+\":\")\n",
    "        for nbPositiveHit in list_nbPositiveHit:\n",
    "            counterBucket=dict_nbPositiveHit_counterBucket_unbalanced[nbPositiveHit]\n",
    "            percentBucket=100*counterBucket/nbBucket\n",
    "            print(\"nbPositiveHit=%.0f\"%nbPositiveHit,\"counterBucket=%.0f\"%counterBucket,\"percentBucket=%.1f\"%percentBucket)\n",
    "            \n",
    "    # plot legend position\n",
    "    legend_position=\"upper right\"\n",
    "            \n",
    "    # make plots\n",
    "    # we want bins from 0 to 21, as bin 20 is between 20 and 21\n",
    "    bins=[i-0.5 for i in range(0,22)]\n",
    "    print(\"bins\",bins)\n",
    "    # plot not normalized\n",
    "    plt.hist(nparray_nbPositiveHit,bins=bins,histtype=\"step\",color=\"red\",label=\"Unbalanced\",)\n",
    "    plt.title(\"Histogram not normalized for \"+TrainOrTest)\n",
    "    plt.xlabel(\"Number of positive hits per bucket\")\n",
    "    plt.ylabel(\"Number of buckets\")\n",
    "    plt.legend(loc=legend_position)\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.savefig(outputFolderNameMerge+\"/plot_histo_1_NbBucket_vs_NbPositiveHit_\"+TrainOrTest+\".png\")\n",
    "    plt.savefig(outputFolderNameMerge+\"/plot_histo_1_NbBucket_vs_NbPositiveHit_\"+TrainOrTest+\".pdf\")\n",
    "    plt.close()\n",
    "    # plot normalized\n",
    "    plt.hist(nparray_nbPositiveHit,bins=bins,density=True,histtype=\"step\",color=\"red\",label=\"Unbalanced\")\n",
    "    plt.title(\"Histogram normalized for \"+TrainOrTest)\n",
    "    plt.xlabel(\"Number of positive hits per bucket\")\n",
    "    plt.ylabel(\"Number of buckets\")\n",
    "    plt.legend(loc=legend_position)\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.savefig(outputFolderNameMerge+\"/plot_histo_1_NbBucket_vs_NbPositiveHit_normalized_\"+TrainOrTest+\".png\")\n",
    "    plt.savefig(outputFolderNameMerge+\"/plot_histo_1_NbBucket_vs_NbPositiveHit_normalized_\"+TrainOrTest+\".pdf\")\n",
    "    plt.close()\n",
    "    \n",
    "    # we want to balance out around 10 by removing not needed buckets, to balance 9 with 11, 8 with 12, etc\n",
    "    \n",
    "    # first calculate the desired number of buckets in each category, so that it is balanced around 10\n",
    "    # original version, trully symmetric, with a peak in the middle\n",
    "    #for i in range(bucketSize//2+1):\n",
    "    #    #print(\"i\",i)\n",
    "    #    val_left=dict_nbPositiveHit_counterBucket_unbalanced[i]\n",
    "    #    #print(\"val_left\",val_left)\n",
    "    #    val_right=dict_nbPositiveHit_counterBucket_unbalanced[bucketSize-i]\n",
    "    #    #print(\"val_right\",val_right)\n",
    "    #    if val_left<=val_right:\n",
    "    #        val_min=val_left\n",
    "    #    else:\n",
    "    #        val_min=val_right\n",
    "    #    dict_nbPositiveHit_counterBucket_balancedDesired[i]=val_min\n",
    "    #    dict_nbPositiveHit_counterBucket_balancedDesired[bucketSize-i]=val_min\n",
    "    # done for loop over i \n",
    "    \n",
    "    # first calculate the desired number of buckets in each category, so that it is balanced around 10\n",
    "    # the idea is we want to cut from the peak to be equal with the sides\n",
    "    # we can not cut all the way to 20, as there would be too little\n",
    "    # we find a compromise and use the same value from 6 to 14, and that value is the value of 14\n",
    "    # as there are more on the left than on the right\n",
    "    # also difference with respect to the first version\n",
    "    # 0 and 20, remain as they are, as 0 is not possible, it is unfair to put 20 to count of 0 just to be symmetric\n",
    "    # do the same for 1 and 19, as very few are at 1\n",
    "    # so for 0-20 and 1-19 to nothing\n",
    "    # then to as above for 2-18, 3-17, 4-16, 5-15\n",
    "    # and then for 6-14, 7-13, 8-12, 9-11 and 10 use the count from 14\n",
    "    for i in range(bucketSize//2+1):\n",
    "        #print(\"i\",i)\n",
    "        val_left=dict_nbPositiveHit_counterBucket_unbalanced[i]\n",
    "        #print(\"val_left\",val_left)\n",
    "        val_right=dict_nbPositiveHit_counterBucket_unbalanced[bucketSize-i]\n",
    "        #print(\"val_right\",val_right)\n",
    "        if i==0 or i==1:\n",
    "            # do not change the values, so set them as they are\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesired[i]=val_left\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesired[bucketSize-i]=val_right\n",
    "        elif i==2 or i==3 or i==4 or i==5:\n",
    "            # do as before\n",
    "            if val_left<=val_right:\n",
    "                val_min=val_left\n",
    "            else:\n",
    "                val_min=val_right\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesired[i]=val_min\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesired[bucketSize-i]=val_min\n",
    "        elif i==6 or i==7 or i==8 or i==9 or i==10:\n",
    "            # set the smallest value of them\n",
    "            # which we know from the shape of the distribution it is the value of i=20-6=14\n",
    "            val_common=dict_nbPositiveHit_counterBucket_unbalanced[14]\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesired[i]=val_common\n",
    "            dict_nbPositiveHit_counterBucket_balancedDesired[bucketSize-i]=val_common\n",
    "        else:\n",
    "            print(\"Warning! You should not get here!\")\n",
    "        # done if\n",
    "    # done for loop over i \n",
    "\n",
    "    # now dictionary is filled and we can print the values\n",
    "    if debug or verbose:\n",
    "        print()\n",
    "        print(\"BalancedDesired. \"+TrainOrTest+\":\")\n",
    "        for nbPositiveHit in list_nbPositiveHit:\n",
    "            counterBucket=dict_nbPositiveHit_counterBucket_balancedDesired[nbPositiveHit]\n",
    "            percentBucket=100*counterBucket/nbBucket\n",
    "            print(\"nbPositiveHit=%.0f\"%nbPositiveHit,\"counterBucket=%.0f\"%counterBucket,\"percentBucket=%.1f\"%percentBucket)\n",
    "    \n",
    "\n",
    "    # we want a nparray_inputC that will have a subset of nparray_input, same for nparray_output\n",
    "    list_inputBalanced=[]\n",
    "    list_outputBalanced=[]\n",
    "    list_nbPositiveHitBalanced=[]\n",
    "    list_etaBalanced=[]\n",
    "    list_layerIDBalanced=[]\n",
    "    list_volumeIDBalanced=[]\n",
    "    list_queryIndexBalanced=[]\n",
    "    # loop over buckets\n",
    "    \n",
    "    print()\n",
    "    for i in range(nbBucket):\n",
    "        # first decide if I need to keep this bucket or not\n",
    "        # first find the nbPositiveHit of this bucket\n",
    "        nbPositiveHit=nparray_nbPositiveHit[i][0]\n",
    "        # find how many buckets I am allowed to keep in this category\n",
    "        nrBucketDesired=dict_nbPositiveHit_counterBucket_balancedDesired[nbPositiveHit]\n",
    "        #print(\"j\",j,\"nbPositiveHit\",nbPositiveHit,\"nrBucketDesired\",nrBucketDesired)\n",
    "        # find if this bucket is allowed, meaning if the counter of this bucketg is smaller than allowed\n",
    "        # so I need to find the current counter\n",
    "        dict_nbPositiveHit_counterBucket_balancedCounter[nbPositiveHit]+=1\n",
    "        # now counter is increased and I can make the check, so first bucket has counter 1 (not start from 0)\n",
    "        # if current current counter smaller or equal with the allowed, you write the current bucket\n",
    "        # if not, you skip it\n",
    "        #print(\"nbPositiveHit\",nbPositiveHit,\"nrBucketDesired\",nrBucketDesired,\"counter\",dict_nbPositiveHit_counterBucket_balancedObtained[nbPositiveHit])\n",
    "        if dict_nbPositiveHit_counterBucket_balancedCounter[nbPositiveHit]<=nrBucketDesired:\n",
    "            dict_nbPositiveHit_counterBucket_balancedObtained[nbPositiveHit]+=1\n",
    "            list_inputBalanced.append(nparray_input[i])\n",
    "            list_outputBalanced.append(nparray_output[i])\n",
    "            list_nbPositiveHitBalanced.append(nparray_nbPositiveHit[i])\n",
    "            list_etaBalanced.append(nparray_eta[i])\n",
    "            list_layerIDBalanced.append(nparray_layerID[i])\n",
    "            list_volumeIDBalanced.append(nparray_volumeID[i])\n",
    "            list_queryIndexBalanced.append(nparray_queryIndex[i])           \n",
    "        else:\n",
    "            # do not add it, so that the datasets remained balanced\n",
    "            #print(\"Not add nbPositiveHit\",nbPositiveHit,\"nrBucketDesired\",nrBucketDesired,\"counter\",dict_nbPositiveHit_counterBucket_balancedObtained[nbPositiveHit])\n",
    "            pass\n",
    "        # done if\n",
    "    # done loop over buckets\n",
    "    \n",
    "    # now dictionary is filled and we can print the values\n",
    "    if debug or verbose:\n",
    "        print()\n",
    "        print(\"BalancedCounter after. \"+TrainOrTest+\":\")\n",
    "        for nbPositiveHit in list_nbPositiveHit:\n",
    "            counterBucket=dict_nbPositiveHit_counterBucket_balancedObtained[nbPositiveHit]\n",
    "            percentBucket=100*counterBucket/nbBucket\n",
    "            print(\"nbPositiveHit=%.0f\"%nbPositiveHit,\"counterBucket=%.0f\"%counterBucket,\"percentBucket=%.1f\"%percentBucket)\n",
    "\n",
    "    \n",
    "    # now dictionary is filled and we can print the values\n",
    "    if debug or verbose:\n",
    "        print()\n",
    "        print(\"BalancedObtained after. \"+TrainOrTest+\":\")\n",
    "        for nbPositiveHit in list_nbPositiveHit:\n",
    "            counterBucket=dict_nbPositiveHit_counterBucket_balancedObtained[nbPositiveHit]\n",
    "            percentBucket=100*counterBucket/nbBucket\n",
    "            print(\"nbPositiveHit=%.0f\"%nbPositiveHit,\"counterBucket=%.0f\"%counterBucket,\"percentBucket=%.1f\"%percentBucket)\n",
    "\n",
    "    # convert list to numpy arrays\n",
    "    nparray_inputBalanced=np.array(list_inputBalanced)\n",
    "    nparray_outputBalanced=np.array(list_outputBalanced)\n",
    "    nparray_nbPositiveHitBalanced=np.array(list_nbPositiveHitBalanced)\n",
    "    nparray_etaBalanced=np.array(list_etaBalanced)\n",
    "    nparray_layerIDBalanced=np.array(list_layerIDBalanced)\n",
    "    nparray_volumeIDBalanced=np.array(list_volumeIDBalanced)\n",
    "    nparray_queryIndexBalanced=np.array(list_queryIndexBalanced)\n",
    "    #\n",
    "    p(\"nparray_inputBalanced\",nparray_inputBalanced)\n",
    "    outputFileNameBalanced=outputFolderNameMerge+\"/NN_2_data_InputBalanced_\"+TrainOrTest+\"_all.npy\"\n",
    "    np.save(outputFileNameBalanced,nparray_inputBalanced)\n",
    "    #\n",
    "    p(\"nparray_outputBalanced\",nparray_outputBalanced)\n",
    "    outputFileNameBalanced=outputFolderNameMerge+\"/NN_2_data_OutputBalanced_\"+TrainOrTest+\"_all.npy\"\n",
    "    np.save(outputFileNameBalanced,nparray_outputBalanced)\n",
    "    #\n",
    "    p(\"nparray_nbPositiveHitBalanced\",nparray_nbPositiveHitBalanced)\n",
    "    outputFileNameBalanced=outputFolderNameMerge+\"/NN_2_data_NbPositiveHitBalanced_\"+TrainOrTest+\"_all.npy\"\n",
    "    np.save(outputFileNameBalanced,nparray_nbPositiveHitBalanced)\n",
    "    \n",
    "    p(\"nparray_etaBalanced\",nparray_etaBalanced)\n",
    "    outputFileNameBalanced=outputFolderNameMerge+\"/NN_2_data_EtaBalanced_\"+TrainOrTest+\"_all.npy\"\n",
    "    np.save(outputFileNameBalanced,nparray_etaBalanced)\n",
    "    \n",
    "    p(\"nparray_layerIDBalanced\",nparray_layerIDBalanced)\n",
    "    outputFileNameBalanced=outputFolderNameMerge+\"/NN_2_data_LayerIDBalanced_\"+TrainOrTest+\"_all.npy\"\n",
    "    np.save(outputFileNameBalanced,nparray_layerIDBalanced)\n",
    "    \n",
    "    p(\"nparray_volumeIDBalanced\",nparray_volumeIDBalanced)\n",
    "    outputFileNameBalanced=outputFolderNameMerge+\"/NN_2_data_VolumeIDBalanced_\"+TrainOrTest+\"_all.npy\"\n",
    "    np.save(outputFileNameBalanced,nparray_volumeIDBalanced)\n",
    "    \n",
    "    p(\"nparray_queryIndexBalanced\",nparray_queryIndexBalanced)\n",
    "    outputFileNameBalanced=outputFolderNameMerge+\"/NN_2_data_QueryIndexBalanced_\"+TrainOrTest+\"_all.npy\"\n",
    "    np.save(outputFileNameBalanced,nparray_queryIndexBalanced)\n",
    "    \n",
    "    # make plots\n",
    "    # we want bins from 0 to 21, as bin 20 is between 20 and 21\n",
    "    bins=[i-0.5 for i in range(0,22)]\n",
    "    print(\"bins\",bins)\n",
    "    # plot not normalized\n",
    "    plt.hist(nparray_nbPositiveHit,bins=bins,histtype=\"step\",color=\"red\",label=\"Unbalanced\")\n",
    "    plt.hist(nparray_nbPositiveHitBalanced,bins=bins,histtype=\"step\",color=\"blue\",label=\"Balanced\")\n",
    "    plt.title(\"Histogram not normalized for \"+TrainOrTest)\n",
    "    plt.xlabel(\"Number of positive hits per bucket\")\n",
    "    plt.ylabel(\"Number of buckets\")\n",
    "    plt.legend(loc=legend_position)\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.savefig(outputFolderNameMerge+\"/plot_histo_2_NbBucket_vs_NbPositiveHit_\"+TrainOrTest+\".png\")\n",
    "    plt.savefig(outputFolderNameMerge+\"/plot_histo_2_NbBucket_vs_NbPositiveHit_\"+TrainOrTest+\".pdf\")\n",
    "    plt.close()\n",
    "    # plot normalized\n",
    "    plt.hist(nparray_nbPositiveHit,bins=bins,density=True,histtype=\"step\",color=\"red\",label=\"Unbalanced\")\n",
    "    plt.hist(nparray_nbPositiveHitBalanced,bins=bins,density=True,histtype=\"step\",color=\"blue\",label=\"Balanced\")\n",
    "    plt.title(\"Histogram normalized for \"+TrainOrTest)\n",
    "    plt.xlabel(\"Number of positive hits per bucket\")\n",
    "    plt.ylabel(\"Number of buckets\")\n",
    "    plt.legend(loc=legend_position)\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.savefig(outputFolderNameMerge+\"/plot_histo_2_NbBucket_vs_NbPositiveHit_normalized_\"+TrainOrTest+\".png\")\n",
    "    plt.savefig(outputFolderNameMerge+\"/plot_histo_2_NbBucket_vs_NbPositiveHit_normalized_\"+TrainOrTest+\".pdf\")\n",
    "    plt.close()\n",
    "\n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_balance_mergeOption(mergeOption):\n",
    "    if debug or verbose:\n",
    "        print(\"do_balance_mergeOption(), with mergeOption=\",mergeOption)\n",
    "        \n",
    "        \n",
    "    # create output folder name for the merge, for the current number of events to merge\n",
    "    eventNumberMin=mergeOption[0] # included\n",
    "    eventNumberMax=mergeOption[1] # excluded\n",
    "    #\n",
    "    outputFolderNameMerge=outputFolderName+\"_ev_\"\n",
    "    #\n",
    "    if eventNumberMin<10:\n",
    "        outputFolderNameMerge+=\"00\"\n",
    "    elif eventNumberMin<100:\n",
    "        outputFolderNameMerge+=\"0\"\n",
    "    outputFolderNameMerge+=str(eventNumberMin)\n",
    "    #\n",
    "    outputFolderNameMerge+=\"_\"\n",
    "    #\n",
    "    if eventNumberMax<10:\n",
    "        outputFolderNameMerge+=\"00\"\n",
    "    elif eventNumberMax<100:\n",
    "        outputFolderNameMerge+=\"0\"\n",
    "    outputFolderNameMerge+=str(eventNumberMax)\n",
    "    #\n",
    "    if not os.path.exists(outputFolderNameMerge):\n",
    "        os.makedirs(outputFolderNameMerge)\n",
    "        \n",
    "    eventNumber=\"all\"\n",
    "    \n",
    "    # Input\n",
    "    fileNameNNInputTrainAll=outputFolderNameMerge+\"/NN_2_data_Input_Train_\"+eventNumber+\".npy\"\n",
    "    fileNameNNInputTestAll=outputFolderNameMerge+\"/NN_2_data_Input_Test_\"+eventNumber+\".npy\"\n",
    "    nparray_Input_Train_all_Events=np.load(fileNameNNInputTrainAll)\n",
    "    nparray_Input_Test_all_Events=np.load(fileNameNNInputTestAll)\n",
    "    if debug:\n",
    "        print()\n",
    "        p(\"nparray_Input_Train_all_Events\",nparray_Input_Train_all_Events)\n",
    "        p(\"nparray_Input_Test_all_Events\",nparray_Input_Test_all_Events) \n",
    "    \n",
    "    # Output\n",
    "    fileNameNNOutputTrainAll=outputFolderNameMerge+\"/NN_2_data_Output_Train_\"+eventNumber+\".npy\"\n",
    "    fileNameNNOutputTestAll=outputFolderNameMerge+\"/NN_2_data_Output_Test_\"+eventNumber+\".npy\"\n",
    "    nparray_Output_Train_all_Events=np.load(fileNameNNOutputTrainAll)\n",
    "    nparray_Output_Test_all_Events=np.load(fileNameNNOutputTestAll)\n",
    "    if debug:\n",
    "        print()\n",
    "        p(\"nparray_Output_Train_all_Events\",nparray_Output_Train_all_Events)\n",
    "        p(\"nparray_Output_Test_all_Events\",nparray_Output_Test_all_Events)  \n",
    "    \n",
    "    # NbPositiveHit\n",
    "    fileNameNNNbPositiveHitTrainAll=outputFolderNameMerge+\"/NN_2_data_NbPositiveHit_Train_\"+eventNumber+\".npy\"\n",
    "    fileNameNNNbPositiveHitTestAll=outputFolderNameMerge+\"/NN_2_data_NbPositiveHit_Test_\"+eventNumber+\".npy\"\n",
    "    nparray_NbPositiveHit_Train_all_Events=np.load(fileNameNNNbPositiveHitTrainAll)\n",
    "    nparray_NbPositiveHit_Test_all_Events=np.load(fileNameNNNbPositiveHitTestAll)\n",
    "    if debug:\n",
    "        print()\n",
    "        p(\"nparray_NbPositiveHit_Train_all_Events\",nparray_NbPositiveHit_Train_all_Events)\n",
    "        p(\"nparray_NbPositiveHit_Test_all_Events\",nparray_NbPositiveHit_Test_all_Events)\n",
    "        \n",
    "    # Eta\n",
    "    fileNameNNEtaTrainAll=outputFolderNameMerge+\"/NN_2_data_Eta_Train_\"+eventNumber+\".npy\"\n",
    "    fileNameNNEtaTestAll=outputFolderNameMerge+\"/NN_2_data_Eta_Test_\"+eventNumber+\".npy\"\n",
    "    nparray_Eta_Train_all_Events=np.load(fileNameNNEtaTrainAll)\n",
    "    nparray_Eta_Test_all_Events=np.load(fileNameNNEtaTestAll)\n",
    "    if debug:\n",
    "        print()\n",
    "        p(\"nparray_Eta_Train_all_Events\",nparray_Eta_Train_all_Events)\n",
    "        p(\"nparray_Eta_Test_all_Events\",nparray_Eta_Test_all_Events)\n",
    "        \n",
    "    # LayerID\n",
    "    fileNameNNLayerIDTrainAll=outputFolderNameMerge+\"/NN_2_data_LayerID_Train_\"+eventNumber+\".npy\"\n",
    "    fileNameNNLayerIDTestAll=outputFolderNameMerge+\"/NN_2_data_LayerID_Test_\"+eventNumber+\".npy\"\n",
    "    nparray_LayerID_Train_all_Events=np.load(fileNameNNLayerIDTrainAll)\n",
    "    nparray_LayerID_Test_all_Events=np.load(fileNameNNLayerIDTestAll)\n",
    "    if debug:\n",
    "        print()\n",
    "        p(\"nparray_LayerID_Train_all_Events\",nparray_LayerID_Train_all_Events)\n",
    "        p(\"nparray_LayerID_Test_all_Events\",nparray_LayerID_Test_all_Events)\n",
    "        \n",
    "    # VolumeID\n",
    "    fileNameNNVolumeIDTrainAll=outputFolderNameMerge+\"/NN_2_data_VolumeID_Train_\"+eventNumber+\".npy\"\n",
    "    fileNameNNVolumeIDTestAll=outputFolderNameMerge+\"/NN_2_data_VolumeID_Test_\"+eventNumber+\".npy\"\n",
    "    nparray_VolumeID_Train_all_Events=np.load(fileNameNNVolumeIDTrainAll)\n",
    "    nparray_VolumeID_Test_all_Events=np.load(fileNameNNVolumeIDTestAll)\n",
    "    if debug:\n",
    "        print()\n",
    "        p(\"nparray_VolumeID_Train_all_Events\",nparray_VolumeID_Train_all_Events)\n",
    "        p(\"nparray_VolumeID_Test_all_Events\",nparray_VolumeID_Test_all_Events)\n",
    "        \n",
    "    # QueryIndex\n",
    "    fileNameNNQueryIndexTrainAll=outputFolderNameMerge+\"/NN_2_data_QueryIndex_Train_\"+eventNumber+\".npy\"\n",
    "    fileNameNNQueryIndexTestAll=outputFolderNameMerge+\"/NN_2_data_QueryIndex_Test_\"+eventNumber+\".npy\"\n",
    "    nparray_QueryIndex_Train_all_Events=np.load(fileNameNNQueryIndexTrainAll)\n",
    "    nparray_QueryIndex_Test_all_Events=np.load(fileNameNNQueryIndexTestAll)\n",
    "    if debug:\n",
    "        print()\n",
    "        p(\"nparray_QueryIndex_Train_all_Events\",nparray_QueryIndex_Train_all_Events)\n",
    "        p(\"nparray_QueryIndex_Test_all_Events\",nparray_QueryIndex_Test_all_Events)\n",
    "    \n",
    "    # for Train and Test\n",
    "    if True:\n",
    "        do_balance(\"Train\",nparray_Input_Train_all_Events,nparray_Output_Train_all_Events,nparray_NbPositiveHit_Train_all_Events,nparray_Eta_Train_all_Events,nparray_LayerID_Train_all_Events,nparray_VolumeID_Train_all_Events,nparray_QueryIndex_Train_all_Events,outputFolderNameMerge,doVerifyForHit=True,debug=debug,verbose=True)\n",
    "        do_balance(\"Test\",nparray_Input_Test_all_Events,nparray_Output_Test_all_Events,nparray_NbPositiveHit_Test_all_Events,nparray_Eta_Test_all_Events,nparray_LayerID_Test_all_Events,nparray_VolumeID_Test_all_Events,nparray_QueryIndex_Test_all_Events,outputFolderNameMerge,doVerifyForHit=True,debug=debug,verbose=True)\n",
    "    # weights ready\n",
    "    \n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "******************************\n",
      "mergeOption [0, 100]\n",
      "******************************\n",
      "do_balance_mergeOption(), with mergeOption= [0, 100]\n",
      "Start get_nparray_weight for TrainOrTest= Train\n",
      "Start nparray_input\n",
      "[[[ 3.68880e+01]\n",
      "  [ 1.38507e+00]\n",
      "  [-1.29750e+03]\n",
      "  ...\n",
      "  [ 3.74781e+01]\n",
      "  [-1.87127e+00]\n",
      "  [-1.29750e+03]]\n",
      "\n",
      " [[ 4.72534e+01]\n",
      "  [ 3.84431e+00]\n",
      "  [-1.50250e+03]\n",
      "  ...\n",
      "  [ 3.99709e+01]\n",
      "  [ 5.32750e+00]\n",
      "  [-1.30250e+03]]\n",
      "\n",
      " [[ 7.07738e+01]\n",
      "  [ 8.25329e+00]\n",
      "  [-1.30250e+03]\n",
      "  ...\n",
      "  [ 4.35503e+01]\n",
      "  [ 7.16400e+00]\n",
      "  [-8.22500e+02]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-9.38969e+02]\n",
      "  [ 7.77719e+01]\n",
      "  [ 2.95250e+03]\n",
      "  ...\n",
      "  [-7.18244e+01]\n",
      "  [ 5.22404e+00]\n",
      "  [ 2.40089e+02]]\n",
      "\n",
      " [[-8.70819e+02]\n",
      "  [-1.58230e+00]\n",
      "  [ 2.95550e+03]\n",
      "  ...\n",
      "  [-3.34212e+01]\n",
      "  [-1.16950e+00]\n",
      "  [ 1.12495e+02]]\n",
      "\n",
      " [[-1.01531e+03]\n",
      "  [ 2.37788e+01]\n",
      "  [ 2.95250e+03]\n",
      "  ...\n",
      "  [-2.64031e+02]\n",
      "  [ 6.17020e+00]\n",
      "  [ 7.97277e+02]]]\n",
      "End nparray_input shape (7353542, 60, 1) dtype float32 type <class 'numpy.ndarray'>\n",
      "Start nparray_output\n",
      "[[ 1  1  1 ... -1 -1 -1]\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " ...\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " [ 1 -1  1 ... -1 -1  1]]\n",
      "End nparray_output shape (7353542, 20) dtype int8 type <class 'numpy.ndarray'>\n",
      "Start nparray_nbPositiveHit\n",
      "[[ 6]\n",
      " [ 8]\n",
      " [ 9]\n",
      " ...\n",
      " [ 7]\n",
      " [14]\n",
      " [ 7]]\n",
      "End nparray_nbPositiveHit shape (7353542, 1) dtype uint8 type <class 'numpy.ndarray'>\n",
      "Start nparray_eta\n",
      "[[-4.2717023  -4.2717023  -4.2717023  ... -4.2464147  -4.2272186\n",
      "  -4.2272186 ]\n",
      " [-4.244235   -4.244235   -4.244235   ... -4.1417737  -4.1417737\n",
      "  -4.1417737 ]\n",
      " [-3.5683124  -3.5683124  -3.5683124  ... -3.5681052  -3.6279647\n",
      "  -3.6279647 ]\n",
      " ...\n",
      " [ 1.8915993   1.8915993   1.8915993  ...  1.7111589   1.3045204\n",
      "   1.9628427 ]\n",
      " [ 1.9287983   1.9287983   1.9287983  ...  0.66878563  1.8374135\n",
      "   2.1429393 ]\n",
      " [ 1.7810965   1.5129554   1.7810965  ...  1.7654951   1.6363424\n",
      "   1.7810965 ]]\n",
      "End nparray_eta shape (7353542, 20) dtype float32 type <class 'numpy.ndarray'>\n",
      "Start nparray_layerID\n",
      "[[ 4  2  6 ...  6  6  4]\n",
      " [ 2  4  4 ...  6  4  4]\n",
      " [ 4  2  6 ...  2 10 10]\n",
      " ...\n",
      " [12 10  8 ...  6  2  4]\n",
      " [12 12 12 ...  2  2  2]\n",
      " [12  4 10 ... 10  4  2]]\n",
      "End nparray_layerID shape (7353542, 20) dtype uint8 type <class 'numpy.ndarray'>\n",
      "Start nparray_volumeID\n",
      "[[ 7  7  7 ...  7  7  7]\n",
      " [ 7  7  7 ...  7  7  7]\n",
      " [ 7  7  7 ...  7  7  7]\n",
      " ...\n",
      " [18 18 14 ...  8  8  8]\n",
      " [18 18 18 ...  8 13  8]\n",
      " [18  8 18 ... 18  8 13]]\n",
      "End nparray_volumeID shape (7353542, 20) dtype uint8 type <class 'numpy.ndarray'>\n",
      "Start nparray_queryIndex\n",
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "End nparray_queryIndex shape (7353542, 1) dtype uint8 type <class 'numpy.ndarray'>\n",
      "\n",
      "\n",
      "Train:\n",
      "nbBucket 7353542 bucketSize 20 nrHitTotal 147070840\n",
      "\n",
      "Original unbalanced. Train:\n",
      "nbPositiveHit=0 counterBucket=0 percentBucket=0.0\n",
      "nbPositiveHit=1 counterBucket=3 percentBucket=0.0\n",
      "nbPositiveHit=2 counterBucket=11977 percentBucket=0.2\n",
      "nbPositiveHit=3 counterBucket=102408 percentBucket=1.4\n",
      "nbPositiveHit=4 counterBucket=363809 percentBucket=4.9\n",
      "nbPositiveHit=5 counterBucket=591638 percentBucket=8.0\n",
      "nbPositiveHit=6 counterBucket=860452 percentBucket=11.7\n",
      "nbPositiveHit=7 counterBucket=943536 percentBucket=12.8\n",
      "nbPositiveHit=8 counterBucket=966108 percentBucket=13.1\n",
      "nbPositiveHit=9 counterBucket=840738 percentBucket=11.4\n",
      "nbPositiveHit=10 counterBucket=847413 percentBucket=11.5\n",
      "nbPositiveHit=11 counterBucket=627344 percentBucket=8.5\n",
      "nbPositiveHit=12 counterBucket=512063 percentBucket=7.0\n",
      "nbPositiveHit=13 counterBucket=324381 percentBucket=4.4\n",
      "nbPositiveHit=14 counterBucket=213198 percentBucket=2.9\n",
      "nbPositiveHit=15 counterBucket=95757 percentBucket=1.3\n",
      "nbPositiveHit=16 counterBucket=34417 percentBucket=0.5\n",
      "nbPositiveHit=17 counterBucket=10756 percentBucket=0.1\n",
      "nbPositiveHit=18 counterBucket=5544 percentBucket=0.1\n",
      "nbPositiveHit=19 counterBucket=1406 percentBucket=0.0\n",
      "nbPositiveHit=20 counterBucket=594 percentBucket=0.0\n",
      "bins [-0.5, 0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5, 10.5, 11.5, 12.5, 13.5, 14.5, 15.5, 16.5, 17.5, 18.5, 19.5, 20.5]\n",
      "\n",
      "BalancedDesired. Train:\n",
      "nbPositiveHit=0 counterBucket=0 percentBucket=0.0\n",
      "nbPositiveHit=1 counterBucket=3 percentBucket=0.0\n",
      "nbPositiveHit=2 counterBucket=5544 percentBucket=0.1\n",
      "nbPositiveHit=3 counterBucket=10756 percentBucket=0.1\n",
      "nbPositiveHit=4 counterBucket=34417 percentBucket=0.5\n",
      "nbPositiveHit=5 counterBucket=95757 percentBucket=1.3\n",
      "nbPositiveHit=6 counterBucket=213198 percentBucket=2.9\n",
      "nbPositiveHit=7 counterBucket=213198 percentBucket=2.9\n",
      "nbPositiveHit=8 counterBucket=213198 percentBucket=2.9\n",
      "nbPositiveHit=9 counterBucket=213198 percentBucket=2.9\n",
      "nbPositiveHit=10 counterBucket=213198 percentBucket=2.9\n",
      "nbPositiveHit=11 counterBucket=213198 percentBucket=2.9\n",
      "nbPositiveHit=12 counterBucket=213198 percentBucket=2.9\n",
      "nbPositiveHit=13 counterBucket=213198 percentBucket=2.9\n",
      "nbPositiveHit=14 counterBucket=213198 percentBucket=2.9\n",
      "nbPositiveHit=15 counterBucket=95757 percentBucket=1.3\n",
      "nbPositiveHit=16 counterBucket=34417 percentBucket=0.5\n",
      "nbPositiveHit=17 counterBucket=10756 percentBucket=0.1\n",
      "nbPositiveHit=18 counterBucket=5544 percentBucket=0.1\n",
      "nbPositiveHit=19 counterBucket=1406 percentBucket=0.0\n",
      "nbPositiveHit=20 counterBucket=594 percentBucket=0.0\n",
      "\n",
      "\n",
      "BalancedCounter after. Train:\n",
      "nbPositiveHit=0 counterBucket=0 percentBucket=0.0\n",
      "nbPositiveHit=1 counterBucket=3 percentBucket=0.0\n",
      "nbPositiveHit=2 counterBucket=5544 percentBucket=0.1\n",
      "nbPositiveHit=3 counterBucket=10756 percentBucket=0.1\n",
      "nbPositiveHit=4 counterBucket=34417 percentBucket=0.5\n",
      "nbPositiveHit=5 counterBucket=95757 percentBucket=1.3\n",
      "nbPositiveHit=6 counterBucket=213198 percentBucket=2.9\n",
      "nbPositiveHit=7 counterBucket=213198 percentBucket=2.9\n",
      "nbPositiveHit=8 counterBucket=213198 percentBucket=2.9\n",
      "nbPositiveHit=9 counterBucket=213198 percentBucket=2.9\n",
      "nbPositiveHit=10 counterBucket=213198 percentBucket=2.9\n",
      "nbPositiveHit=11 counterBucket=213198 percentBucket=2.9\n",
      "nbPositiveHit=12 counterBucket=213198 percentBucket=2.9\n",
      "nbPositiveHit=13 counterBucket=213198 percentBucket=2.9\n",
      "nbPositiveHit=14 counterBucket=213198 percentBucket=2.9\n",
      "nbPositiveHit=15 counterBucket=95757 percentBucket=1.3\n",
      "nbPositiveHit=16 counterBucket=34417 percentBucket=0.5\n",
      "nbPositiveHit=17 counterBucket=10756 percentBucket=0.1\n",
      "nbPositiveHit=18 counterBucket=5544 percentBucket=0.1\n",
      "nbPositiveHit=19 counterBucket=1406 percentBucket=0.0\n",
      "nbPositiveHit=20 counterBucket=594 percentBucket=0.0\n",
      "\n",
      "BalancedObtained after. Train:\n",
      "nbPositiveHit=0 counterBucket=0 percentBucket=0.0\n",
      "nbPositiveHit=1 counterBucket=3 percentBucket=0.0\n",
      "nbPositiveHit=2 counterBucket=5544 percentBucket=0.1\n",
      "nbPositiveHit=3 counterBucket=10756 percentBucket=0.1\n",
      "nbPositiveHit=4 counterBucket=34417 percentBucket=0.5\n",
      "nbPositiveHit=5 counterBucket=95757 percentBucket=1.3\n",
      "nbPositiveHit=6 counterBucket=213198 percentBucket=2.9\n",
      "nbPositiveHit=7 counterBucket=213198 percentBucket=2.9\n",
      "nbPositiveHit=8 counterBucket=213198 percentBucket=2.9\n",
      "nbPositiveHit=9 counterBucket=213198 percentBucket=2.9\n",
      "nbPositiveHit=10 counterBucket=213198 percentBucket=2.9\n",
      "nbPositiveHit=11 counterBucket=213198 percentBucket=2.9\n",
      "nbPositiveHit=12 counterBucket=213198 percentBucket=2.9\n",
      "nbPositiveHit=13 counterBucket=213198 percentBucket=2.9\n",
      "nbPositiveHit=14 counterBucket=213198 percentBucket=2.9\n",
      "nbPositiveHit=15 counterBucket=95757 percentBucket=1.3\n",
      "nbPositiveHit=16 counterBucket=34417 percentBucket=0.5\n",
      "nbPositiveHit=17 counterBucket=10756 percentBucket=0.1\n",
      "nbPositiveHit=18 counterBucket=5544 percentBucket=0.1\n",
      "nbPositiveHit=19 counterBucket=1406 percentBucket=0.0\n",
      "nbPositiveHit=20 counterBucket=594 percentBucket=0.0\n",
      "Start nparray_inputBalanced\n",
      "[[[ 3.68880e+01]\n",
      "  [ 1.38507e+00]\n",
      "  [-1.29750e+03]\n",
      "  ...\n",
      "  [ 3.74781e+01]\n",
      "  [-1.87127e+00]\n",
      "  [-1.29750e+03]]\n",
      "\n",
      " [[ 4.72534e+01]\n",
      "  [ 3.84431e+00]\n",
      "  [-1.50250e+03]\n",
      "  ...\n",
      "  [ 3.99709e+01]\n",
      "  [ 5.32750e+00]\n",
      "  [-1.30250e+03]]\n",
      "\n",
      " [[ 7.07738e+01]\n",
      "  [ 8.25329e+00]\n",
      "  [-1.30250e+03]\n",
      "  ...\n",
      "  [ 4.35503e+01]\n",
      "  [ 7.16400e+00]\n",
      "  [-8.22500e+02]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-3.39920e+02]\n",
      "  [ 3.81811e+02]\n",
      "  [ 1.50450e+03]\n",
      "  ...\n",
      "  [-1.87632e+02]\n",
      "  [ 1.87763e+02]\n",
      "  [ 7.77216e+02]]\n",
      "\n",
      " [[-9.43905e+02]\n",
      "  [ 6.73048e-01]\n",
      "  [ 2.95550e+03]\n",
      "  ...\n",
      "  [-4.10712e+02]\n",
      "  [ 9.52379e+00]\n",
      "  [ 1.22150e+03]]\n",
      "\n",
      " [[-8.70819e+02]\n",
      "  [-1.58230e+00]\n",
      "  [ 2.95550e+03]\n",
      "  ...\n",
      "  [-3.34212e+01]\n",
      "  [-1.16950e+00]\n",
      "  [ 1.12495e+02]]]\n",
      "End nparray_inputBalanced shape (2213733, 60, 1) dtype float32 type <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start nparray_outputBalanced\n",
      "[[ 1  1  1 ... -1 -1 -1]\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " ...\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " [ 1  1  1 ... -1 -1 -1]]\n",
      "End nparray_outputBalanced shape (2213733, 20) dtype int8 type <class 'numpy.ndarray'>\n",
      "Start nparray_nbPositiveHitBalanced\n",
      "[[ 6]\n",
      " [ 8]\n",
      " [ 9]\n",
      " ...\n",
      " [15]\n",
      " [14]\n",
      " [14]]\n",
      "End nparray_nbPositiveHitBalanced shape (2213733, 1) dtype uint8 type <class 'numpy.ndarray'>\n",
      "Start nparray_etaBalanced\n",
      "[[-4.2717023  -4.2717023  -4.2717023  ... -4.2464147  -4.2272186\n",
      "  -4.2272186 ]\n",
      " [-4.244235   -4.244235   -4.244235   ... -4.1417737  -4.1417737\n",
      "  -4.1417737 ]\n",
      " [-3.5683124  -3.5683124  -3.5683124  ... -3.5681052  -3.6279647\n",
      "  -3.6279647 ]\n",
      " ...\n",
      " [ 1.8007632   1.8007632   1.8007632  ...  0.03138195  1.6682348\n",
      "   1.768864  ]\n",
      " [ 1.8374135   1.8374135   1.8374135  ...  1.7654951   1.3045204\n",
      "   1.7810965 ]\n",
      " [ 1.9287983   1.9287983   1.9287983  ...  0.66878563  1.8374135\n",
      "   2.1429393 ]]\n",
      "End nparray_etaBalanced shape (2213733, 20) dtype float32 type <class 'numpy.ndarray'>\n",
      "Start nparray_layerIDBalanced\n",
      "[[ 4  2  6 ...  6  6  4]\n",
      " [ 2  4  4 ...  6  4  4]\n",
      " [ 4  2  6 ...  2 10 10]\n",
      " ...\n",
      " [ 4 12  6 ...  2  4  2]\n",
      " [12 12  8 ...  4  2  2]\n",
      " [12 12 12 ...  2  2  2]]\n",
      "End nparray_layerIDBalanced shape (2213733, 20) dtype uint8 type <class 'numpy.ndarray'>\n",
      "Start nparray_volumeIDBalanced\n",
      "[[ 7  7  7 ...  7  7  7]\n",
      " [ 7  7  7 ...  7  7  7]\n",
      " [ 7  7  7 ...  7  7  7]\n",
      " ...\n",
      " [14 18 14 ...  8  8 13]\n",
      " [18 18 14 ...  8  8 14]\n",
      " [18 18 18 ...  8 13  8]]\n",
      "End nparray_volumeIDBalanced shape (2213733, 20) dtype uint8 type <class 'numpy.ndarray'>\n",
      "Start nparray_queryIndexBalanced\n",
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "End nparray_queryIndexBalanced shape (2213733, 1) dtype uint8 type <class 'numpy.ndarray'>\n",
      "bins [-0.5, 0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5, 10.5, 11.5, 12.5, 13.5, 14.5, 15.5, 16.5, 17.5, 18.5, 19.5, 20.5]\n",
      "Start get_nparray_weight for TrainOrTest= Test\n",
      "Start nparray_input\n",
      "[[[ 6.10612e+01]\n",
      "  [ 5.48271e+00]\n",
      "  [-1.30250e+03]\n",
      "  ...\n",
      "  [ 3.11451e+01]\n",
      "  [ 3.69277e+00]\n",
      "  [-6.97500e+02]]\n",
      "\n",
      " [[ 5.66778e+01]\n",
      "  [ 6.69252e+00]\n",
      "  [-1.30250e+03]\n",
      "  ...\n",
      "  [ 5.79303e+01]\n",
      "  [ 4.73475e+00]\n",
      "  [-1.30250e+03]]\n",
      "\n",
      " [[ 3.15741e+01]\n",
      "  [ 3.40823e+00]\n",
      "  [-1.49750e+03]\n",
      "  ...\n",
      "  [ 3.07265e+01]\n",
      "  [-2.34300e+00]\n",
      "  [-1.49750e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-9.37832e+02]\n",
      "  [ 9.58561e+01]\n",
      "  [ 2.95250e+03]\n",
      "  ...\n",
      "  [-2.60461e+02]\n",
      "  [ 2.15204e+01]\n",
      "  [ 8.56198e+02]]\n",
      "\n",
      " [[-9.01023e+02]\n",
      "  [ 1.83920e+01]\n",
      "  [ 2.95250e+03]\n",
      "  ...\n",
      "  [-7.32054e+01]\n",
      "  [-1.71362e+00]\n",
      "  [ 2.52744e+02]]\n",
      "\n",
      " [[-8.76783e+02]\n",
      "  [ 7.24166e+01]\n",
      "  [ 2.95250e+03]\n",
      "  ...\n",
      "  [-6.16801e+02]\n",
      "  [ 7.02741e+01]\n",
      "  [ 2.14850e+03]]]\n",
      "End nparray_input shape (3218871, 60, 1) dtype float32 type <class 'numpy.ndarray'>\n",
      "Start nparray_output\n",
      "[[ 1  1  1 ... -1 -1 -1]\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " [-1 -1 -1 ...  1 -1 -1]\n",
      " ...\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " [ 1  1  1 ... -1 -1 -1]]\n",
      "End nparray_output shape (3218871, 20) dtype int8 type <class 'numpy.ndarray'>\n",
      "Start nparray_nbPositiveHit\n",
      "[[ 9]\n",
      " [ 9]\n",
      " [ 6]\n",
      " ...\n",
      " [10]\n",
      " [ 9]\n",
      " [ 8]]\n",
      "End nparray_nbPositiveHit shape (3218871, 1) dtype uint8 type <class 'numpy.ndarray'>\n",
      "Start nparray_eta\n",
      "[[-3.7804482 -3.7804482 -3.7804482 ... -3.7871683 -3.7871683 -3.851502 ]\n",
      " [-3.851502  -3.851502  -3.851502  ... -3.932433  -3.7871683 -3.7871683]\n",
      " [-4.597932  -4.597932  -4.5397544 ... -4.503565  -4.51713   -4.5574536]\n",
      " ...\n",
      " [ 1.8539613  1.8539613  1.8539613 ...  1.9936935  1.9315617  1.9315617]\n",
      " [ 1.9019215  1.9019215  1.9019215 ...  1.8027561  2.2765532  1.6355878]\n",
      " [ 1.9315617  1.9315617  1.9315617 ...  1.9936935  1.9936935  1.9936935]]\n",
      "End nparray_eta shape (3218871, 20) dtype float32 type <class 'numpy.ndarray'>\n",
      "Start nparray_layerID\n",
      "[[ 4  2  6 ...  2  4 12]\n",
      " [ 4  6  8 ... 10  6  4]\n",
      " [ 2  2  2 ...  4  2  2]\n",
      " ...\n",
      " [12  8  8 ...  2  4  2]\n",
      " [12 10  8 ...  2  2  4]\n",
      " [12 12  8 ...  6  2  8]]\n",
      "End nparray_layerID shape (3218871, 20) dtype uint8 type <class 'numpy.ndarray'>\n",
      "Start nparray_volumeID\n",
      "[[ 7  7  7 ...  7  7  7]\n",
      " [ 7  7  7 ...  7  7  7]\n",
      " [ 7  7  7 ...  7  7  7]\n",
      " ...\n",
      " [18 14 14 ... 13  8 13]\n",
      " [18 18 14 ... 13  8  8]\n",
      " [18 18 14 ... 14 13 14]]\n",
      "End nparray_volumeID shape (3218871, 20) dtype uint8 type <class 'numpy.ndarray'>\n",
      "Start nparray_queryIndex\n",
      "[[1]\n",
      " [3]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "End nparray_queryIndex shape (3218871, 1) dtype uint8 type <class 'numpy.ndarray'>\n",
      "\n",
      "\n",
      "Test:\n",
      "nbBucket 3218871 bucketSize 20 nrHitTotal 64377420\n",
      "\n",
      "Original unbalanced. Test:\n",
      "nbPositiveHit=0 counterBucket=0 percentBucket=0.0\n",
      "nbPositiveHit=1 counterBucket=2 percentBucket=0.0\n",
      "nbPositiveHit=2 counterBucket=5299 percentBucket=0.2\n",
      "nbPositiveHit=3 counterBucket=45907 percentBucket=1.4\n",
      "nbPositiveHit=4 counterBucket=162072 percentBucket=5.0\n",
      "nbPositiveHit=5 counterBucket=263175 percentBucket=8.2\n",
      "nbPositiveHit=6 counterBucket=374306 percentBucket=11.6\n",
      "nbPositiveHit=7 counterBucket=408595 percentBucket=12.7\n",
      "nbPositiveHit=8 counterBucket=416068 percentBucket=12.9\n",
      "nbPositiveHit=9 counterBucket=365600 percentBucket=11.4\n",
      "nbPositiveHit=10 counterBucket=371571 percentBucket=11.5\n",
      "nbPositiveHit=11 counterBucket=275118 percentBucket=8.5\n",
      "nbPositiveHit=12 counterBucket=226593 percentBucket=7.0\n",
      "nbPositiveHit=13 counterBucket=142882 percentBucket=4.4\n",
      "nbPositiveHit=14 counterBucket=94391 percentBucket=2.9\n",
      "nbPositiveHit=15 counterBucket=43452 percentBucket=1.3\n",
      "nbPositiveHit=16 counterBucket=15491 percentBucket=0.5\n",
      "nbPositiveHit=17 counterBucket=5191 percentBucket=0.2\n",
      "nbPositiveHit=18 counterBucket=2198 percentBucket=0.1\n",
      "nbPositiveHit=19 counterBucket=685 percentBucket=0.0\n",
      "nbPositiveHit=20 counterBucket=275 percentBucket=0.0\n",
      "bins [-0.5, 0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5, 10.5, 11.5, 12.5, 13.5, 14.5, 15.5, 16.5, 17.5, 18.5, 19.5, 20.5]\n",
      "\n",
      "BalancedDesired. Test:\n",
      "nbPositiveHit=0 counterBucket=0 percentBucket=0.0\n",
      "nbPositiveHit=1 counterBucket=2 percentBucket=0.0\n",
      "nbPositiveHit=2 counterBucket=2198 percentBucket=0.1\n",
      "nbPositiveHit=3 counterBucket=5191 percentBucket=0.2\n",
      "nbPositiveHit=4 counterBucket=15491 percentBucket=0.5\n",
      "nbPositiveHit=5 counterBucket=43452 percentBucket=1.3\n",
      "nbPositiveHit=6 counterBucket=94391 percentBucket=2.9\n",
      "nbPositiveHit=7 counterBucket=94391 percentBucket=2.9\n",
      "nbPositiveHit=8 counterBucket=94391 percentBucket=2.9\n",
      "nbPositiveHit=9 counterBucket=94391 percentBucket=2.9\n",
      "nbPositiveHit=10 counterBucket=94391 percentBucket=2.9\n",
      "nbPositiveHit=11 counterBucket=94391 percentBucket=2.9\n",
      "nbPositiveHit=12 counterBucket=94391 percentBucket=2.9\n",
      "nbPositiveHit=13 counterBucket=94391 percentBucket=2.9\n",
      "nbPositiveHit=14 counterBucket=94391 percentBucket=2.9\n",
      "nbPositiveHit=15 counterBucket=43452 percentBucket=1.3\n",
      "nbPositiveHit=16 counterBucket=15491 percentBucket=0.5\n",
      "nbPositiveHit=17 counterBucket=5191 percentBucket=0.2\n",
      "nbPositiveHit=18 counterBucket=2198 percentBucket=0.1\n",
      "nbPositiveHit=19 counterBucket=685 percentBucket=0.0\n",
      "nbPositiveHit=20 counterBucket=275 percentBucket=0.0\n",
      "\n",
      "\n",
      "BalancedCounter after. Test:\n",
      "nbPositiveHit=0 counterBucket=0 percentBucket=0.0\n",
      "nbPositiveHit=1 counterBucket=2 percentBucket=0.0\n",
      "nbPositiveHit=2 counterBucket=2198 percentBucket=0.1\n",
      "nbPositiveHit=3 counterBucket=5191 percentBucket=0.2\n",
      "nbPositiveHit=4 counterBucket=15491 percentBucket=0.5\n",
      "nbPositiveHit=5 counterBucket=43452 percentBucket=1.3\n",
      "nbPositiveHit=6 counterBucket=94391 percentBucket=2.9\n",
      "nbPositiveHit=7 counterBucket=94391 percentBucket=2.9\n",
      "nbPositiveHit=8 counterBucket=94391 percentBucket=2.9\n",
      "nbPositiveHit=9 counterBucket=94391 percentBucket=2.9\n",
      "nbPositiveHit=10 counterBucket=94391 percentBucket=2.9\n",
      "nbPositiveHit=11 counterBucket=94391 percentBucket=2.9\n",
      "nbPositiveHit=12 counterBucket=94391 percentBucket=2.9\n",
      "nbPositiveHit=13 counterBucket=94391 percentBucket=2.9\n",
      "nbPositiveHit=14 counterBucket=94391 percentBucket=2.9\n",
      "nbPositiveHit=15 counterBucket=43452 percentBucket=1.3\n",
      "nbPositiveHit=16 counterBucket=15491 percentBucket=0.5\n",
      "nbPositiveHit=17 counterBucket=5191 percentBucket=0.2\n",
      "nbPositiveHit=18 counterBucket=2198 percentBucket=0.1\n",
      "nbPositiveHit=19 counterBucket=685 percentBucket=0.0\n",
      "nbPositiveHit=20 counterBucket=275 percentBucket=0.0\n",
      "\n",
      "BalancedObtained after. Test:\n",
      "nbPositiveHit=0 counterBucket=0 percentBucket=0.0\n",
      "nbPositiveHit=1 counterBucket=2 percentBucket=0.0\n",
      "nbPositiveHit=2 counterBucket=2198 percentBucket=0.1\n",
      "nbPositiveHit=3 counterBucket=5191 percentBucket=0.2\n",
      "nbPositiveHit=4 counterBucket=15491 percentBucket=0.5\n",
      "nbPositiveHit=5 counterBucket=43452 percentBucket=1.3\n",
      "nbPositiveHit=6 counterBucket=94391 percentBucket=2.9\n",
      "nbPositiveHit=7 counterBucket=94391 percentBucket=2.9\n",
      "nbPositiveHit=8 counterBucket=94391 percentBucket=2.9\n",
      "nbPositiveHit=9 counterBucket=94391 percentBucket=2.9\n",
      "nbPositiveHit=10 counterBucket=94391 percentBucket=2.9\n",
      "nbPositiveHit=11 counterBucket=94391 percentBucket=2.9\n",
      "nbPositiveHit=12 counterBucket=94391 percentBucket=2.9\n",
      "nbPositiveHit=13 counterBucket=94391 percentBucket=2.9\n",
      "nbPositiveHit=14 counterBucket=94391 percentBucket=2.9\n",
      "nbPositiveHit=15 counterBucket=43452 percentBucket=1.3\n",
      "nbPositiveHit=16 counterBucket=15491 percentBucket=0.5\n",
      "nbPositiveHit=17 counterBucket=5191 percentBucket=0.2\n",
      "nbPositiveHit=18 counterBucket=2198 percentBucket=0.1\n",
      "nbPositiveHit=19 counterBucket=685 percentBucket=0.0\n",
      "nbPositiveHit=20 counterBucket=275 percentBucket=0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start nparray_inputBalanced\n",
      "[[[ 6.10612e+01]\n",
      "  [ 5.48271e+00]\n",
      "  [-1.30250e+03]\n",
      "  ...\n",
      "  [ 3.11451e+01]\n",
      "  [ 3.69277e+00]\n",
      "  [-6.97500e+02]]\n",
      "\n",
      " [[ 5.66778e+01]\n",
      "  [ 6.69252e+00]\n",
      "  [-1.30250e+03]\n",
      "  ...\n",
      "  [ 5.79303e+01]\n",
      "  [ 4.73475e+00]\n",
      "  [-1.30250e+03]]\n",
      "\n",
      " [[ 3.15741e+01]\n",
      "  [ 3.40823e+00]\n",
      "  [-1.49750e+03]\n",
      "  ...\n",
      "  [ 3.07265e+01]\n",
      "  [-2.34300e+00]\n",
      "  [-1.49750e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 3.41097e+02]\n",
      "  [ 9.00468e+02]\n",
      "  [ 2.55550e+03]\n",
      "  ...\n",
      "  [ 2.75365e+02]\n",
      "  [ 7.69948e+02]\n",
      "  [ 2.14450e+03]]\n",
      "\n",
      " [[ 9.90678e-01]\n",
      "  [-1.56666e+02]\n",
      "  [ 6.02000e+02]\n",
      "  ...\n",
      "  [ 2.14916e-01]\n",
      "  [-3.37550e+01]\n",
      "  [ 1.26807e+02]]\n",
      "\n",
      " [[-7.47925e+02]\n",
      "  [ 2.43178e+02]\n",
      "  [ 2.94450e+03]\n",
      "  ...\n",
      "  [-1.47942e+02]\n",
      "  [ 4.31422e+01]\n",
      "  [ 5.98000e+02]]]\n",
      "End nparray_inputBalanced shape (983145, 60, 1) dtype float32 type <class 'numpy.ndarray'>\n",
      "Start nparray_outputBalanced\n",
      "[[ 1  1  1 ... -1 -1 -1]\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " [-1 -1 -1 ...  1 -1 -1]\n",
      " ...\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " [ 1  1  1 ... -1 -1  1]\n",
      " [ 1  1  1 ... -1  1 -1]]\n",
      "End nparray_outputBalanced shape (983145, 20) dtype int8 type <class 'numpy.ndarray'>\n",
      "Start nparray_nbPositiveHitBalanced\n",
      "[[ 9]\n",
      " [ 9]\n",
      " [ 6]\n",
      " ...\n",
      " [14]\n",
      " [16]\n",
      " [14]]\n",
      "End nparray_nbPositiveHitBalanced shape (983145, 1) dtype uint8 type <class 'numpy.ndarray'>\n",
      "Start nparray_etaBalanced\n",
      "[[-3.7804482 -3.7804482 -3.7804482 ... -3.7871683 -3.7871683 -3.851502 ]\n",
      " [-3.851502  -3.851502  -3.851502  ... -3.932433  -3.7871683 -3.7871683]\n",
      " [-4.597932  -4.597932  -4.5397544 ... -4.503565  -4.51713   -4.5574536]\n",
      " ...\n",
      " [ 1.7052298  1.7052298  1.7052298 ...  1.6711477  1.6711477  1.6711477]\n",
      " [ 2.0617988  2.0617988  2.0617988 ...  0.5750069  2.4096217  2.0617988]\n",
      " [ 2.0247416  2.0247416  2.0247416 ...  1.9789946  2.0247416  2.5101397]]\n",
      "End nparray_etaBalanced shape (983145, 20) dtype float32 type <class 'numpy.ndarray'>\n",
      "Start nparray_layerIDBalanced\n",
      "[[ 4  2  6 ...  2  4 12]\n",
      " [ 4  6  8 ... 10  6  4]\n",
      " [ 2  2  2 ...  4  2  2]\n",
      " ...\n",
      " [10  4  2 ...  4  4  8]\n",
      " [ 2  2 12 ...  2  4  2]\n",
      " [12 10 10 ...  4  6  2]]\n",
      "End nparray_layerIDBalanced shape (983145, 20) dtype uint8 type <class 'numpy.ndarray'>\n",
      "Start nparray_volumeIDBalanced\n",
      "[[ 7  7  7 ...  7  7  7]\n",
      " [ 7  7  7 ...  7  7  7]\n",
      " [ 7  7  7 ...  7  7  7]\n",
      " ...\n",
      " [18  8  8 ... 13 14 18]\n",
      " [ 9  9 18 ...  8  8  8]\n",
      " [18 14 14 ...  8  8  9]]\n",
      "End nparray_volumeIDBalanced shape (983145, 20) dtype uint8 type <class 'numpy.ndarray'>\n",
      "Start nparray_queryIndexBalanced\n",
      "[[1]\n",
      " [3]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [2]\n",
      " [0]]\n",
      "End nparray_queryIndexBalanced shape (983145, 1) dtype uint8 type <class 'numpy.ndarray'>\n",
      "bins [-0.5, 0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5, 10.5, 11.5, 12.5, 13.5, 14.5, 15.5, 16.5, 17.5, 18.5, 19.5, 20.5]\n"
     ]
    }
   ],
   "source": [
    "if doBalance:\n",
    "    for mergeOption in list_mergeOption:\n",
    "        print()\n",
    "        print()\n",
    "        print(\"******************************\")\n",
    "        print(\"mergeOption\",mergeOption)\n",
    "        print(\"******************************\")\n",
    "        do_balance_mergeOption(mergeOption)\n",
    "        if debug:\n",
    "            print(\"Done mergeOption\",mergeOption)\n",
    "    # done for loop over mergeOption\n",
    "# done if"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done do_create_output_based_on_min_nb_of_positive_hits(list_eventNumber\n"
     ]
    }
   ],
   "source": [
    "def do_process_merged(mergeOption):\n",
    "    \n",
    "    # create output folder name for the merge, for the current number of events to merge\n",
    "    eventNumberMin=mergeOption[0] # included\n",
    "    eventNumberMax=mergeOption[1] # excluded\n",
    "    #\n",
    "    outputFolderNameMerge=outputFolderName+\"_ev_\"\n",
    "    #\n",
    "    if eventNumberMin<10:\n",
    "        outputFolderNameMerge+=\"00\"\n",
    "    elif eventNumberMin<100:\n",
    "        outputFolderNameMerge+=\"0\"\n",
    "    outputFolderNameMerge+=str(eventNumberMin)\n",
    "    #\n",
    "    outputFolderNameMerge+=\"_\"\n",
    "    #\n",
    "    if eventNumberMax<10:\n",
    "        outputFolderNameMerge+=\"00\"\n",
    "    elif eventNumberMax<100:\n",
    "        outputFolderNameMerge+=\"0\"\n",
    "    outputFolderNameMerge+=str(eventNumberMax)\n",
    "    #\n",
    "    if not os.path.exists(outputFolderNameMerge):\n",
    "        os.makedirs(outputFolderNameMerge)\n",
    "    \n",
    "    \n",
    "    # create the numpy array with all negative with which we replace those that need to\n",
    "    nparray_output_negative_one_bucket=np.array([-1 for i in range(bucketSize)]).astype(np.int8)\n",
    "    \n",
    "    # we change outputC for both Train and Test\n",
    "    list_TrainOrTest=[\"Train\",\"Test\"]\n",
    "    \n",
    "    eventNumber=\"all\"\n",
    "    \n",
    "    # for loop over TrainOrTest\n",
    "    for TrainOrTest in list_TrainOrTest:\n",
    "        if debug or verbose:\n",
    "            print(\"TrainOrTest\",TrainOrTest)\n",
    "        # open the files needed for this event: Output and NbPositiveHit\n",
    "        fileNameNNOutputAll=outputFolderNameMerge+\"/NN_2_data_OutputBalanced_\"+TrainOrTest+\"_\"+eventNumber+\".npy\"\n",
    "        nparray_output_all=np.load(fileNameNNOutputAll)\n",
    "        fileNameNNNbPositiveHitAll=outputFolderNameMerge+\"/NN_2_data_NbPositiveHitBalanced_\"+TrainOrTest+\"_\"+eventNumber+\".npy\"\n",
    "        nparray_nbPositiveHit_all=np.load(fileNameNNNbPositiveHitAll)\n",
    "        # \n",
    "        dict_minNbPositiveHit_nparray_output_all={}\n",
    "        # for loop over each val of min number of positive fits\n",
    "        for minNbPositiveHit in list_minNbPositiveHit:\n",
    "            if debug:\n",
    "                print(\"i\",i,\"eventNumber\",eventNumber,\"minNbPositiveHit\",minNbPositiveHit)\n",
    "            # copy the current output in a completely new version with deep copy\n",
    "            # as we will want to replace only the values that need changed for less than needed positive hits\n",
    "            dict_minNbPositiveHit_nparray_output_all[minNbPositiveHit]=copy.deepcopy(nparray_output_all)\n",
    "        # done for loop over minNbPositiveHit\n",
    "        #\n",
    "        # loop over each row (bucket) in the output\n",
    "        for i in range(len(nparray_output_all)):\n",
    "            nbPositiveHit=nparray_nbPositiveHit_all[i][0]\n",
    "            if debug:\n",
    "                print(\"i=%.0f, nbPositiveHit=%.0f\"%(i,nbPositiveHit))\n",
    "            # to speed up, change only wheen needed\n",
    "            for minNbPositiveHit in list_minNbPositiveHit:\n",
    "                if nbPositiveHit<minNbPositiveHit:\n",
    "                    # replace this one line with the one with all negative\n",
    "                    dict_minNbPositiveHit_nparray_output_all[minNbPositiveHit][i]=nparray_output_negative_one_bucket\n",
    "                if debug:\n",
    "                    print(\"i=%.0f, minNbPositiveHit=%.0f, bucket\"%(i,minNbPositiveHit),dict_minNbPositiveHit_nparray_output_all[minNbPositiveHit][j])\n",
    "            # done for loop over minNbPositiveHit  \n",
    "        # done for loop over buckets\n",
    "        # write the new processed output to file\n",
    "        for minNbPositiveHit in list_minNbPositiveHit:\n",
    "            minString=\"Min\"\n",
    "            if minNbPositiveHit<9:\n",
    "                minString+=\"0\"\n",
    "            minString+=str(minNbPositiveHit)\n",
    "            fileNameNNOutputAll=outputFolderNameMerge+\"/NN_2_data_OutputBalanced\"+minString+\"_\"+TrainOrTest+\"_\"+eventNumber+\".npy\"\n",
    "            np.save(fileNameNNOutputAll,dict_minNbPositiveHit_nparray_output_all[minNbPositiveHit])\n",
    "        # done for loop over minNbPositiveHit  \n",
    "    # done for loop over events\n",
    "# done function\n",
    "if debug or verbose:\n",
    "    print(\"Done do_create_output_based_on_min_nb_of_positive_hits(list_eventNumber\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "******************************\n",
      "mergeOption [0, 100]\n",
      "******************************\n",
      "TrainOrTest Train\n",
      "TrainOrTest Test\n"
     ]
    }
   ],
   "source": [
    "if doProcessBalanced:\n",
    "    for mergeOption in list_mergeOption:\n",
    "        print()\n",
    "        print()\n",
    "        print(\"******************************\")\n",
    "        print(\"mergeOption\",mergeOption)\n",
    "        print(\"******************************\")\n",
    "        do_process_merged(mergeOption)\n",
    "        if debug:\n",
    "            print(\"Done mergeOption\",mergeOption)\n",
    "    # done for loop over mergeOption\n",
    "# done if"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done all!\n"
     ]
    }
   ],
   "source": [
    "print(\"Done all!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
