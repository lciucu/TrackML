{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from annoy import AnnoyIndex\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug=False\n",
    "bucketSize=20\n",
    "minValueOfNrHitsForParticleWithMostHits=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pt orice proiect fisier de input din care sa iau date si fisier de output in care sa pun rezultatul obtinut\n",
    "inputFolderName=\"/Users/luizaadelinaciucu/Work/ATLAS/data/TrackML/ttbar_mu200-generic\"\n",
    "outputFolderName=\"/Users/luizaadelinaciucu/Work/ATLAS/TrackML/outputC2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eventNumber_from_fileName(fileName):\n",
    "    eventNumber=fileName.replace(\"event\",\"\").replace(\"-hits.csv\",\"\")\n",
    "    return eventNumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All events available in my folder. list_eventNumber ['000000007']\n"
     ]
    }
   ],
   "source": [
    "# calculate event numbers from my folder\n",
    "# lista goala in care sa pun numerele evenimentelor\n",
    "list_eventNumber=[]\n",
    "# The sorted() function returns a sorted list of the specified iterable object \n",
    "# Strings are sorted alphabetically, and numbers are sorted numerically\n",
    "# os operating system \n",
    "# hits ca sa nu am de doura ori evenimentul in lista\n",
    "for fileName in sorted(os.listdir(inputFolderName)):\n",
    "    if fileName.endswith(\"-hits.csv\"):\n",
    "        #print(fileName)\n",
    "        eventNumber=get_eventNumber_from_fileName(fileName)\n",
    "        #print(eventNumber)\n",
    "        list_eventNumber.append(eventNumber)\n",
    "# done for loop\n",
    "#list_eventNumber=[\"000000007\"]\n",
    "print(\"All events available in my folder. list_eventNumber\", list_eventNumber)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this runs only once per event\n",
    "def buildAnnoyIndex(nparray_position,metric=\"angular\",ntrees=10,debug=False): \n",
    "    \n",
    "    numberDimension=nparray_position.shape[1] # 3 (x,y,z)\n",
    "    if debug:\n",
    "        print(\"numberDimension\",numberDimension,\"metric\",metric)\n",
    "    index=AnnoyIndex(numberDimension,metric)\n",
    "    if debug:\n",
    "        print(\"type(index)\",type(index))\n",
    "        print(\"enumerate data\")\n",
    "    # add each hit to the index\n",
    "    for i,position in enumerate(nparray_position):\n",
    "        if debug:\n",
    "            print(\"i\",i,\"position\",position)\n",
    "        index.add_item(i,position)\n",
    "    # done for loop over hits\n",
    "    # build the index with 10 trees\n",
    "    index.build(ntrees) # 10 trees\n",
    "    return index\n",
    "# done function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_output_from_df_hits(df_hits,index):\n",
    "    list_nparray_input=[]\n",
    "    list_nparray_output=[]\n",
    "    \n",
    "    nparray_volume_id=df_hits[\"volume_id\"].values\n",
    "    nparray_layer_id=df_hits[\"layer_id\"].values\n",
    "    \n",
    "    for i in range(len(df_hits)):\n",
    "        if (nparray_volume_id[i]==8 and nparray_layer_id[i]==2)==False:\n",
    "            continue\n",
    "            \n",
    "        # using annoy to find the 20 nearest neighboring hits by angle to this hit\n",
    "        list_index=index.get_nns_by_item(i,bucketSize)\n",
    "        if debug:\n",
    "            print(\"list_index\",list_index)\n",
    "            \n",
    "        \n",
    "            \n",
    "        # create bucket\n",
    "        df_bucket=df_hits.iloc[list_index]\n",
    "        \n",
    "        # create NN input for this one bucket\n",
    "        nparray_input=df_bucket[[\"x\",\"y\",\"z\"]].values.flatten()\n",
    "        if debug:\n",
    "            print(\"nparray_input\",nparray_input,\"shape\",nparray_input.shape,\"type\",type(nparray_input))\n",
    "            \n",
    "        \n",
    "            \n",
    "        # create NN output for this one bucket\n",
    "        # identify particle with the largest number of hits in this bucket\n",
    "        nparray_particleID=df_bucket[\"particle_id\"].values\n",
    "        \n",
    "\n",
    "        if debug:\n",
    "            print(\"nparray_particleID\",nparray_particleID,\"shape\",nparray_particleID.shape,\"type\",type(nparray_particleID))\n",
    "        dict_particleID_counterParticleID={}\n",
    "        for particleID in nparray_particleID:\n",
    "            if particleID not in dict_particleID_counterParticleID:\n",
    "                dict_particleID_counterParticleID[particleID]=1\n",
    "            else:\n",
    "                dict_particleID_counterParticleID[particleID]+=1\n",
    "        if debug:\n",
    "            print(\"dict_particleID_counterParticleID\",dict_particleID_counterParticleID)\n",
    "          \n",
    "        # find the maximum value of the counters\n",
    "        particleIDWithMaxHits=0\n",
    "        counterParticleIDWithMaxHits=0\n",
    "        for particleID in dict_particleID_counterParticleID:\n",
    "            counterParticleID=dict_particleID_counterParticleID[particleID]\n",
    "            if counterParticleID>counterParticleIDWithMaxHits:\n",
    "                counterParticleIDWithMaxHits=counterParticleID\n",
    "                particleIDWithMaxHits=particleID\n",
    "        if debug:\n",
    "            print(\"counterBucket\",counterBucket,\"particleIDWithMaxHits\",particleIDWithMaxHits,\"counterParticleIDWithMaxHits\",counterParticleIDWithMaxHits)        \n",
    "        \n",
    "        # create nparray_output\n",
    "        list_output=[]\n",
    "        \n",
    "        # loop over every hit in the bucket\n",
    "        for particleID in nparray_particleID:      \n",
    "            if counterParticleIDWithMaxHits<minValueOfNrHitsForParticleWithMostHits:\n",
    "                list_output.append(-1)\n",
    "            else:\n",
    "                # do normally\n",
    "                if particleID==particleIDWithMaxHits:\n",
    "                    list_output.append(1)\n",
    "                else:\n",
    "                    list_output.append(-1)\n",
    "                # done if\n",
    "            # done if\n",
    "        # done for loop for each hit in the bucket\n",
    "        if debug:        \n",
    "            print(\"list_output\",list_output)\n",
    "        nparray_output=np.array(list_output)\n",
    "        if debug:\n",
    "            print(\"nparray_output\",nparray_output)\n",
    "            \n",
    "        if debug and doTest:\n",
    "            print(\"i\",i,\"counterBucket\",counterBucket)\n",
    "            plt.plot(df_bucket.x,df_bucket.y,\"o\",color=\"red\")\n",
    "            plt.plot(0,0,\"r+\")\n",
    "            plt.title(str(i))\n",
    "            plt.show()\n",
    "            plotBucketFileNameStem=outputFolderName+\"/bucket_i_\"+str(i)+\"_counterBucket_\"+str(counterBucket)+\"_eventNumber_\"+eventNumber\n",
    "            # plt.savefig(plotBucketFileNameStem+\".png\")\n",
    "            # plt.savefig(plotBucketFileNameStem+\".pdf\")\n",
    "        # done if\n",
    "            \n",
    "        # add for this current bucket to the list for the entire event (which has several buckets)\n",
    "        list_nparray_input.append(nparray_input)\n",
    "        list_nparray_output.append(nparray_output)\n",
    "    # done for loop over hits in the event\n",
    "    \n",
    "    # transform list of numpy array into numpy array of dimension 2 \n",
    "    # rows are buckets, columns are intput and output values\n",
    "    nparray_input_all=np.array(list_nparray_input)\n",
    "    nparray_output_all=np.array(list_nparray_output)\n",
    "    \n",
    "    return nparray_input_all, nparray_output_all\n",
    "# done function      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0 eventNumber 000000007\n",
      "nparray_Input_Train_all_Events [[[-3.27351e+01]\n",
      "  [-3.13191e+00]\n",
      "  [-4.42867e+02]\n",
      "  ...\n",
      "  [-4.44534e+01]\n",
      "  [-1.76822e+00]\n",
      "  [-6.02500e+02]]\n",
      "\n",
      " [[-3.05373e+01]\n",
      "  [-9.41792e+00]\n",
      "  [-4.80159e+02]\n",
      "  ...\n",
      "  [-4.29130e+01]\n",
      "  [-1.39791e+01]\n",
      "  [-6.98000e+02]]\n",
      "\n",
      " [[-3.36008e+01]\n",
      "  [-6.55743e-01]\n",
      "  [-4.51391e+02]\n",
      "  ...\n",
      "  [-6.22143e+01]\n",
      "  [-1.99989e+00]\n",
      "  [-8.17500e+02]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-3.09951e+01]\n",
      "  [ 8.76215e+00]\n",
      "  [ 4.39276e+02]\n",
      "  ...\n",
      "  [-5.03296e+01]\n",
      "  [ 1.54623e+01]\n",
      "  [ 6.98000e+02]]\n",
      "\n",
      " [[-3.09529e+01]\n",
      "  [ 9.50895e+00]\n",
      "  [ 4.26645e+02]\n",
      "  ...\n",
      "  [-8.28716e+01]\n",
      "  [ 2.29552e+01]\n",
      "  [ 1.10200e+03]]\n",
      "\n",
      " [[-2.87558e+01]\n",
      "  [ 1.27489e+01]\n",
      "  [ 4.53941e+02]\n",
      "  ...\n",
      "  [-3.63047e+01]\n",
      "  [ 1.46320e+01]\n",
      "  [ 5.98000e+02]]] shape (5458, 60, 1) type <class 'numpy.ndarray'>\n",
      "nparray_Input_Test_all_Events [[[-3.72866e+01]\n",
      "  [-1.21141e+01]\n",
      "  [-5.98000e+02]\n",
      "  ...\n",
      "  [-4.40492e+01]\n",
      "  [-1.35834e+01]\n",
      "  [-6.98000e+02]]\n",
      "\n",
      " [[-3.31042e+01]\n",
      "  [-2.07623e+00]\n",
      "  [-4.35250e+02]\n",
      "  ...\n",
      "  [-3.20580e+01]\n",
      "  [-1.28937e+00]\n",
      "  [-4.28113e+02]]\n",
      "\n",
      " [[-3.03680e+01]\n",
      "  [-9.90215e+00]\n",
      "  [-4.76327e+02]\n",
      "  ...\n",
      "  [-6.88241e+01]\n",
      "  [-2.12282e+01]\n",
      "  [-1.09800e+03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-3.13507e+01]\n",
      "  [ 2.45697e+00]\n",
      "  [ 4.64460e+02]\n",
      "  ...\n",
      "  [-8.65712e+01]\n",
      "  [ 1.18538e+01]\n",
      "  [ 1.29750e+03]]\n",
      "\n",
      " [[-2.97822e+01]\n",
      "  [ 1.06189e+01]\n",
      "  [ 4.56965e+02]\n",
      "  ...\n",
      "  [-8.60676e+01]\n",
      "  [ 2.98866e+01]\n",
      "  [ 1.29800e+03]]\n",
      "\n",
      " [[-3.07156e+01]\n",
      "  [ 1.37165e+01]\n",
      "  [ 4.73768e+02]\n",
      "  ...\n",
      "  [-1.03475e+02]\n",
      "  [ 4.35296e+01]\n",
      "  [ 1.49800e+03]]] shape (5458, 60, 1) type <class 'numpy.ndarray'>\n",
      "nparray_Output_Train_all_Events [[-1 -1 -1 ... -1 -1 -1]\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " ...\n",
      " [-1 -1 -1 ...  1  1  1]\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " [ 1  1  1 ... -1 -1 -1]] shape (5458, 20) type <class 'numpy.ndarray'>\n",
      "nparray_Output_Test_all_Events [[-1 -1 -1 ...  1  1  1]\n",
      " [-1 -1 -1 ...  1 -1 -1]\n",
      " [-1 -1  1 ...  1  1  1]\n",
      " ...\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " [ 1  1  1 ... -1 -1 -1]\n",
      " [-1  1  1 ... -1 -1 -1]] shape (5458, 20) type <class 'numpy.ndarray'>\n",
      "Done all!\n"
     ]
    }
   ],
   "source": [
    "# for loop over eventNumber from list_eventNumber\n",
    "for i,eventNumber in enumerate(list_eventNumber):\n",
    "    print(\"i\",i,\"eventNumber\",eventNumber)\n",
    "    \n",
    "    inputFileName_hits_recon=inputFolderName+\"/event\"+eventNumber+\"-hits.csv\"\n",
    "    inputFileName_hits_truth=inputFolderName+\"/event\"+eventNumber+\"-truth.csv\"\n",
    "    \n",
    "    df_hits_recon=pd.read_csv(inputFileName_hits_recon)\n",
    "    df_hits_truth=pd.read_csv(inputFileName_hits_truth)\n",
    "    df_hits=pd.concat([df_hits_recon,df_hits_truth],axis=1,sort=False)\n",
    "    \n",
    "    # build annoy index\n",
    "    nparray_position=df_hits[[\"x\",\"y\",\"z\"]].values\n",
    "    if debug:\n",
    "        print(\"nparray_position\",nparray_position)\n",
    "\n",
    "    index=buildAnnoyIndex(nparray_position,metric=\"angular\",ntrees=10,debug=False)\n",
    "    \n",
    "    # for loop peste hituri peste hiturile preselectate volume_id=8, layer_id=2\n",
    "    nparray_input_all,nparray_output_all=get_input_output_from_df_hits(df_hits,index)\n",
    "    if debug:\n",
    "        print(\"nparray_input_all\",nparray_input_all,\"shape\",nparray_input_all.shape,\"type\",type(nparray_input_all)) \n",
    "        print(\"nparray_output_all\",nparray_output_all,\"shape\",nparray_output_all.shape,\"type\",type(nparray_output_all))\n",
    "\n",
    "    # spliting part\n",
    "    # insure that the number of buckets is even, si to split later in train and test\n",
    "    nrBucket=nparray_input_all.shape[0]\n",
    "    if  nrBucket%2==1:\n",
    "        nparray_input_all=nparray_input_all[0:-1]\n",
    "        nparray_output_all=nparray_output_all[0:-1]\n",
    "    else:\n",
    "        pass\n",
    "    if debug:\n",
    "        print(\"nparray_input_all\",nparray_input_all,\"shape\",nparray_input_all.shape,\"type\",type(nparray_input_all))\n",
    "        print(\"nparray_output_all\",nparray_output_all,\"shape\",nparray_output_all.shape,\"type\",type(nparray_output_all))\n",
    "        \n",
    "    # reshape only input by adding one extra dimension needed by tensorflow, in practice it puts all into one extra bracket\n",
    "    nparray_input_all=nparray_input_all.reshape(nparray_input_all.shape[0],nparray_input_all.shape[1],1)\n",
    "    if debug:\n",
    "        print(\"nparray_input_all\",nparray_input_all,\"shape\",nparray_input_all.shape,\"type\",type(nparray_input_all))\n",
    "\n",
    "    # spliting; list comprehension\n",
    "    nrBucket=nparray_input_all.shape[0]\n",
    "    list_index_Train=[i for i in range(nrBucket) if i%2==0]\n",
    "    list_index_Test=[i for i in range(nrBucket) if i%2==1]\n",
    "\n",
    "    nparray_Input_Train=nparray_input_all[list_index_Train]\n",
    "    nparray_Input_Test=nparray_input_all[list_index_Test]\n",
    "    nparray_Output_Train=nparray_output_all[list_index_Train]\n",
    "    nparray_Output_Test=nparray_output_all[list_index_Test]\n",
    "    \n",
    "    if i==0:\n",
    "        nparray_Input_Train_all_Events=copy.deepcopy(nparray_Input_Train)\n",
    "        nparray_Input_Test_all_Events=copy.deepcopy(nparray_Input_Test)\n",
    "        nparray_Output_Train_all_Events=copy.deepcopy(nparray_Output_Train)\n",
    "        nparray_Output_Test_all_Events=copy.deepcopy(nparray_Output_Test)\n",
    "    else:\n",
    "        nparray_Input_Train_all_Events=np.concatenate((nparray_Input_Train_all_Events,nparray_Input_Train),axis=0,out=None)\n",
    "        nparray_Input_Test_all_Events=np.concatenate((nparray_Input_Test_all_Events,nparray_Input_Test),axis=0,out=None)\n",
    "        nparray_Output_Train_all_Events=np.concatenate((nparray_Output_Train_all_Events,nparray_Output_Train),axis=0,out=None)\n",
    "        nparray_Output_Test_all_Events=np.concatenate((nparray_Output_Test_all_Events,nparray_Output_Test),axis=0,out=None)\n",
    "    # done if\n",
    "# done for loop over eventNumber\n",
    "\n",
    "eventNumber=\"all\"\n",
    "fileNameNNInputTrainAll=outputFolderName+\"/NN_2_data_Input_Train_\"+eventNumber+\".npy\"\n",
    "fileNameNNInputTestAll=outputFolderName+\"/NN_2_data_Input_Test_\"+eventNumber+\".npy\"\n",
    "fileNameNNOutputTrainAll=outputFolderName+\"/NN_2_data_Output_Train_\"+eventNumber+\".npy\"\n",
    "fileNameNNOutputTestAll=outputFolderName+\"/NN_2_data_Output_Test_\"+eventNumber+\".npy\"\n",
    "\n",
    "np.save(fileNameNNInputTrainAll,nparray_Input_Train_all_Events)\n",
    "np.save(fileNameNNOutputTrainAll,nparray_Output_Train_all_Events)\n",
    "np.save(fileNameNNInputTestAll,nparray_Input_Test_all_Events)\n",
    "np.save(fileNameNNOutputTestAll,nparray_Output_Test_all_Events)\n",
    "\n",
    "print(\"nparray_Input_Train_all_Events\",nparray_Input_Train_all_Events,\"shape\",nparray_Input_Train_all_Events.shape,\"type\",type(nparray_Input_Train_all_Events))\n",
    "print(\"nparray_Input_Test_all_Events\",nparray_Input_Test_all_Events,\"shape\",nparray_Input_Test_all_Events.shape,\"type\",type(nparray_Input_Test_all_Events))\n",
    "print(\"nparray_Output_Train_all_Events\",nparray_Output_Train_all_Events,\"shape\",nparray_Output_Train_all_Events.shape,\"type\",type(nparray_Output_Train_all_Events))\n",
    "print(\"nparray_Output_Test_all_Events\",nparray_Output_Test_all_Events,\"shape\",nparray_Output_Test_all_Events.shape,\"type\",type(nparray_Output_Test_all_Events))\n",
    "\n",
    "print(\"Done all!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
